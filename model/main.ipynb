{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import jieba\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from data import Data\n",
    "from torch import nn, optim\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed.\n",
    "random_seed = 0\n",
    "\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    0: \"牡羊\",\n",
    "    1: \"金牛\",\n",
    "    2: \"雙子\",\n",
    "    3: \"巨蠍\",\n",
    "    4: \"獅子\",\n",
    "    5: \"處女\",\n",
    "    6: \"天秤\",\n",
    "    7: \"天蠍\",\n",
    "    8: \"射手\",\n",
    "    9: \"魔羯\",\n",
    "    10: \"水瓶\",\n",
    "    11: \"雙魚\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning completed.\n",
      "ToDataset completed.\n",
      "Argumantation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.523 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenlization completed.\n",
      "Padding completed.\n",
      "Token2id completed.\n",
      "Process completed.\n"
     ]
    }
   ],
   "source": [
    "raw = {i: open(f\"./dataset/{classes[i]}.txt\", encoding=\"utf-8\").read() for i in range(12)}\n",
    "data = Data(data=raw, padding_length=32)\n",
    "\n",
    "train_raw = data.get(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6327"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3369,\n",
       "  2899,\n",
       "  3515,\n",
       "  3828,\n",
       "  3959,\n",
       "  2455,\n",
       "  2775,\n",
       "  1360,\n",
       "  4646,\n",
       "  2219,\n",
       "  1428,\n",
       "  1428,\n",
       "  1428,\n",
       "  1428,\n",
       "  1428,\n",
       "  1428,\n",
       "  1428,\n",
       "  1428,\n",
       "  1428,\n",
       "  1428,\n",
       "  1428,\n",
       "  1428,\n",
       "  1428,\n",
       "  1428,\n",
       "  1428,\n",
       "  1428,\n",
       "  1428,\n",
       "  1428,\n",
       "  1428,\n",
       "  1428,\n",
       "  1428,\n",
       "  1428],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, data: list, label: list):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index]), torch.tensor(self.label[index], dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, l = [], []\n",
    "\n",
    "for i, j in train_raw:\n",
    "    d.append(i); l.append(j)\n",
    "\n",
    "train_ds = CreateDataset(d, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3369, 2899, 3515, 3828, 3959, 2455, 2775, 1360, 4646, 2219, 1428, 1428,\n",
       "         1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428,\n",
       "         1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428]),\n",
       " tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.linear1 = nn.Linear(256, 24)\n",
    "        self.linear2 = nn.Linear(24, 1)\n",
    "        self.relu = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        b = x.size(0)\n",
    "        x = x.reshape(-1, 256)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        x = x.reshape(b, -1)\n",
    "        x = torch.softmax(x, dim=1)\n",
    "\n",
    "        return x.unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionClassifier, self).__init__()\n",
    "        self.attention = Attention()\n",
    "        self.linear = nn.Linear(256, 12)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        attention = self.attention(x)\n",
    "\n",
    "        x = (x * attention).sum(dim=1)\n",
    "        x = torch.log_softmax(self.linear(x), dim=1)\n",
    "\n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(data.get(\"token_len\"), 64)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=64,\n",
    "            hidden_size=256,\n",
    "            num_layers=4,\n",
    "            dropout=0.5,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.linear = nn.Linear(16384, 12)\n",
    "        self.classifier = AttentionClassifier()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        x = x[:, :, :256] + x[:, :, :256]\n",
    "\n",
    "        x, _ = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traning history data.\n",
    "train_accuracy_h = []\n",
    "train_loss_h = []\n",
    "validate_accuracy_h = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs: int, model: nn.Module, optimizer: optim.Optimizer, loss: nn.CrossEntropyLoss, dataloader: DataLoader):\n",
    "    # Set model to training mode.\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_accuracy = 0\n",
    "        train_loss = 0\n",
    "        train_total = 0\n",
    "        train_process = 0\n",
    "        train_time = datetime.now().timestamp()\n",
    "\n",
    "        for texts, labels in dataloader:\n",
    "            texts: torch.Tensor\n",
    "            labels: torch.Tensor\n",
    "\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs: torch.Tensor = model(texts)\n",
    "            losses: torch.Tensor = loss(outputs, labels)\n",
    "\n",
    "            # optimizer.zero_grad()\n",
    "            for param in model.parameters(): param.grad = None\n",
    "\n",
    "            # Backpropagation.\n",
    "            losses.backward()\n",
    "\n",
    "            # Update parameters.\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predict = torch.max(outputs, 1)\n",
    "            train_accuracy += sum([labels[i][predict[i]] == 1 for i in range(len(predict))])\n",
    "            train_loss += losses.item()\n",
    "            train_total += labels.shape[0]\n",
    "            train_process += 1\n",
    "\n",
    "            print(\n",
    "                f\"{datetime.now().strftime('%Y/%m/%d %H:%M:%S')} \"\n",
    "                f\"Epoch: {epoch:03d} \"\n",
    "                f\"Time: {datetime.now().timestamp() - train_time:.2f} \"\n",
    "                f\"Process: {train_process / len(dataloader) * 100:.2f}% \"\n",
    "                f\"Accuracy: {train_accuracy / train_total * 100:.2f}% \"\n",
    "                f\"Loss: {train_loss:.3f}\",\n",
    "                end=\"\\r\"\n",
    "            )\n",
    "\n",
    "        train_accuracy_h.append(train_accuracy / train_total * 100)\n",
    "        train_loss_h.append(train_loss)\n",
    "\n",
    "        print()\n",
    "\n",
    "        # Early stop.\n",
    "        if train_accuracy / train_total > 0.99:\n",
    "            print(\"Early stopped.\")\n",
    "            break\n",
    "\n",
    "    # Set model to evaluation mode.\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024/04/01 17:19:37 Epoch: 000 Time: 15.22 Process: 100.00% Accuracy: 10.34% Loss: 1964.608\n",
      "2024/04/01 17:19:51 Epoch: 001 Time: 13.76 Process: 100.00% Accuracy: 11.16% Loss: 1960.328\n",
      "2024/04/01 17:20:04 Epoch: 002 Time: 13.60 Process: 100.00% Accuracy: 12.23% Loss: 1955.031\n",
      "2024/04/01 17:20:18 Epoch: 003 Time: 13.94 Process: 100.00% Accuracy: 13.99% Loss: 1939.723\n",
      "2024/04/01 17:20:33 Epoch: 004 Time: 14.11 Process: 100.00% Accuracy: 17.10% Loss: 1848.588\n",
      "2024/04/01 17:20:47 Epoch: 005 Time: 14.19 Process: 100.00% Accuracy: 24.15% Loss: 1709.146\n",
      "2024/04/01 17:21:01 Epoch: 006 Time: 14.36 Process: 100.00% Accuracy: 30.66% Loss: 1569.472\n",
      "2024/04/01 17:21:15 Epoch: 007 Time: 14.38 Process: 100.00% Accuracy: 36.98% Loss: 1432.271\n",
      "2024/04/01 17:21:30 Epoch: 008 Time: 14.41 Process: 100.00% Accuracy: 43.76% Loss: 1271.954\n",
      "2024/04/01 17:21:44 Epoch: 009 Time: 14.42 Process: 100.00% Accuracy: 52.17% Loss: 1086.149\n",
      "2024/04/01 17:21:59 Epoch: 010 Time: 14.47 Process: 100.00% Accuracy: 62.76% Loss: 861.885\n",
      "2024/04/01 17:22:13 Epoch: 011 Time: 14.65 Process: 100.00% Accuracy: 72.28% Loss: 654.063\n",
      "2024/04/01 17:22:28 Epoch: 012 Time: 14.93 Process: 100.00% Accuracy: 81.08% Loss: 450.172\n",
      "2024/04/01 17:22:43 Epoch: 013 Time: 14.58 Process: 100.00% Accuracy: 86.44% Loss: 313.369\n",
      "2024/04/01 17:22:57 Epoch: 014 Time: 14.33 Process: 100.00% Accuracy: 92.03% Loss: 191.101\n",
      "2024/04/01 17:23:12 Epoch: 015 Time: 14.73 Process: 100.00% Accuracy: 94.50% Loss: 142.325\n",
      "2024/04/01 17:23:26 Epoch: 016 Time: 14.23 Process: 100.00% Accuracy: 96.60% Loss: 84.530\n",
      "2024/04/01 17:23:41 Epoch: 017 Time: 14.45 Process: 100.00% Accuracy: 97.20% Loss: 72.141\n",
      "2024/04/01 17:23:55 Epoch: 018 Time: 14.51 Process: 100.00% Accuracy: 98.34% Loss: 46.632\n",
      "2024/04/01 17:24:10 Epoch: 019 Time: 14.46 Process: 100.00% Accuracy: 98.45% Loss: 43.670\n",
      "2024/04/01 17:24:24 Epoch: 020 Time: 14.30 Process: 100.00% Accuracy: 98.56% Loss: 36.283\n",
      "2024/04/01 17:24:38 Epoch: 021 Time: 14.48 Process: 100.00% Accuracy: 99.46% Loss: 18.433\n",
      "Early stopped.\n"
     ]
    }
   ],
   "source": [
    "train(epochs=epochs, model=model, optimizer=optimizer, loss=loss, dataloader=train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3453,  416, 3257, 3361, 4929, 1901, 4147, 5283, 1389,  168, 4378, 4929,\n",
      "         2822, 3546, 2219, 1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428,\n",
      "         1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428],\n",
      "        [4775, 5127, 1004, 1360, 2256, 2219, 3959, 2411, 1719, 1395,  323, 3353,\n",
      "         5407,  323, 3353, 5283,  379, 3369, 1201,  376, 3104, 2611, 1815, 3820,\n",
      "         1360, 1279, 5283, 2654,  182, 4633, 3632, 3301],\n",
      "        [5321, 1321, 4537, 2011, 1033, 1360, 5283,  176, 4929, 1321, 2245, 5283,\n",
      "         2813, 1251, 5283, 1863, 1321, 5193, 2379,  740, 2219, 1428, 1428, 1428,\n",
      "         1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428],\n",
      "        [3652, 1834, 5198, 5193, 2825, 3960, 5041, 4929, 3786, 1360,  427, 5283,\n",
      "          568, 5283, 5321, 1719,  570, 1360, 2551, 2344, 4929, 4112, 1067, 1000,\n",
      "         1798,  355, 2219, 1428, 1428, 1428, 1428, 1428],\n",
      "        [1834, 4227, 1120, 1360, 4509, 3429, 5097, 2602, 2219, 1428, 1428, 1428,\n",
      "         1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428,\n",
      "         1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428],\n",
      "        [2706, 2046, 5283, 1691, 1834,  252, 3253, 1360, 3722, 2219, 1428, 1428,\n",
      "         1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428,\n",
      "         1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428],\n",
      "        [2555, 2978, 3959, 2484, 3959, 5436, 3959,  392, 5172, 3959, 3724, 3959,\n",
      "         1388, 3959, 2187, 3959, 3551, 1584, 3959, 3841, 1321, 3959, 1834, 2485,\n",
      "         3959, 1397, 2219, 1428, 1428, 1428, 1428, 1428],\n",
      "        [3858,  737, 5193, 1067, 5321, 1321, 1360, 3172, 4537, 1305, 5283, 2087,\n",
      "         3203, 5443, 1312, 1773, 1321, 1360,  768, 4929,  422, 2219, 1428, 1428,\n",
      "         1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428]])\n",
      "tensor([[-1.2017e+01, -1.2966e+01, -1.9586e+01, -2.2163e+01, -2.1082e+01,\n",
      "         -1.2687e+01, -1.3562e+01, -2.4069e+01, -1.9008e+01, -1.0888e+01,\n",
      "         -4.3153e-05, -1.1360e+01],\n",
      "        [-9.1300e+00, -1.4539e+01, -1.6680e+01, -1.5295e+01, -1.8077e+01,\n",
      "         -1.5628e+01, -1.2352e+01, -1.5704e+01, -1.3602e+01, -6.1071e+00,\n",
      "         -2.3489e-03, -1.2390e+01],\n",
      "        [-2.0666e+01, -1.2534e+01, -2.2541e+01, -1.1827e+01, -2.4064e+01,\n",
      "         -1.3999e+01, -1.7564e+01, -2.2417e+01, -2.4728e+01, -1.3386e-04,\n",
      "         -9.0110e+00, -1.6800e+01],\n",
      "        [-1.0914e+01, -8.5813e-03, -1.2428e+01, -7.2112e+00, -1.6993e+01,\n",
      "         -6.1617e+00, -7.2089e+00, -6.6681e+00, -1.1157e+01, -8.5668e+00,\n",
      "         -5.6851e+00, -9.6596e+00],\n",
      "        [-2.2790e-04, -1.5481e+01, -1.7369e+01, -1.5904e+01, -8.6082e+00,\n",
      "         -1.3658e+01, -1.9510e+01, -1.0888e+01, -1.0627e+01, -2.0253e+01,\n",
      "         -1.4134e+01, -1.6699e+01],\n",
      "        [-1.2390e+01, -8.0940e-05, -1.4825e+01, -1.3850e+01, -2.0093e+01,\n",
      "         -1.0629e+01, -9.9809e+00, -1.4439e+01, -1.2372e+01, -1.6277e+01,\n",
      "         -1.7261e+01, -2.0466e+01],\n",
      "        [-1.6397e+01, -9.7516e+00, -1.9720e+01, -1.0187e+01, -2.1092e+01,\n",
      "         -1.2777e+01, -1.5944e+01, -2.4502e+01, -2.1547e+01, -2.2266e-04,\n",
      "         -8.9973e+00, -1.7180e+01],\n",
      "        [-1.3280e+01, -3.0343e+01, -1.3374e+01, -1.9689e+01, -1.4782e-05,\n",
      "         -1.1482e+01, -3.0173e+01, -2.1289e+01, -1.7194e+01, -3.3166e+01,\n",
      "         -3.0742e+01, -1.3723e+01]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "for i, j in train_dl:\n",
    "    print(i)\n",
    "    print(model(i.to(device)))\n",
    "    print(j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = [\n",
    "    \"勇敢無畏，充滿了活力和冒險精神，他們喜歡追求挑戰，敢於冒險，常常是行動派的領導者。\",\n",
    "    \"穩重可靠，以堅韌的意志力和耐心著稱，他們注重安全和舒適，並對物質生活有著強烈的執著。\",\n",
    "    \"機智聰明，好奇心旺盛，喜歡交際和表達自己，具有多才多藝的特質，常常充滿了靈活的思維和活力。\",\n",
    "    \"情感豐富，善解人意，對家庭和親密關係非常重視，他們總是充滿了溫柔和關懷，是很好的傾聽者和支持者。\",\n",
    "    \"自信大方，追求著成為焦點的慾望，他們充滿了熱情和活力，喜歡引領和影響身邊的人，時常展現出優越感和領導能力。\",\n",
    "    \"細心謹慎，追求完美，他們善於分析和解決問題，注重細節和有組織性，常常是值得信賴的夥伴和顧問。\",\n",
    "    \"追求和諧，優雅而公正，他們注重平衡和公平，善於溝通協調，是很好的調解者和中介者。\",\n",
    "    \"神秘內斂，充滿了熱情和直覺，他們擁有強烈的意志力和洞察力，常常是充滿挑戰性和魅力的個體。\",\n",
    "    \"自由奔放，熱愛冒險和探索，他們追求著廣闊的視野和新鮮的體驗，時常充滿了樂觀和幽默。\",\n",
    "    \"勤奮負責，追求事業成功和社會地位，他們具有堅毅的意志力和耐心，常常是穩健和實際的決策者。\",\n",
    "    \"獨立思考，充滿了理想主義和創意，他們追求著獨特的生活方式和社會價值觀，常常是前衛和不拘一格的個體。\",\n",
    "    \"敏感善良，充滿了同情心和想像力，他們常常是理想主義者和夢想家，追求著內心的情感和精神實踐。\",\n",
    "    \"勇敢無畏，充滿活力，喜歡追求挑戰，常常是行動派的領導者。\",\n",
    "    \"穩重可靠，堅韌耐心，注重安全舒適，對物質生活有強烈執著。\",\n",
    "    \"機智聰明，好奇心旺盛，善於交際表達，充滿靈活思維和活力。\",\n",
    "    \"情感豐富，善解人意，重視家庭和親密關係，溫柔關懷，傾聽支持者。\",\n",
    "    \"自信大方，追求成為焦點，充滿熱情活力，喜歡引領影響身邊的人。\",\n",
    "    \"細心謹慎，追求完美，善於分析解決問題，注重細節有組織性。\",\n",
    "    \"追求和諧，優雅公正，注重平衡公平，善於溝通協調。\",\n",
    "    \"神秘內斂，熱情直覺，意志力洞察力強，充滿挑戰性和魅力。\",\n",
    "    \"自由奔放，熱愛冒險探索，追求廣闊視野和新鮮體驗，樂觀幽默。\",\n",
    "    \"勤奮負責，追求事業成功社會地位，具堅毅意志力和耐心，穩健決策者。\",\n",
    "    \"獨立思考，理想主義創意，追求獨特生活方式和價值觀，前衛不拘一格。\",\n",
    "    \"敏感善良，同情心想像力豐富，理想主義夢想家，追求內心情感精神實踐。\",\n",
    "    \"勇敢果敢，充滿活力，愛冒險。\",\n",
    "    \"穩重堅定，堅持自我價值觀。\",\n",
    "    \"靈活機智，善於溝通交際。\",\n",
    "    \"情感豐富，家庭意識強。\",\n",
    "    \"自信領導，熱情奔放。\",\n",
    "    \"細心謹慎，追求完美。\",\n",
    "    \"追求和諧，公正公平。\",\n",
    "    \"神秘敏感，探索深度。\",\n",
    "    \"自由探險，樂觀向上。\",\n",
    "    \"勤奮穩健，追求成功。\",\n",
    "    \"獨立創新，理想主義者。\",\n",
    "    \"敏感浪漫，夢想家。\",\n",
    "]\n",
    "\n",
    "answare = [\n",
    "    '牡羊', '金牛', '雙子', '巨蠍', '獅子', '處女', '天秤', '天蠍', '射手', '魔羯', '水瓶', '雙魚',\n",
    "    '牡羊', '金牛', '雙子', '巨蠍', '獅子', '處女', '天秤', '天蠍', '射手', '魔羯', '水瓶', '雙魚',\n",
    "    '牡羊', '金牛', '雙子', '巨蠍', '獅子', '處女', '天秤', '天蠍', '射手', '魔羯', '水瓶', '雙魚',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(testset: list) -> torch.Tensor:\n",
    "    result = []\n",
    "\n",
    "    for line in testset:\n",
    "        temp = jieba.lcut(line)\n",
    "        temp = temp + [\"<PAD>\"] * (32 - len(temp))\n",
    "        temp = [data.w2i[x] if x in data.w2i else data.w2i[\"<PAD>\"] for x in temp][:32]\n",
    "        result.append(temp)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = process(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding): Embedding(5470, 64)\n",
       "  (lstm): LSTM(64, 256, num_layers=4, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (linear): Linear(in_features=16384, out_features=12, bias=True)\n",
       "  (classifier): AttentionClassifier(\n",
       "    (attention): Attention(\n",
       "      (linear1): Linear(in_features=256, out_features=24, bias=True)\n",
       "      (linear2): Linear(in_features=24, out_features=1, bias=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (linear): Linear(in_features=256, out_features=12, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.7656e-05, -2.2271e+01, -2.1413e+01, -2.1863e+01, -1.0559e+01,\n",
      "         -1.7787e+01, -2.4455e+01, -1.6618e+01, -1.3482e+01, -2.5198e+01,\n",
      "         -1.5914e+01, -2.0373e+01],\n",
      "        [-1.3959e+01, -6.1976e-02, -5.2192e+00, -3.5737e+00, -1.2092e+01,\n",
      "         -4.1886e+00, -4.6672e+00, -1.2275e+01, -9.9678e+00, -6.5426e+00,\n",
      "         -1.1054e+01, -7.5077e+00],\n",
      "        [-1.5109e+01, -1.4354e+01, -3.2989e+00, -1.4830e+01, -1.6827e+01,\n",
      "         -1.3161e+01, -6.7574e+00, -1.5112e+01, -4.0288e-02, -1.9216e+01,\n",
      "         -6.6045e+00, -1.0004e+01],\n",
      "        [-1.2627e+01, -8.9444e-01, -5.7307e+00, -6.2980e-01, -1.2105e+01,\n",
      "         -7.2610e+00, -3.0345e+00, -9.2380e+00, -7.6899e+00, -5.1643e+00,\n",
      "         -1.0675e+01, -9.0409e+00],\n",
      "        [-4.2704e+00, -1.8180e+01, -1.0015e+01, -9.5704e+00, -3.8215e-01,\n",
      "         -1.1348e+01, -1.6217e+01, -1.2162e+00, -5.4185e+00, -2.0195e+01,\n",
      "         -1.3109e+01, -5.9100e+00],\n",
      "        [-1.6070e+01, -6.8883e-03, -1.9752e+01, -1.5954e+01, -2.4289e+01,\n",
      "         -4.9830e+00, -1.5926e+01, -2.3103e+01, -2.1030e+01, -1.1782e+01,\n",
      "         -1.2640e+01, -1.7851e+01],\n",
      "        [-3.3975e+01, -2.0492e+01, -1.2124e+01, -3.1987e+01, -2.7955e+01,\n",
      "         -2.6166e+01, -5.6028e-06, -2.9917e+01, -2.2244e+01, -2.4615e+01,\n",
      "         -2.0547e+01, -1.6117e+01],\n",
      "        [-1.7892e+00, -1.2109e+01, -1.1370e+01, -1.0273e+01, -7.8911e+00,\n",
      "         -1.2739e+01, -1.1464e+01, -2.9002e-01, -2.4774e+00, -1.7559e+01,\n",
      "         -8.5126e+00, -9.8764e+00],\n",
      "        [-5.9742e+00, -1.4291e+01, -1.0051e+01, -1.2309e+01, -1.1523e+01,\n",
      "         -1.4629e+01, -1.1094e+01, -6.0527e+00, -5.1083e-03, -1.9043e+01,\n",
      "         -9.0279e+00, -1.1948e+01],\n",
      "        [-3.8457e-01, -6.5858e+00, -1.0052e+01, -6.4049e+00, -5.5683e+00,\n",
      "         -7.5342e+00, -8.7888e+00, -1.6905e+00, -5.2318e+00, -8.5027e+00,\n",
      "         -2.2056e+00, -4.4633e+00],\n",
      "        [-1.3634e+01, -9.1782e+00, -1.4169e+01, -1.8800e+01, -2.2332e+01,\n",
      "         -1.2068e+01, -7.3580e+00, -1.5467e+01, -1.3079e+01, -1.1576e+01,\n",
      "         -7.7539e-04, -1.1113e+01],\n",
      "        [-1.9914e+01, -1.4392e+01, -1.6524e+01, -2.4546e+01, -2.4504e+01,\n",
      "         -1.1492e+01, -1.0704e+01, -2.7213e+01, -2.2098e+01, -1.2126e+01,\n",
      "         -1.3044e-03, -6.6729e+00],\n",
      "        [-1.2278e-04, -1.7651e+01, -1.8696e+01, -1.7790e+01, -1.1315e+01,\n",
      "         -1.4694e+01, -2.1653e+01, -1.3825e+01, -9.1644e+00, -2.1419e+01,\n",
      "         -1.2301e+01, -1.8279e+01],\n",
      "        [-1.9248e+01, -1.1465e+00, -1.6394e+01, -8.7939e+00, -2.2772e+01,\n",
      "         -4.2841e+00, -1.1674e+01, -2.3121e+01, -2.2170e+01, -4.0303e-01,\n",
      "         -1.1832e+01, -1.4736e+01],\n",
      "        [-1.5136e+01, -1.5017e+01, -7.6879e-02, -1.2769e+01, -1.2064e+01,\n",
      "         -1.1352e+01, -5.3495e+00, -1.5889e+01, -3.0282e+00, -1.4196e+01,\n",
      "         -4.2936e+00, -4.9381e+00],\n",
      "        [-1.6474e+01, -1.9792e-01, -1.8004e+01, -8.1893e+00, -1.8173e+01,\n",
      "         -1.7206e+00, -1.5863e+01, -1.9799e+01, -2.3251e+01, -8.0074e+00,\n",
      "         -1.8741e+01, -1.4825e+01],\n",
      "        [-7.4098e+00, -2.6307e+01, -1.1707e+01, -1.5796e+01, -2.2338e-03,\n",
      "         -1.7546e+01, -1.9762e+01, -6.9467e+00, -7.4039e+00, -2.6120e+01,\n",
      "         -1.9095e+01, -9.9668e+00],\n",
      "        [-1.2916e+01, -4.9565e-01, -2.0042e+01, -1.2731e+01, -2.1192e+01,\n",
      "         -9.4084e-01, -1.8495e+01, -2.2398e+01, -2.1804e+01, -7.6135e+00,\n",
      "         -1.0451e+01, -1.5551e+01],\n",
      "        [-3.4687e+01, -2.2941e+01, -9.1439e+00, -3.3071e+01, -2.6575e+01,\n",
      "         -2.4697e+01, -1.0847e-04, -3.2698e+01, -2.1955e+01, -2.3922e+01,\n",
      "         -1.8760e+01, -1.3382e+01],\n",
      "        [-7.7729e+00, -1.0909e+01, -9.3061e+00, -1.0036e+01, -1.1511e+01,\n",
      "         -1.4659e+01, -6.5153e+00, -1.2082e+00, -3.5904e-01, -1.5581e+01,\n",
      "         -7.1997e+00, -9.0905e+00],\n",
      "        [-6.0540e+00, -1.2509e+01, -8.1337e+00, -1.1652e+01, -1.0485e+01,\n",
      "         -1.4836e+01, -6.2433e+00, -5.9389e+00, -8.6033e-03, -1.4266e+01,\n",
      "         -6.6696e+00, -1.0239e+01],\n",
      "        [-2.1374e-01, -1.5377e+01, -1.0941e+01, -9.0421e+00, -2.1313e+00,\n",
      "         -9.5525e+00, -1.4661e+01, -7.2802e+00, -6.6674e+00, -1.1510e+01,\n",
      "         -3.2415e+00, -3.4276e+00],\n",
      "        [-2.0164e+01, -1.7192e+01, -1.6226e+01, -2.6685e+01, -2.6570e+01,\n",
      "         -1.5281e+01, -1.1054e+01, -2.7298e+01, -1.9854e+01, -1.5384e+01,\n",
      "         -4.6252e-05, -1.0422e+01],\n",
      "        [-1.6464e+01, -1.5099e+01, -1.5537e+01, -1.6154e+01, -1.7689e+01,\n",
      "         -1.2064e+01, -1.1966e+01, -2.4054e+01, -1.9734e+01, -4.0876e+00,\n",
      "         -2.3300e-02, -5.0772e+00],\n",
      "        [-2.2282e-01, -1.1216e+01, -9.1065e+00, -9.5633e+00, -5.9117e+00,\n",
      "         -8.9322e+00, -1.0643e+01, -6.4877e+00, -2.7503e+00, -1.1545e+01,\n",
      "         -2.0592e+00, -5.6029e+00],\n",
      "        [-1.0968e+01, -3.8949e-02, -1.3853e+01, -1.2640e+01, -1.6463e+01,\n",
      "         -1.2280e+01, -3.3665e+00, -5.6140e+00, -1.3533e+01, -1.1317e+01,\n",
      "         -1.2410e+01, -1.3812e+01],\n",
      "        [-2.9083e+01, -2.3376e+01, -1.5543e-01, -2.9383e+01, -1.5575e+01,\n",
      "         -1.5881e+01, -2.0950e+00, -2.9552e+01, -1.8709e+01, -2.4535e+01,\n",
      "         -1.6143e+01, -3.8691e+00],\n",
      "        [-2.0398e+01, -1.4164e+01, -1.8857e+01, -8.4638e-06, -1.8345e+01,\n",
      "         -1.2827e+01, -2.5463e+01, -2.0480e+01, -1.8214e+01, -1.2192e+01,\n",
      "         -2.2513e+01, -1.8861e+01],\n",
      "        [-5.2580e-01, -1.9650e+01, -1.3310e+01, -1.6687e+01, -9.2131e-01,\n",
      "         -1.6330e+01, -1.5849e+01, -4.7479e+00, -6.0990e+00, -2.3934e+01,\n",
      "         -1.7345e+01, -1.2613e+01],\n",
      "        [-7.8132e+00, -6.6855e+00, -8.0904e+00, -6.2783e+00, -8.0246e+00,\n",
      "         -6.7208e-03, -1.6516e+01, -1.3420e+01, -6.3707e+00, -1.4572e+01,\n",
      "         -1.0572e+01, -7.1358e+00],\n",
      "        [-2.4021e+01, -1.8552e+01, -8.3483e+00, -2.5617e+01, -2.2393e+01,\n",
      "         -2.0354e+01, -3.9379e-04, -2.3327e+01, -1.2551e+01, -1.8651e+01,\n",
      "         -8.9292e+00, -1.0780e+01],\n",
      "        [-7.7411e+00, -9.6224e+00, -1.1331e+01, -2.0349e-02, -8.9342e+00,\n",
      "         -1.0781e+01, -1.2183e+01, -5.6198e+00, -5.6409e+00, -4.7126e+00,\n",
      "         -5.9096e+00, -7.4130e+00],\n",
      "        [-7.3179e+00, -1.1153e+01, -6.7605e+00, -1.0228e+01, -1.0666e+01,\n",
      "         -1.3321e+01, -4.1755e+00, -5.2605e+00, -9.9121e-02, -1.1099e+01,\n",
      "         -2.6502e+00, -6.6802e+00],\n",
      "        [-4.4789e+00, -1.5437e+01, -6.0202e+00, -7.5583e+00, -3.1192e-01,\n",
      "         -9.9237e+00, -1.1301e+01, -3.9739e+00, -2.0860e+00, -1.5073e+01,\n",
      "         -7.6780e+00, -2.2058e+00],\n",
      "        [-1.7353e+01, -1.6347e+01, -2.0135e+01, -2.2502e+01, -2.4546e+01,\n",
      "         -1.7420e+01, -1.2940e+01, -2.2887e+01, -2.0859e+01, -8.8324e+00,\n",
      "         -1.5270e-04, -1.2358e+01],\n",
      "        [-4.0189e+00, -7.2728e+00, -1.0429e+01, -2.5079e+00, -3.7722e+00,\n",
      "         -6.6514e+00, -1.0212e+01, -1.7315e-01, -7.3087e+00, -7.3479e+00,\n",
      "         -6.6461e+00, -3.4449e+00]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "result = model(torch.tensor(test).to(device))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normolization\n",
    "results = []\n",
    "\n",
    "for i in range(len(result)):\n",
    "    temp = result[i]\n",
    "    temp = [x - min(temp) for x in temp]\n",
    "    temp = [x / max(temp) for x in temp]\n",
    "    temp = [round(x, 3) for x in temp]\n",
    "\n",
    "    results.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "牡羊✅\n",
      "金牛✅\n",
      "射手 <- 雙子\n",
      "巨蠍✅\n",
      "獅子✅\n",
      "金牛 <- 處女\n",
      "天秤✅\n",
      "天蠍✅\n",
      "射手✅\n",
      "牡羊 <- 魔羯\n",
      "水瓶✅\n",
      "水瓶 <- 雙魚\n",
      "牡羊✅\n",
      "魔羯 <- 金牛\n",
      "雙子✅\n",
      "金牛 <- 巨蠍\n",
      "獅子✅\n",
      "金牛 <- 處女\n",
      "天秤✅\n",
      "射手 <- 天蠍\n",
      "射手✅\n",
      "牡羊 <- 魔羯\n",
      "水瓶✅\n",
      "水瓶 <- 雙魚\n",
      "牡羊✅\n",
      "金牛✅\n",
      "雙子✅\n",
      "巨蠍✅\n",
      "牡羊 <- 獅子\n",
      "處女✅\n",
      "天秤✅\n",
      "巨蠍 <- 天蠍\n",
      "射手✅\n",
      "獅子 <- 魔羯\n",
      "水瓶✅\n",
      "天蠍 <- 雙魚\n",
      "Correct: 22 (61.11%)\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    t = classes[int(torch.argmax(torch.tensor(r)))]\n",
    "    if t == answare[i]:\n",
    "        count += 1\n",
    "        print(t + \"✅\")\n",
    "    else:\n",
    "        print(t + \" <- \" + answare[i])\n",
    "\n",
    "print(f\"Correct: {count} ({(count / len(answare)) * 100 :.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# angles_A = np.linspace(start=0, stop=2*np.pi, num=len(result)+1, endpoint=True)\n",
    "# values_A = np.concatenate((result, [result[0]]))\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(5, 5), subplot_kw={'projection': 'polar'})\n",
    "# ax.plot(angles_A, values_A, 'o-', color=\"blue\", label=\"A\")\n",
    "\n",
    "# ax.fill(angles_A, values_A, alpha=0.3, color=\"blue\")\n",
    "# ax.set_thetagrids(angles_A[:-1] * 180 / np.pi, range(12), fontsize=15)\n",
    "# ax.set_theta_zero_location('N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:4315: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\onnx\\_internal\\jit_utils.py:258: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\jit\\passes\\onnx\\shape_type_inference.cpp:1888.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "c:\\Users\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\onnx\\utils.py:687: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\jit\\passes\\onnx\\shape_type_inference.cpp:1888.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "c:\\Users\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\onnx\\utils.py:1178: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\jit\\passes\\onnx\\shape_type_inference.cpp:1888.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "# Please check the virtual input of the ONNX model.\n",
    "torch.onnx.export(model, torch.tensor([test[0]]).to(device=device), 'constellator.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph torch_jit (\n",
      "  %input.1[INT64, 1x32]\n",
      ") initializers (\n",
      "  %embedding.weight[FLOAT, 5470x64]\n",
      "  %classifier.attention.linear1.weight[FLOAT, 24x256]\n",
      "  %classifier.attention.linear1.bias[FLOAT, 24]\n",
      "  %classifier.attention.linear2.weight[FLOAT, 1x24]\n",
      "  %classifier.attention.linear2.bias[FLOAT, 1]\n",
      "  %classifier.linear.weight[FLOAT, 12x256]\n",
      "  %classifier.linear.bias[FLOAT, 12]\n",
      "  %onnx::LSTM_742[FLOAT, 2x2048]\n",
      "  %onnx::LSTM_743[FLOAT, 2x1024x64]\n",
      "  %onnx::LSTM_744[FLOAT, 2x1024x256]\n",
      "  %onnx::LSTM_789[FLOAT, 2x2048]\n",
      "  %onnx::LSTM_790[FLOAT, 2x1024x512]\n",
      "  %onnx::LSTM_791[FLOAT, 2x1024x256]\n",
      "  %onnx::LSTM_836[FLOAT, 2x2048]\n",
      "  %onnx::LSTM_837[FLOAT, 2x1024x512]\n",
      "  %onnx::LSTM_838[FLOAT, 2x1024x256]\n",
      "  %onnx::LSTM_883[FLOAT, 2x2048]\n",
      "  %onnx::LSTM_884[FLOAT, 2x1024x512]\n",
      "  %onnx::LSTM_885[FLOAT, 2x1024x256]\n",
      ") {\n",
      "  %/embedding/Gather_output_0 = Gather(%embedding.weight, %input.1)\n",
      "  %/lstm/Transpose_output_0 = Transpose[perm = [1, 0, 2]](%/embedding/Gather_output_0)\n",
      "  %/lstm/Constant_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Constant_1_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Shape_output_0 = Shape(%/lstm/Transpose_output_0)\n",
      "  %/lstm/Constant_2_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_output_0 = Gather(%/lstm/Shape_output_0, %/lstm/Constant_2_output_0)\n",
      "  %onnx::Unsqueeze_179 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_180 = Unsqueeze(%/lstm/Gather_output_0, %onnx::Unsqueeze_179)\n",
      "  %/lstm/Constant_3_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_747 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_output_0 = Concat[axis = 0](%onnx::Concat_747, %onnx::Concat_180, %/lstm/Constant_3_output_0)\n",
      "  %/lstm/Expand_output_0 = Expand(%/lstm/Constant_output_0, %/lstm/Concat_output_0)\n",
      "  %/lstm/Shape_1_output_0 = Shape(%/lstm/Transpose_output_0)\n",
      "  %/lstm/Constant_4_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_1_output_0 = Gather(%/lstm/Shape_1_output_0, %/lstm/Constant_4_output_0)\n",
      "  %onnx::Unsqueeze_190 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_191 = Unsqueeze(%/lstm/Gather_1_output_0, %onnx::Unsqueeze_190)\n",
      "  %/lstm/Constant_5_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_748 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_1_output_0 = Concat[axis = 0](%onnx::Concat_748, %onnx::Concat_191, %/lstm/Constant_5_output_0)\n",
      "  %/lstm/Expand_1_output_0 = Expand(%/lstm/Constant_1_output_0, %/lstm/Concat_1_output_0)\n",
      "  %/lstm/LSTM_output_0, %/lstm/LSTM_output_1, %/lstm/LSTM_output_2 = LSTM[direction = 'bidirectional', hidden_size = 256](%/lstm/Transpose_output_0, %onnx::LSTM_743, %onnx::LSTM_744, %onnx::LSTM_742, %, %/lstm/Expand_output_0, %/lstm/Expand_1_output_0)\n",
      "  %/lstm/Transpose_1_output_0 = Transpose[perm = [0, 2, 1, 3]](%/lstm/LSTM_output_0)\n",
      "  %/lstm/Constant_6_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Reshape_output_0 = Reshape[allowzero = 0](%/lstm/Transpose_1_output_0, %/lstm/Constant_6_output_0)\n",
      "  %/lstm/Constant_7_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Constant_8_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Shape_2_output_0 = Shape(%/lstm/Reshape_output_0)\n",
      "  %/lstm/Constant_9_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_2_output_0 = Gather(%/lstm/Shape_2_output_0, %/lstm/Constant_9_output_0)\n",
      "  %onnx::Unsqueeze_336 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_337 = Unsqueeze(%/lstm/Gather_2_output_0, %onnx::Unsqueeze_336)\n",
      "  %/lstm/Constant_10_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_794 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_2_output_0 = Concat[axis = 0](%onnx::Concat_794, %onnx::Concat_337, %/lstm/Constant_10_output_0)\n",
      "  %/lstm/Expand_2_output_0 = Expand(%/lstm/Constant_7_output_0, %/lstm/Concat_2_output_0)\n",
      "  %/lstm/Shape_3_output_0 = Shape(%/lstm/Reshape_output_0)\n",
      "  %/lstm/Constant_11_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_3_output_0 = Gather(%/lstm/Shape_3_output_0, %/lstm/Constant_11_output_0)\n",
      "  %onnx::Unsqueeze_347 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_348 = Unsqueeze(%/lstm/Gather_3_output_0, %onnx::Unsqueeze_347)\n",
      "  %/lstm/Constant_12_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_795 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_3_output_0 = Concat[axis = 0](%onnx::Concat_795, %onnx::Concat_348, %/lstm/Constant_12_output_0)\n",
      "  %/lstm/Expand_3_output_0 = Expand(%/lstm/Constant_8_output_0, %/lstm/Concat_3_output_0)\n",
      "  %/lstm/LSTM_1_output_0, %/lstm/LSTM_1_output_1, %/lstm/LSTM_1_output_2 = LSTM[direction = 'bidirectional', hidden_size = 256](%/lstm/Reshape_output_0, %onnx::LSTM_790, %onnx::LSTM_791, %onnx::LSTM_789, %, %/lstm/Expand_2_output_0, %/lstm/Expand_3_output_0)\n",
      "  %/lstm/Transpose_2_output_0 = Transpose[perm = [0, 2, 1, 3]](%/lstm/LSTM_1_output_0)\n",
      "  %/lstm/Constant_13_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Reshape_1_output_0 = Reshape[allowzero = 0](%/lstm/Transpose_2_output_0, %/lstm/Constant_13_output_0)\n",
      "  %/lstm/Constant_14_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Constant_15_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Shape_4_output_0 = Shape(%/lstm/Reshape_1_output_0)\n",
      "  %/lstm/Constant_16_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_4_output_0 = Gather(%/lstm/Shape_4_output_0, %/lstm/Constant_16_output_0)\n",
      "  %onnx::Unsqueeze_493 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_494 = Unsqueeze(%/lstm/Gather_4_output_0, %onnx::Unsqueeze_493)\n",
      "  %/lstm/Constant_17_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_841 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_4_output_0 = Concat[axis = 0](%onnx::Concat_841, %onnx::Concat_494, %/lstm/Constant_17_output_0)\n",
      "  %/lstm/Expand_4_output_0 = Expand(%/lstm/Constant_14_output_0, %/lstm/Concat_4_output_0)\n",
      "  %/lstm/Shape_5_output_0 = Shape(%/lstm/Reshape_1_output_0)\n",
      "  %/lstm/Constant_18_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_5_output_0 = Gather(%/lstm/Shape_5_output_0, %/lstm/Constant_18_output_0)\n",
      "  %onnx::Unsqueeze_504 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_505 = Unsqueeze(%/lstm/Gather_5_output_0, %onnx::Unsqueeze_504)\n",
      "  %/lstm/Constant_19_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_842 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_5_output_0 = Concat[axis = 0](%onnx::Concat_842, %onnx::Concat_505, %/lstm/Constant_19_output_0)\n",
      "  %/lstm/Expand_5_output_0 = Expand(%/lstm/Constant_15_output_0, %/lstm/Concat_5_output_0)\n",
      "  %/lstm/LSTM_2_output_0, %/lstm/LSTM_2_output_1, %/lstm/LSTM_2_output_2 = LSTM[direction = 'bidirectional', hidden_size = 256](%/lstm/Reshape_1_output_0, %onnx::LSTM_837, %onnx::LSTM_838, %onnx::LSTM_836, %, %/lstm/Expand_4_output_0, %/lstm/Expand_5_output_0)\n",
      "  %/lstm/Transpose_3_output_0 = Transpose[perm = [0, 2, 1, 3]](%/lstm/LSTM_2_output_0)\n",
      "  %/lstm/Constant_20_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Reshape_2_output_0 = Reshape[allowzero = 0](%/lstm/Transpose_3_output_0, %/lstm/Constant_20_output_0)\n",
      "  %/lstm/Constant_21_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Constant_22_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Shape_6_output_0 = Shape(%/lstm/Reshape_2_output_0)\n",
      "  %/lstm/Constant_23_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_6_output_0 = Gather(%/lstm/Shape_6_output_0, %/lstm/Constant_23_output_0)\n",
      "  %onnx::Unsqueeze_650 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_651 = Unsqueeze(%/lstm/Gather_6_output_0, %onnx::Unsqueeze_650)\n",
      "  %/lstm/Constant_24_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_888 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_6_output_0 = Concat[axis = 0](%onnx::Concat_888, %onnx::Concat_651, %/lstm/Constant_24_output_0)\n",
      "  %/lstm/Expand_6_output_0 = Expand(%/lstm/Constant_21_output_0, %/lstm/Concat_6_output_0)\n",
      "  %/lstm/Shape_7_output_0 = Shape(%/lstm/Reshape_2_output_0)\n",
      "  %/lstm/Constant_25_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_7_output_0 = Gather(%/lstm/Shape_7_output_0, %/lstm/Constant_25_output_0)\n",
      "  %onnx::Unsqueeze_661 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_662 = Unsqueeze(%/lstm/Gather_7_output_0, %onnx::Unsqueeze_661)\n",
      "  %/lstm/Constant_26_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_889 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_7_output_0 = Concat[axis = 0](%onnx::Concat_889, %onnx::Concat_662, %/lstm/Constant_26_output_0)\n",
      "  %/lstm/Expand_7_output_0 = Expand(%/lstm/Constant_22_output_0, %/lstm/Concat_7_output_0)\n",
      "  %/lstm/LSTM_3_output_0, %/lstm/LSTM_3_output_1, %/lstm/LSTM_3_output_2 = LSTM[direction = 'bidirectional', hidden_size = 256](%/lstm/Reshape_2_output_0, %onnx::LSTM_884, %onnx::LSTM_885, %onnx::LSTM_883, %, %/lstm/Expand_6_output_0, %/lstm/Expand_7_output_0)\n",
      "  %/lstm/Transpose_4_output_0 = Transpose[perm = [0, 2, 1, 3]](%/lstm/LSTM_3_output_0)\n",
      "  %/lstm/Constant_27_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Reshape_3_output_0 = Reshape[allowzero = 0](%/lstm/Transpose_4_output_0, %/lstm/Constant_27_output_0)\n",
      "  %/lstm/Transpose_5_output_0 = Transpose[perm = [1, 0, 2]](%/lstm/Reshape_3_output_0)\n",
      "  %/Constant_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Constant_1_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Constant_2_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Constant_3_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Slice_output_0 = Slice(%/lstm/Transpose_5_output_0, %/Constant_1_output_0, %/Constant_2_output_0, %/Constant_output_0, %/Constant_3_output_0)\n",
      "  %/Add_output_0 = Add(%/Slice_output_0, %/Slice_output_0)\n",
      "  %/classifier/attention/Constant_output_0 = Constant[value = <Tensor>]()\n",
      "  %/classifier/attention/Reshape_output_0 = Reshape[allowzero = 0](%/Add_output_0, %/classifier/attention/Constant_output_0)\n",
      "  %/classifier/attention/linear1/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/classifier/attention/Reshape_output_0, %classifier.attention.linear1.weight, %classifier.attention.linear1.bias)\n",
      "  %/classifier/attention/relu/Relu_output_0 = Relu(%/classifier/attention/linear1/Gemm_output_0)\n",
      "  %/classifier/attention/linear2/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/classifier/attention/relu/Relu_output_0, %classifier.attention.linear2.weight, %classifier.attention.linear2.bias)\n",
      "  %/classifier/attention/Constant_1_output_0 = Constant[value = <Tensor>]()\n",
      "  %/classifier/attention/Reshape_1_output_0 = Reshape[allowzero = 0](%/classifier/attention/linear2/Gemm_output_0, %/classifier/attention/Constant_1_output_0)\n",
      "  %/classifier/attention/Softmax_output_0 = Softmax[axis = 1](%/classifier/attention/Reshape_1_output_0)\n",
      "  %/classifier/attention/Constant_2_output_0 = Constant[value = <Tensor>]()\n",
      "  %/classifier/attention/Unsqueeze_output_0 = Unsqueeze(%/classifier/attention/Softmax_output_0, %/classifier/attention/Constant_2_output_0)\n",
      "  %/classifier/Mul_output_0 = Mul(%/Add_output_0, %/classifier/attention/Unsqueeze_output_0)\n",
      "  %onnx::ReduceSum_698 = Constant[value = <Tensor>]()\n",
      "  %/classifier/ReduceSum_output_0 = ReduceSum[keepdims = 0](%/classifier/Mul_output_0, %onnx::ReduceSum_698)\n",
      "  %/classifier/linear/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/classifier/ReduceSum_output_0, %classifier.linear.weight, %classifier.linear.bias)\n",
      "  %701 = LogSoftmax[axis = 1](%/classifier/linear/Gemm_output_0)\n",
      "  return %701\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnx.helper\n",
    "\n",
    "onnx_model = onnx.load('./constellator.onnx')\n",
    "print(onnx.helper.printable_graph(onnx_model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
