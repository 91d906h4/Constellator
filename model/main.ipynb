{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import jieba\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from data import Data\n",
    "from torch import nn, optim\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed.\n",
    "random_seed = 0\n",
    "\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    0: \"牡羊\",\n",
    "    1: \"金牛\",\n",
    "    2: \"雙子\",\n",
    "    3: \"巨蠍\",\n",
    "    4: \"獅子\",\n",
    "    5: \"處女\",\n",
    "    6: \"天秤\",\n",
    "    7: \"天蠍\",\n",
    "    8: \"射手\",\n",
    "    9: \"魔羯\",\n",
    "    10: \"水瓶\",\n",
    "    11: \"雙魚\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning completed.\n",
      "ToDataset completed.\n",
      "Argumantation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.593 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenlization completed.\n",
      "Padding completed.\n",
      "Token2id completed.\n",
      "Process completed.\n"
     ]
    }
   ],
   "source": [
    "raw = {i: open(f\"./dataset/{classes[i]}.txt\", encoding=\"utf-8\").read() for i in range(12)}\n",
    "data = Data(data=raw, padding_length=32)\n",
    "\n",
    "train_raw = data.get(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5627"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4273,\n",
       "  1555,\n",
       "  5312,\n",
       "  2569,\n",
       "  2128,\n",
       "  5213,\n",
       "  4714,\n",
       "  3737,\n",
       "  444,\n",
       "  3591,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, data: list, label: list):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index]), torch.tensor(self.label[index], dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, l = [], []\n",
    "\n",
    "for i, j in train_raw:\n",
    "    d.append(i); l.append(j)\n",
    "\n",
    "train_ds = CreateDataset(d, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5213, 4714, 3737,  444, 3591, 2128, 4273, 1555, 5312, 2569, 4432, 4432,\n",
       "         4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432,\n",
       "         4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432]),\n",
       " tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.linear1 = nn.Linear(256, 24)\n",
    "        self.linear2 = nn.Linear(24, 1)\n",
    "        self.relu = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        b = x.size(0)\n",
    "        x = x.reshape(-1, 256)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        x = x.reshape(b, -1)\n",
    "        x = torch.softmax(x, dim=1)\n",
    "\n",
    "        return x.unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionClassifier, self).__init__()\n",
    "        self.attention = Attention()\n",
    "        self.linear = nn.Linear(256, 12)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        attention = self.attention(x)\n",
    "\n",
    "        x = (x * attention).sum(dim=1)\n",
    "        x = torch.log_softmax(self.linear(x), dim=1)\n",
    "\n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(data.get(\"token_len\"), 64)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=64,\n",
    "            hidden_size=256,\n",
    "            num_layers=4,\n",
    "            dropout=0.5,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.linear = nn.Linear(16384, 12)\n",
    "        self.classifier = AttentionClassifier()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        x = x[:, :, :256] + x[:, :, :256]\n",
    "\n",
    "        x, _ = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traning history data.\n",
    "train_accuracy_h = []\n",
    "train_loss_h = []\n",
    "validate_accuracy_h = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs: int, model: nn.Module, optimizer: optim.Optimizer, loss: nn.CrossEntropyLoss, dataloader: DataLoader):\n",
    "    # Set model to training mode.\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_accuracy = 0\n",
    "        train_loss = 0\n",
    "        train_total = 0\n",
    "        train_process = 0\n",
    "        train_time = datetime.now().timestamp()\n",
    "\n",
    "        for texts, labels in dataloader:\n",
    "            texts: torch.Tensor\n",
    "            labels: torch.Tensor\n",
    "\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs: torch.Tensor = model(texts)\n",
    "            losses: torch.Tensor = loss(outputs, labels)\n",
    "\n",
    "            # optimizer.zero_grad()\n",
    "            for param in model.parameters(): param.grad = None\n",
    "\n",
    "            # Backpropagation.\n",
    "            losses.backward()\n",
    "\n",
    "            # Update parameters.\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predict = torch.max(outputs, 1)\n",
    "            train_accuracy += sum([labels[i][predict[i]] == 1 for i in range(len(predict))])\n",
    "            train_loss += losses.item()\n",
    "            train_total += labels.shape[0]\n",
    "            train_process += 1\n",
    "\n",
    "            print(\n",
    "                f\"{datetime.now().strftime('%Y/%m/%d %H:%M:%S')} \"\n",
    "                f\"Epoch: {epoch:03d} \"\n",
    "                f\"Time: {datetime.now().timestamp() - train_time:.2f} \"\n",
    "                f\"Process: {train_process / len(dataloader) * 100:.2f}% \"\n",
    "                f\"Accuracy: {train_accuracy / train_total * 100:.2f}% \"\n",
    "                f\"Loss: {train_loss:.3f}\",\n",
    "                end=\"\\r\"\n",
    "            )\n",
    "\n",
    "        train_accuracy_h.append(train_accuracy / train_total * 100)\n",
    "        train_loss_h.append(train_loss)\n",
    "\n",
    "        print()\n",
    "\n",
    "        # Early stop.\n",
    "        if train_accuracy / train_total > 0.95:\n",
    "            print(\"Early stopped.\")\n",
    "            break\n",
    "\n",
    "    # Set model to evaluation mode.\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024/04/01 17:59:06 Epoch: 000 Time: 15.32 Process: 100.00% Accuracy: 10.09% Loss: 1749.044\n",
      "2024/04/01 17:59:20 Epoch: 001 Time: 13.68 Process: 100.00% Accuracy: 11.23% Loss: 1743.776\n",
      "2024/04/01 17:59:34 Epoch: 002 Time: 13.93 Process: 100.00% Accuracy: 11.52% Loss: 1741.714\n",
      "2024/04/01 17:59:48 Epoch: 003 Time: 13.79 Process: 100.00% Accuracy: 12.46% Loss: 1735.675\n",
      "2024/04/01 18:00:04 Epoch: 004 Time: 15.76 Process: 100.00% Accuracy: 13.95% Loss: 1722.923\n",
      "2024/04/01 18:00:18 Epoch: 005 Time: 14.07 Process: 100.00% Accuracy: 17.04% Loss: 1673.418\n",
      "2024/04/01 18:00:34 Epoch: 006 Time: 16.41 Process: 100.00% Accuracy: 21.91% Loss: 1566.390\n",
      "2024/04/01 18:00:49 Epoch: 007 Time: 15.23 Process: 100.00% Accuracy: 31.12% Loss: 1418.244\n",
      "2024/04/01 18:01:04 Epoch: 008 Time: 14.98 Process: 100.00% Accuracy: 38.08% Loss: 1282.270\n",
      "2024/04/01 18:01:19 Epoch: 009 Time: 14.29 Process: 100.00% Accuracy: 44.43% Loss: 1150.651\n",
      "2024/04/01 18:01:33 Epoch: 010 Time: 14.34 Process: 100.00% Accuracy: 52.27% Loss: 993.300\n",
      "2024/04/01 18:01:48 Epoch: 011 Time: 14.86 Process: 100.00% Accuracy: 60.89% Loss: 816.023\n",
      "2024/04/01 18:02:02 Epoch: 012 Time: 14.58 Process: 100.00% Accuracy: 71.17% Loss: 617.543\n",
      "2024/04/01 18:02:16 Epoch: 013 Time: 14.02 Process: 100.00% Accuracy: 79.39% Loss: 439.890\n",
      "2024/04/01 18:02:31 Epoch: 014 Time: 14.47 Process: 100.00% Accuracy: 85.69% Loss: 308.605\n",
      "2024/04/01 18:02:45 Epoch: 015 Time: 14.20 Process: 100.00% Accuracy: 90.87% Loss: 200.005\n",
      "2024/04/01 18:02:59 Epoch: 016 Time: 14.28 Process: 100.00% Accuracy: 92.61% Loss: 159.303\n",
      "2024/04/01 18:03:13 Epoch: 017 Time: 14.11 Process: 100.00% Accuracy: 95.75% Loss: 96.231\n",
      "Early stopped.\n"
     ]
    }
   ],
   "source": [
    "train(epochs=epochs, model=model, optimizer=optimizer, loss=loss, dataloader=train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5269, 2867, 4689, 2962, 1035, 3909, 4689, 5439, 2129, 3287,  165, 4077,\n",
      "         3889, 5329, 2136, 4245, 3591, 4432, 4432, 4432, 4432, 4432, 4432, 4432,\n",
      "         4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432],\n",
      "        [5018, 2115, 2001, 1565, 1190, 2984, 3591, 4689, 1565, 2424, 3473, 4689,\n",
      "         3933, 4901, 3521, 1565, 3382, 3441, 4432, 4432, 4432, 4432, 4432, 4432,\n",
      "         4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432],\n",
      "        [1102, 4689, 1363, 3745, 3737,  602, 5374, 1043, 4749, 2895, 4077, 5248,\n",
      "         3591, 4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432,\n",
      "         4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432],\n",
      "        [4238, 2115, 4273, 1555, 5216, 1085, 3737, 3564, 3591, 2128, 3297,  208,\n",
      "         3564, 3737, 3369, 1436,  809, 4858, 3737, 5386, 4432, 4432, 4432, 4432,\n",
      "         4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432],\n",
      "        [3018, 4068, 4689, 3149, 4273, 2934, 1642, 3737, 3564, 4689, 1363, 3745,\n",
      "          479, 1830, 2228, 3737,   68, 4689, 1224, 1363, 3745, 3004, 1259, 4077,\n",
      "          602, 3010, 3078, 3454,  580, 3591, 4432, 4432],\n",
      "        [2926, 4689, 3584, 3908, 4613, 4689, 3908, 4173, 4689, 2709, 2609, 4689,\n",
      "         4798, 4908, 3117, 4689, 3129, 3737,  992, 2002, 4660, 3985, 1426, 3403,\n",
      "          516, 3336, 3591, 4689, 1137, 5127, 4689, 3441],\n",
      "        [2239, 4689,  334, 4689, 2962, 2842, 3966, 4689, 5297, 3591, 4689, 5422,\n",
      "         5235, 4689, 2980,  872, 4689, 5422, 3926, 1119, 3737,  718, 4689, 1922,\n",
      "         1328, 4689, 3441, 2709, 3660, 4689, 5422,  509],\n",
      "        [  77, 2397,  114, 4141, 4077, 2829, 5107, 3591, 4689, 3149, 3004,  602,\n",
      "         3010, 4295, 4829, 4459, 3909, 4689, 3018, 3737,  490, 3382, 4689, 3042,\n",
      "         4077, 3135, 3737,   68, 4432, 4432, 4432, 4432]])\n",
      "tensor([[-1.2010e+01, -2.7287e+01, -1.4348e+01, -1.6447e+01, -7.7486e-06,\n",
      "         -1.7910e+01, -2.5754e+01, -2.1535e+01, -2.5399e+01, -1.8095e+01,\n",
      "         -2.2465e+01, -1.3857e+01],\n",
      "        [-1.5026e+01, -1.2145e+01, -9.2144e+00, -1.2835e+01, -1.3187e+01,\n",
      "         -1.6787e+01, -1.1840e+01, -2.1129e+01, -6.6460e+00, -1.4282e+01,\n",
      "         -1.5377e-02, -4.2800e+00],\n",
      "        [-8.2540e+00, -1.6040e+01, -9.8840e+00, -1.6320e+01, -1.5182e+01,\n",
      "         -1.3194e+01, -1.2903e+01, -3.5399e-04, -1.0182e+01, -1.9424e+01,\n",
      "         -2.0123e+01, -1.7796e+01],\n",
      "        [-1.8706e+01, -1.7121e+01, -1.7142e+01, -1.0705e+01, -7.7784e+00,\n",
      "         -7.1894e+00, -2.0710e+01, -2.7414e+01, -1.7216e+01, -1.2102e+01,\n",
      "         -7.7139e+00, -1.6492e-03],\n",
      "        [-1.9234e+01, -2.6890e-04, -1.7100e+01, -1.1853e+01, -2.0708e+01,\n",
      "         -1.2298e+01, -9.7612e+00, -1.1835e+01, -1.4205e+01, -9.0664e+00,\n",
      "         -9.4838e+00, -1.8233e+01],\n",
      "        [-1.2673e+01, -1.7303e+01, -1.6009e-03, -2.5994e+01, -7.9538e+00,\n",
      "         -6.7137e+00, -1.7921e+01, -1.0387e+01, -2.0782e+01, -1.6989e+01,\n",
      "         -2.8065e+01, -1.8055e+01],\n",
      "        [-5.3057e+00, -1.9397e+00, -1.1968e+01, -1.4266e+01, -6.8820e+00,\n",
      "         -3.7715e+00, -1.2076e+01, -7.5905e+00, -9.9553e+00, -1.9038e-01,\n",
      "         -1.0439e+01, -1.6657e+01],\n",
      "        [-9.7934e+00, -9.9843e+00, -7.6653e+00, -7.7615e+00, -1.0873e+01,\n",
      "         -1.7980e+01, -1.1696e+01, -1.4718e+01, -6.8661e+00, -1.2215e+01,\n",
      "         -2.8200e-03, -7.2031e+00]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "for i, j in train_dl:\n",
    "    print(i)\n",
    "    print(model(i.to(device)))\n",
    "    print(j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = [\n",
    "    \"勇敢無畏，充滿了活力和冒險精神，他們喜歡追求挑戰，敢於冒險，常常是行動派的領導者。\",\n",
    "    \"穩重可靠，以堅韌的意志力和耐心著稱，他們注重安全和舒適，並對物質生活有著強烈的執著。\",\n",
    "    \"機智聰明，好奇心旺盛，喜歡交際和表達自己，具有多才多藝的特質，常常充滿了靈活的思維和活力。\",\n",
    "    \"情感豐富，善解人意，對家庭和親密關係非常重視，他們總是充滿了溫柔和關懷，是很好的傾聽者和支持者。\",\n",
    "    \"自信大方，追求著成為焦點的慾望，他們充滿了熱情和活力，喜歡引領和影響身邊的人，時常展現出優越感和領導能力。\",\n",
    "    \"細心謹慎，追求完美，他們善於分析和解決問題，注重細節和有組織性，常常是值得信賴的夥伴和顧問。\",\n",
    "    \"追求和諧，優雅而公正，他們注重平衡和公平，善於溝通協調，是很好的調解者和中介者。\",\n",
    "    \"神秘內斂，充滿了熱情和直覺，他們擁有強烈的意志力和洞察力，常常是充滿挑戰性和魅力的個體。\",\n",
    "    \"自由奔放，熱愛冒險和探索，他們追求著廣闊的視野和新鮮的體驗，時常充滿了樂觀和幽默。\",\n",
    "    \"勤奮負責，追求事業成功和社會地位，他們具有堅毅的意志力和耐心，常常是穩健和實際的決策者。\",\n",
    "    \"獨立思考，充滿了理想主義和創意，他們追求著獨特的生活方式和社會價值觀，常常是前衛和不拘一格的個體。\",\n",
    "    \"敏感善良，充滿了同情心和想像力，他們常常是理想主義者和夢想家，追求著內心的情感和精神實踐。\",\n",
    "    \"勇敢無畏，充滿活力，喜歡追求挑戰，常常是行動派的領導者。\",\n",
    "    \"穩重可靠，堅韌耐心，注重安全舒適，對物質生活有強烈執著。\",\n",
    "    \"機智聰明，好奇心旺盛，善於交際表達，充滿靈活思維和活力。\",\n",
    "    \"情感豐富，善解人意，重視家庭和親密關係，溫柔關懷，傾聽支持者。\",\n",
    "    \"自信大方，追求成為焦點，充滿熱情活力，喜歡引領影響身邊的人。\",\n",
    "    \"細心謹慎，追求完美，善於分析解決問題，注重細節有組織性。\",\n",
    "    \"追求和諧，優雅公正，注重平衡公平，善於溝通協調。\",\n",
    "    \"神秘內斂，熱情直覺，意志力洞察力強，充滿挑戰性和魅力。\",\n",
    "    \"自由奔放，熱愛冒險探索，追求廣闊視野和新鮮體驗，樂觀幽默。\",\n",
    "    \"勤奮負責，追求事業成功社會地位，具堅毅意志力和耐心，穩健決策者。\",\n",
    "    \"獨立思考，理想主義創意，追求獨特生活方式和價值觀，前衛不拘一格。\",\n",
    "    \"敏感善良，同情心想像力豐富，理想主義夢想家，追求內心情感精神實踐。\",\n",
    "    \"勇敢果敢，充滿活力，愛冒險。\",\n",
    "    \"穩重堅定，堅持自我價值觀。\",\n",
    "    \"靈活機智，善於溝通交際。\",\n",
    "    \"情感豐富，家庭意識強。\",\n",
    "    \"自信領導，熱情奔放。\",\n",
    "    \"細心謹慎，追求完美。\",\n",
    "    \"追求和諧，公正公平。\",\n",
    "    \"神秘敏感，探索深度。\",\n",
    "    \"自由探險，樂觀向上。\",\n",
    "    \"勤奮穩健，追求成功。\",\n",
    "    \"獨立創新，理想主義者。\",\n",
    "    \"敏感浪漫，夢想家。\",\n",
    "]\n",
    "\n",
    "answare = [\n",
    "    '牡羊', '金牛', '雙子', '巨蠍', '獅子', '處女', '天秤', '天蠍', '射手', '魔羯', '水瓶', '雙魚',\n",
    "    '牡羊', '金牛', '雙子', '巨蠍', '獅子', '處女', '天秤', '天蠍', '射手', '魔羯', '水瓶', '雙魚',\n",
    "    '牡羊', '金牛', '雙子', '巨蠍', '獅子', '處女', '天秤', '天蠍', '射手', '魔羯', '水瓶', '雙魚',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(testset: list) -> torch.Tensor:\n",
    "    result = []\n",
    "\n",
    "    for line in testset:\n",
    "        temp = jieba.lcut(line)\n",
    "        temp = temp + [\"<PAD>\"] * (32 - len(temp))\n",
    "        temp = [data.w2i[x] if x in data.w2i else data.w2i[\"<PAD>\"] for x in temp][:32]\n",
    "        result.append(temp)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = process(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding): Embedding(5470, 64)\n",
       "  (lstm): LSTM(64, 256, num_layers=4, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (linear): Linear(in_features=16384, out_features=12, bias=True)\n",
       "  (classifier): AttentionClassifier(\n",
       "    (attention): Attention(\n",
       "      (linear1): Linear(in_features=256, out_features=24, bias=True)\n",
       "      (linear2): Linear(in_features=24, out_features=1, bias=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (linear): Linear(in_features=256, out_features=12, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-9.5851e-03, -1.6574e+01, -1.0449e+01, -1.9204e+01, -7.8254e+00,\n",
      "         -1.1585e+01, -1.7886e+01, -1.7001e+01, -4.6998e+00, -1.6646e+01,\n",
      "         -1.2482e+01, -1.4076e+01],\n",
      "        [-5.6847e+00, -2.8154e+00, -1.1720e+01, -6.2457e+00, -8.2063e+00,\n",
      "         -3.2011e+00, -5.8260e+00, -4.1979e-01, -5.6571e+00, -1.4714e+00,\n",
      "         -7.5402e+00, -1.1248e+01],\n",
      "        [-6.9645e+00, -1.0732e+01, -9.5546e-02, -1.3814e+01, -6.8981e+00,\n",
      "         -6.3431e+00, -3.0260e+00, -9.5706e+00, -3.6247e+00, -1.7829e+01,\n",
      "         -1.1208e+01, -4.4115e+00],\n",
      "        [-1.7589e+01, -1.3601e+00, -8.8568e+00, -3.4013e+00, -1.4687e+01,\n",
      "         -1.0287e+01, -6.4061e-01, -1.2804e+01, -8.5628e+00, -1.2223e+01,\n",
      "         -1.7626e+00, -4.5022e+00],\n",
      "        [-1.5684e-02, -1.3361e+01, -6.5870e+00, -1.2985e+01, -5.9094e+00,\n",
      "         -1.2427e+01, -1.5508e+01, -1.1656e+01, -4.4903e+00, -1.4023e+01,\n",
      "         -8.4171e+00, -1.1201e+01],\n",
      "        [-8.1930e+00, -8.2090e-02, -1.1450e+01, -1.5449e+01, -1.1717e+01,\n",
      "         -2.7841e+00, -6.0273e+00, -1.4302e+01, -4.4629e+00, -7.1106e+00,\n",
      "         -6.2359e+00, -1.1286e+01],\n",
      "        [-2.9007e+01, -1.2986e+01, -1.5571e+01, -1.4534e+01, -2.2707e+01,\n",
      "         -1.4899e+01, -5.5788e-05, -1.9480e+01, -1.5474e+01, -2.5247e+01,\n",
      "         -1.6636e+01, -9.8599e+00],\n",
      "        [-8.6521e-01, -1.2679e+01, -1.2170e+00, -1.4546e+01, -8.7712e+00,\n",
      "         -1.0732e+01, -1.0462e+01, -6.2340e+00, -1.2707e+00, -1.8454e+01,\n",
      "         -1.0507e+01, -9.6670e+00],\n",
      "        [-7.6280e+00, -1.3954e+01, -1.0182e+01, -2.0411e+01, -1.7579e+01,\n",
      "         -1.6903e+01, -1.1748e+01, -1.6259e+01, -8.9367e-04, -2.0531e+01,\n",
      "         -7.9329e+00, -1.3728e+01],\n",
      "        [-6.0730e+00, -3.6837e+00, -9.1369e+00, -8.0902e+00, -7.9417e+00,\n",
      "         -7.3759e+00, -7.2426e+00, -1.1534e+01, -2.1964e+00, -5.4394e+00,\n",
      "         -1.5901e-01, -6.2629e+00],\n",
      "        [-1.3129e+01, -9.9997e+00, -1.4246e+01, -1.0765e+01, -1.6748e+01,\n",
      "         -1.7837e+01, -1.6545e+01, -1.9692e+01, -7.8879e+00, -1.2316e+01,\n",
      "         -4.8506e-04, -1.0234e+01],\n",
      "        [-1.9625e+01, -1.5182e+01, -1.3053e+01, -7.0122e+00, -1.3715e+01,\n",
      "         -1.3402e+01, -1.7617e+01, -2.2657e+01, -1.3202e+01, -1.7773e+01,\n",
      "         -3.8233e+00, -2.3027e-02],\n",
      "        [-1.3530e-02, -1.6358e+01, -1.0193e+01, -2.2146e+01, -1.0890e+01,\n",
      "         -1.3435e+01, -1.8652e+01, -1.6511e+01, -4.3140e+00, -1.8137e+01,\n",
      "         -1.3238e+01, -1.7063e+01],\n",
      "        [-1.6800e+01, -1.7684e+00, -1.8150e+01, -2.2860e+00, -1.5265e+01,\n",
      "         -1.0089e+01, -1.2780e+01, -1.2804e+01, -1.2199e+01, -1.2936e+00,\n",
      "         -7.9108e-01, -1.0332e+01],\n",
      "        [-1.4477e+01, -1.2619e+01, -2.8002e+00, -1.3242e+01, -8.0112e+00,\n",
      "         -4.6844e+00, -3.6404e+00, -1.2821e+01, -8.1072e+00, -1.8483e+01,\n",
      "         -1.2134e+01, -1.0195e-01],\n",
      "        [-2.3353e+01, -4.6582e+00, -1.7442e+01, -1.9899e-01, -1.7649e+01,\n",
      "         -1.5285e+01, -1.0601e+01, -1.8538e+01, -1.7714e+01, -1.0081e+01,\n",
      "         -1.7696e+00, -7.6046e+00],\n",
      "        [-2.6275e-02, -1.2903e+01, -4.8770e+00, -1.8230e+01, -4.3307e+00,\n",
      "         -1.0511e+01, -1.6682e+01, -1.3864e+01, -5.2832e+00, -1.2899e+01,\n",
      "         -1.0111e+01, -1.2298e+01],\n",
      "        [-1.6032e+01, -4.6972e+00, -1.5904e+01, -1.3956e+01, -1.3867e+01,\n",
      "         -1.0118e-02, -9.6152e+00, -2.0636e+01, -1.0459e+01, -1.0141e+01,\n",
      "         -9.1309e+00, -7.2636e+00],\n",
      "        [-3.1308e+01, -1.2656e+01, -1.8423e+01, -1.5731e+01, -2.5526e+01,\n",
      "         -1.6201e+01, -3.8147e-06, -1.8929e+01, -1.9083e+01, -2.5978e+01,\n",
      "         -2.0412e+01, -1.4723e+01],\n",
      "        [-5.4485e+00, -1.0953e+01, -1.9015e-01, -1.3509e+01, -7.8759e+00,\n",
      "         -3.8108e+00, -6.4405e+00, -2.9324e+00, -2.4502e+00, -1.7289e+01,\n",
      "         -1.3156e+01, -5.2602e+00],\n",
      "        [-7.0526e+00, -9.0072e+00, -1.1415e+01, -1.8300e+01, -1.6133e+01,\n",
      "         -1.5278e+01, -1.2813e+01, -1.6126e+01, -2.5260e-01, -1.1903e+01,\n",
      "         -1.5041e+00, -1.2966e+01],\n",
      "        [-1.2619e+01, -4.7058e+00, -1.6764e+01, -1.0302e+01, -1.1875e+01,\n",
      "         -1.0608e+01, -1.4386e+01, -1.6726e+01, -9.4782e+00, -8.8950e-01,\n",
      "         -5.4484e-01, -1.0997e+01],\n",
      "        [-1.4081e+01, -7.0252e+00, -1.4594e+01, -7.7617e+00, -1.7269e+01,\n",
      "         -1.6060e+01, -1.4753e+01, -1.8124e+01, -8.6601e+00, -1.1179e+01,\n",
      "         -1.5514e-03, -9.9806e+00],\n",
      "        [-2.2079e+01, -1.8285e+01, -1.4564e+01, -2.0267e+00, -1.2544e+01,\n",
      "         -1.4508e+01, -1.6559e+01, -1.9035e+01, -1.7735e+01, -1.9400e+01,\n",
      "         -8.2279e+00, -1.4161e-01],\n",
      "        [-5.6279e+00, -1.6222e+01, -3.2431e+00, -1.9080e+01, -7.0595e-02,\n",
      "         -3.6980e+00, -1.8176e+01, -1.6955e+01, -1.4106e+01, -1.4535e+01,\n",
      "         -1.7301e+01, -7.1969e+00],\n",
      "        [-6.8462e+00, -5.3522e+00, -7.4389e+00, -7.4050e+00, -1.0912e+01,\n",
      "         -7.8604e+00, -3.0648e+00, -7.4279e-02, -4.0574e+00, -1.0440e+01,\n",
      "         -8.7926e+00, -9.9342e+00],\n",
      "        [-6.8892e+00, -1.3556e+01, -2.5674e-01, -9.9223e+00, -1.5700e+00,\n",
      "         -1.0976e+01, -6.9999e+00, -7.2263e+00, -7.2508e+00, -1.5672e+01,\n",
      "         -1.0716e+01, -4.2047e+00],\n",
      "        [-1.1813e+01, -1.2816e+01, -1.3926e+01, -3.9144e-03, -9.0439e+00,\n",
      "         -1.1111e+01, -1.0305e+01, -8.5558e+00, -1.1541e+01, -1.1175e+01,\n",
      "         -7.9286e+00, -5.7595e+00],\n",
      "        [-5.8496e-02, -1.5764e+01, -4.6892e+00, -1.3067e+01, -3.0808e+00,\n",
      "         -1.1151e+01, -1.3615e+01, -7.1995e+00, -7.0257e+00, -1.4736e+01,\n",
      "         -1.2535e+01, -1.0020e+01],\n",
      "        [-1.4156e+00, -4.3921e+00, -6.9846e+00, -1.5213e+01, -7.3200e+00,\n",
      "         -5.0826e-01, -1.0325e+01, -8.9711e+00, -1.9645e+00, -7.0494e+00,\n",
      "         -7.8645e+00, -9.7269e+00],\n",
      "        [-1.4411e+01, -7.2292e+00, -4.6041e+00, -7.2370e+00, -1.1925e+01,\n",
      "         -1.1022e+01, -1.7274e-02, -7.2843e+00, -8.4840e+00, -1.6482e+01,\n",
      "         -8.4978e+00, -5.3925e+00],\n",
      "        [-1.4383e+01, -7.3878e+00, -3.6254e-01, -1.4379e+01, -1.1990e+01,\n",
      "         -1.6982e+00, -2.5909e+00, -3.1039e+00, -1.0563e+01, -1.6944e+01,\n",
      "         -1.7549e+01, -7.4279e+00],\n",
      "        [-5.4655e+00, -1.2565e+01, -1.0417e+01, -1.9442e+01, -1.4716e+01,\n",
      "         -7.4681e+00, -1.2857e+01, -1.3937e+01, -4.8957e-03, -1.7399e+01,\n",
      "         -1.0457e+01, -1.1022e+01],\n",
      "        [-1.0881e+01, -1.2934e+00, -1.3163e+01, -9.7128e+00, -1.1787e+01,\n",
      "         -9.3398e+00, -1.2766e+01, -1.7247e+01, -1.0478e+01, -2.0110e+00,\n",
      "         -5.2495e-01, -1.1116e+01],\n",
      "        [-1.9518e+00, -8.5814e+00, -5.5386e+00, -2.2235e+00, -3.8171e+00,\n",
      "         -1.0428e+01, -8.1108e+00, -6.6778e+00, -2.4167e+00, -7.5123e+00,\n",
      "         -5.0928e-01, -3.4622e+00],\n",
      "        [-1.5496e+01, -1.0670e+01, -6.5389e+00, -4.8239e+00, -8.5311e+00,\n",
      "         -3.4322e+00, -6.2472e+00, -7.2450e+00, -1.1586e+01, -1.4050e+01,\n",
      "         -1.0931e+01, -4.5726e-02]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "result = model(torch.tensor(test).to(device))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normolization\n",
    "results = []\n",
    "\n",
    "for i in range(len(result)):\n",
    "    temp = result[i]\n",
    "    temp = [x - min(temp) for x in temp]\n",
    "    temp = [x / max(temp) for x in temp]\n",
    "    temp = [round(x, 3) for x in temp]\n",
    "\n",
    "    results.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "牡羊✅\n",
      "天蠍 <- 金牛\n",
      "雙子✅\n",
      "天秤 <- 巨蠍\n",
      "牡羊 <- 獅子\n",
      "金牛 <- 處女\n",
      "天秤✅\n",
      "牡羊 <- 天蠍\n",
      "射手✅\n",
      "水瓶 <- 魔羯\n",
      "水瓶✅\n",
      "雙魚✅\n",
      "牡羊✅\n",
      "水瓶 <- 金牛\n",
      "雙魚 <- 雙子\n",
      "巨蠍✅\n",
      "牡羊 <- 獅子\n",
      "處女✅\n",
      "天秤✅\n",
      "雙子 <- 天蠍\n",
      "射手✅\n",
      "水瓶 <- 魔羯\n",
      "水瓶✅\n",
      "雙魚✅\n",
      "獅子 <- 牡羊\n",
      "天蠍 <- 金牛\n",
      "雙子✅\n",
      "巨蠍✅\n",
      "牡羊 <- 獅子\n",
      "處女✅\n",
      "天秤✅\n",
      "雙子 <- 天蠍\n",
      "射手✅\n",
      "水瓶 <- 魔羯\n",
      "水瓶✅\n",
      "雙魚✅\n",
      "Correct: 20 (55.56%)\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    t = classes[int(torch.argmax(torch.tensor(r)))]\n",
    "    if t == answare[i]:\n",
    "        count += 1\n",
    "        print(t + \"✅\")\n",
    "    else:\n",
    "        print(t + \" <- \" + answare[i])\n",
    "\n",
    "print(f\"Correct: {count} ({(count / len(answare)) * 100 :.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# angles_A = np.linspace(start=0, stop=2*np.pi, num=len(result)+1, endpoint=True)\n",
    "# values_A = np.concatenate((result, [result[0]]))\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(5, 5), subplot_kw={'projection': 'polar'})\n",
    "# ax.plot(angles_A, values_A, 'o-', color=\"blue\", label=\"A\")\n",
    "\n",
    "# ax.fill(angles_A, values_A, alpha=0.3, color=\"blue\")\n",
    "# ax.set_thetagrids(angles_A[:-1] * 180 / np.pi, range(12), fontsize=15)\n",
    "# ax.set_theta_zero_location('N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:4315: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\onnx\\_internal\\jit_utils.py:258: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\jit\\passes\\onnx\\shape_type_inference.cpp:1888.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "c:\\Users\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\onnx\\utils.py:687: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\jit\\passes\\onnx\\shape_type_inference.cpp:1888.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "c:\\Users\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\onnx\\utils.py:1178: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\jit\\passes\\onnx\\shape_type_inference.cpp:1888.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "# Please check the virtual input of the ONNX model.\n",
    "torch.onnx.export(model, torch.tensor([test[0]]).to(device=device), 'constellator.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph torch_jit (\n",
      "  %input.1[INT64, 1x32]\n",
      ") initializers (\n",
      "  %embedding.weight[FLOAT, 5470x64]\n",
      "  %classifier.attention.linear1.weight[FLOAT, 24x256]\n",
      "  %classifier.attention.linear1.bias[FLOAT, 24]\n",
      "  %classifier.attention.linear2.weight[FLOAT, 1x24]\n",
      "  %classifier.attention.linear2.bias[FLOAT, 1]\n",
      "  %classifier.linear.weight[FLOAT, 12x256]\n",
      "  %classifier.linear.bias[FLOAT, 12]\n",
      "  %onnx::LSTM_742[FLOAT, 2x2048]\n",
      "  %onnx::LSTM_743[FLOAT, 2x1024x64]\n",
      "  %onnx::LSTM_744[FLOAT, 2x1024x256]\n",
      "  %onnx::LSTM_789[FLOAT, 2x2048]\n",
      "  %onnx::LSTM_790[FLOAT, 2x1024x512]\n",
      "  %onnx::LSTM_791[FLOAT, 2x1024x256]\n",
      "  %onnx::LSTM_836[FLOAT, 2x2048]\n",
      "  %onnx::LSTM_837[FLOAT, 2x1024x512]\n",
      "  %onnx::LSTM_838[FLOAT, 2x1024x256]\n",
      "  %onnx::LSTM_883[FLOAT, 2x2048]\n",
      "  %onnx::LSTM_884[FLOAT, 2x1024x512]\n",
      "  %onnx::LSTM_885[FLOAT, 2x1024x256]\n",
      ") {\n",
      "  %/embedding/Gather_output_0 = Gather(%embedding.weight, %input.1)\n",
      "  %/lstm/Transpose_output_0 = Transpose[perm = [1, 0, 2]](%/embedding/Gather_output_0)\n",
      "  %/lstm/Constant_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Constant_1_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Shape_output_0 = Shape(%/lstm/Transpose_output_0)\n",
      "  %/lstm/Constant_2_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_output_0 = Gather(%/lstm/Shape_output_0, %/lstm/Constant_2_output_0)\n",
      "  %onnx::Unsqueeze_179 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_180 = Unsqueeze(%/lstm/Gather_output_0, %onnx::Unsqueeze_179)\n",
      "  %/lstm/Constant_3_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_747 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_output_0 = Concat[axis = 0](%onnx::Concat_747, %onnx::Concat_180, %/lstm/Constant_3_output_0)\n",
      "  %/lstm/Expand_output_0 = Expand(%/lstm/Constant_output_0, %/lstm/Concat_output_0)\n",
      "  %/lstm/Shape_1_output_0 = Shape(%/lstm/Transpose_output_0)\n",
      "  %/lstm/Constant_4_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_1_output_0 = Gather(%/lstm/Shape_1_output_0, %/lstm/Constant_4_output_0)\n",
      "  %onnx::Unsqueeze_190 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_191 = Unsqueeze(%/lstm/Gather_1_output_0, %onnx::Unsqueeze_190)\n",
      "  %/lstm/Constant_5_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_748 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_1_output_0 = Concat[axis = 0](%onnx::Concat_748, %onnx::Concat_191, %/lstm/Constant_5_output_0)\n",
      "  %/lstm/Expand_1_output_0 = Expand(%/lstm/Constant_1_output_0, %/lstm/Concat_1_output_0)\n",
      "  %/lstm/LSTM_output_0, %/lstm/LSTM_output_1, %/lstm/LSTM_output_2 = LSTM[direction = 'bidirectional', hidden_size = 256](%/lstm/Transpose_output_0, %onnx::LSTM_743, %onnx::LSTM_744, %onnx::LSTM_742, %, %/lstm/Expand_output_0, %/lstm/Expand_1_output_0)\n",
      "  %/lstm/Transpose_1_output_0 = Transpose[perm = [0, 2, 1, 3]](%/lstm/LSTM_output_0)\n",
      "  %/lstm/Constant_6_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Reshape_output_0 = Reshape[allowzero = 0](%/lstm/Transpose_1_output_0, %/lstm/Constant_6_output_0)\n",
      "  %/lstm/Constant_7_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Constant_8_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Shape_2_output_0 = Shape(%/lstm/Reshape_output_0)\n",
      "  %/lstm/Constant_9_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_2_output_0 = Gather(%/lstm/Shape_2_output_0, %/lstm/Constant_9_output_0)\n",
      "  %onnx::Unsqueeze_336 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_337 = Unsqueeze(%/lstm/Gather_2_output_0, %onnx::Unsqueeze_336)\n",
      "  %/lstm/Constant_10_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_794 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_2_output_0 = Concat[axis = 0](%onnx::Concat_794, %onnx::Concat_337, %/lstm/Constant_10_output_0)\n",
      "  %/lstm/Expand_2_output_0 = Expand(%/lstm/Constant_7_output_0, %/lstm/Concat_2_output_0)\n",
      "  %/lstm/Shape_3_output_0 = Shape(%/lstm/Reshape_output_0)\n",
      "  %/lstm/Constant_11_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_3_output_0 = Gather(%/lstm/Shape_3_output_0, %/lstm/Constant_11_output_0)\n",
      "  %onnx::Unsqueeze_347 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_348 = Unsqueeze(%/lstm/Gather_3_output_0, %onnx::Unsqueeze_347)\n",
      "  %/lstm/Constant_12_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_795 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_3_output_0 = Concat[axis = 0](%onnx::Concat_795, %onnx::Concat_348, %/lstm/Constant_12_output_0)\n",
      "  %/lstm/Expand_3_output_0 = Expand(%/lstm/Constant_8_output_0, %/lstm/Concat_3_output_0)\n",
      "  %/lstm/LSTM_1_output_0, %/lstm/LSTM_1_output_1, %/lstm/LSTM_1_output_2 = LSTM[direction = 'bidirectional', hidden_size = 256](%/lstm/Reshape_output_0, %onnx::LSTM_790, %onnx::LSTM_791, %onnx::LSTM_789, %, %/lstm/Expand_2_output_0, %/lstm/Expand_3_output_0)\n",
      "  %/lstm/Transpose_2_output_0 = Transpose[perm = [0, 2, 1, 3]](%/lstm/LSTM_1_output_0)\n",
      "  %/lstm/Constant_13_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Reshape_1_output_0 = Reshape[allowzero = 0](%/lstm/Transpose_2_output_0, %/lstm/Constant_13_output_0)\n",
      "  %/lstm/Constant_14_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Constant_15_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Shape_4_output_0 = Shape(%/lstm/Reshape_1_output_0)\n",
      "  %/lstm/Constant_16_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_4_output_0 = Gather(%/lstm/Shape_4_output_0, %/lstm/Constant_16_output_0)\n",
      "  %onnx::Unsqueeze_493 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_494 = Unsqueeze(%/lstm/Gather_4_output_0, %onnx::Unsqueeze_493)\n",
      "  %/lstm/Constant_17_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_841 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_4_output_0 = Concat[axis = 0](%onnx::Concat_841, %onnx::Concat_494, %/lstm/Constant_17_output_0)\n",
      "  %/lstm/Expand_4_output_0 = Expand(%/lstm/Constant_14_output_0, %/lstm/Concat_4_output_0)\n",
      "  %/lstm/Shape_5_output_0 = Shape(%/lstm/Reshape_1_output_0)\n",
      "  %/lstm/Constant_18_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_5_output_0 = Gather(%/lstm/Shape_5_output_0, %/lstm/Constant_18_output_0)\n",
      "  %onnx::Unsqueeze_504 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_505 = Unsqueeze(%/lstm/Gather_5_output_0, %onnx::Unsqueeze_504)\n",
      "  %/lstm/Constant_19_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_842 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_5_output_0 = Concat[axis = 0](%onnx::Concat_842, %onnx::Concat_505, %/lstm/Constant_19_output_0)\n",
      "  %/lstm/Expand_5_output_0 = Expand(%/lstm/Constant_15_output_0, %/lstm/Concat_5_output_0)\n",
      "  %/lstm/LSTM_2_output_0, %/lstm/LSTM_2_output_1, %/lstm/LSTM_2_output_2 = LSTM[direction = 'bidirectional', hidden_size = 256](%/lstm/Reshape_1_output_0, %onnx::LSTM_837, %onnx::LSTM_838, %onnx::LSTM_836, %, %/lstm/Expand_4_output_0, %/lstm/Expand_5_output_0)\n",
      "  %/lstm/Transpose_3_output_0 = Transpose[perm = [0, 2, 1, 3]](%/lstm/LSTM_2_output_0)\n",
      "  %/lstm/Constant_20_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Reshape_2_output_0 = Reshape[allowzero = 0](%/lstm/Transpose_3_output_0, %/lstm/Constant_20_output_0)\n",
      "  %/lstm/Constant_21_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Constant_22_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Shape_6_output_0 = Shape(%/lstm/Reshape_2_output_0)\n",
      "  %/lstm/Constant_23_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_6_output_0 = Gather(%/lstm/Shape_6_output_0, %/lstm/Constant_23_output_0)\n",
      "  %onnx::Unsqueeze_650 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_651 = Unsqueeze(%/lstm/Gather_6_output_0, %onnx::Unsqueeze_650)\n",
      "  %/lstm/Constant_24_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_888 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_6_output_0 = Concat[axis = 0](%onnx::Concat_888, %onnx::Concat_651, %/lstm/Constant_24_output_0)\n",
      "  %/lstm/Expand_6_output_0 = Expand(%/lstm/Constant_21_output_0, %/lstm/Concat_6_output_0)\n",
      "  %/lstm/Shape_7_output_0 = Shape(%/lstm/Reshape_2_output_0)\n",
      "  %/lstm/Constant_25_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_7_output_0 = Gather(%/lstm/Shape_7_output_0, %/lstm/Constant_25_output_0)\n",
      "  %onnx::Unsqueeze_661 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_662 = Unsqueeze(%/lstm/Gather_7_output_0, %onnx::Unsqueeze_661)\n",
      "  %/lstm/Constant_26_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_889 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_7_output_0 = Concat[axis = 0](%onnx::Concat_889, %onnx::Concat_662, %/lstm/Constant_26_output_0)\n",
      "  %/lstm/Expand_7_output_0 = Expand(%/lstm/Constant_22_output_0, %/lstm/Concat_7_output_0)\n",
      "  %/lstm/LSTM_3_output_0, %/lstm/LSTM_3_output_1, %/lstm/LSTM_3_output_2 = LSTM[direction = 'bidirectional', hidden_size = 256](%/lstm/Reshape_2_output_0, %onnx::LSTM_884, %onnx::LSTM_885, %onnx::LSTM_883, %, %/lstm/Expand_6_output_0, %/lstm/Expand_7_output_0)\n",
      "  %/lstm/Transpose_4_output_0 = Transpose[perm = [0, 2, 1, 3]](%/lstm/LSTM_3_output_0)\n",
      "  %/lstm/Constant_27_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Reshape_3_output_0 = Reshape[allowzero = 0](%/lstm/Transpose_4_output_0, %/lstm/Constant_27_output_0)\n",
      "  %/lstm/Transpose_5_output_0 = Transpose[perm = [1, 0, 2]](%/lstm/Reshape_3_output_0)\n",
      "  %/Constant_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Constant_1_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Constant_2_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Constant_3_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Slice_output_0 = Slice(%/lstm/Transpose_5_output_0, %/Constant_1_output_0, %/Constant_2_output_0, %/Constant_output_0, %/Constant_3_output_0)\n",
      "  %/Add_output_0 = Add(%/Slice_output_0, %/Slice_output_0)\n",
      "  %/classifier/attention/Constant_output_0 = Constant[value = <Tensor>]()\n",
      "  %/classifier/attention/Reshape_output_0 = Reshape[allowzero = 0](%/Add_output_0, %/classifier/attention/Constant_output_0)\n",
      "  %/classifier/attention/linear1/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/classifier/attention/Reshape_output_0, %classifier.attention.linear1.weight, %classifier.attention.linear1.bias)\n",
      "  %/classifier/attention/relu/Relu_output_0 = Relu(%/classifier/attention/linear1/Gemm_output_0)\n",
      "  %/classifier/attention/linear2/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/classifier/attention/relu/Relu_output_0, %classifier.attention.linear2.weight, %classifier.attention.linear2.bias)\n",
      "  %/classifier/attention/Constant_1_output_0 = Constant[value = <Tensor>]()\n",
      "  %/classifier/attention/Reshape_1_output_0 = Reshape[allowzero = 0](%/classifier/attention/linear2/Gemm_output_0, %/classifier/attention/Constant_1_output_0)\n",
      "  %/classifier/attention/Softmax_output_0 = Softmax[axis = 1](%/classifier/attention/Reshape_1_output_0)\n",
      "  %/classifier/attention/Constant_2_output_0 = Constant[value = <Tensor>]()\n",
      "  %/classifier/attention/Unsqueeze_output_0 = Unsqueeze(%/classifier/attention/Softmax_output_0, %/classifier/attention/Constant_2_output_0)\n",
      "  %/classifier/Mul_output_0 = Mul(%/Add_output_0, %/classifier/attention/Unsqueeze_output_0)\n",
      "  %onnx::ReduceSum_698 = Constant[value = <Tensor>]()\n",
      "  %/classifier/ReduceSum_output_0 = ReduceSum[keepdims = 0](%/classifier/Mul_output_0, %onnx::ReduceSum_698)\n",
      "  %/classifier/linear/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/classifier/ReduceSum_output_0, %classifier.linear.weight, %classifier.linear.bias)\n",
      "  %701 = LogSoftmax[axis = 1](%/classifier/linear/Gemm_output_0)\n",
      "  return %701\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnx.helper\n",
    "\n",
    "onnx_model = onnx.load('./constellator.onnx')\n",
    "print(onnx.helper.printable_graph(onnx_model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
