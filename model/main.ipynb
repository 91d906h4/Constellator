{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files (x86)\\Anaconda\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import jieba\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from data import Data\n",
    "from torch import nn, optim\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed.\n",
    "random_seed = 0\n",
    "\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    0: \"牡羊\",\n",
    "    1: \"金牛\",\n",
    "    2: \"雙子\",\n",
    "    3: \"巨蠍\",\n",
    "    4: \"獅子\",\n",
    "    5: \"處女\",\n",
    "    6: \"天秤\",\n",
    "    7: \"天蠍\",\n",
    "    8: \"射手\",\n",
    "    9: \"魔羯\",\n",
    "    10: \"水瓶\",\n",
    "    11: \"雙魚\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\mamiy\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning completed.\n",
      "ToDataset completed.\n",
      "Argumantation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.338 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenlization completed.\n",
      "Padding completed.\n",
      "Token2id completed.\n",
      "Process completed.\n"
     ]
    }
   ],
   "source": [
    "raw = {i: open(f\"./dataset/{classes[i]}.txt\", \"r\", encoding='UTF-8').read() for i in range(12)}\n",
    "data = Data(data=raw, padding_length=32)\n",
    "\n",
    "train_raw = data.get(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5627"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3411,\n",
       "  5403,\n",
       "  1156,\n",
       "  2523,\n",
       "  214,\n",
       "  2792,\n",
       "  50,\n",
       "  1284,\n",
       "  2769,\n",
       "  3671,\n",
       "  4911,\n",
       "  4911,\n",
       "  4911,\n",
       "  4911,\n",
       "  4911,\n",
       "  4911,\n",
       "  4911,\n",
       "  4911,\n",
       "  4911,\n",
       "  4911,\n",
       "  4911,\n",
       "  4911,\n",
       "  4911,\n",
       "  4911,\n",
       "  4911,\n",
       "  4911,\n",
       "  4911,\n",
       "  4911,\n",
       "  4911,\n",
       "  4911,\n",
       "  4911,\n",
       "  4911],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, data: list, label: list):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index]), torch.tensor(self.label[index], dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, l = [], []\n",
    "\n",
    "for i, j in train_raw:\n",
    "    d.append(i); l.append(j)\n",
    "\n",
    "train_ds = CreateDataset(d, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2792,   50, 1284, 2769, 3671,  214, 3411, 5403, 1156, 2523, 4911, 4911,\n",
       "         4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911,\n",
       "         4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911]),\n",
       " tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.linear1 = nn.Linear(256, 64)\n",
    "        self.linear2 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        b = x.size(0)\n",
    "        x = x.reshape(-1, 256)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        x = x.reshape(b, -1)\n",
    "        x = torch.softmax(x, dim=1)\n",
    "\n",
    "        return x.unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionClassifier, self).__init__()\n",
    "        self.attention = Attention()\n",
    "        self.linear = nn.Linear(256, 12)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        attention = self.attention(x)\n",
    "\n",
    "        x = (x * attention).sum(dim=1)\n",
    "        x = torch.log_softmax(self.linear(x), dim=1)\n",
    "\n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(data.get(\"token_len\"), 64)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=64,\n",
    "            hidden_size=256,\n",
    "            num_layers=4,\n",
    "            dropout=0.5,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.linear = nn.Linear(16384, 12)\n",
    "        self.classifier = AttentionClassifier()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        x = x[:, :, :256] + x[:, :, :256]\n",
    "\n",
    "        x, _ = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traning history data.\n",
    "train_accuracy_h = []\n",
    "train_loss_h = []\n",
    "validate_accuracy_h = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs: int, model: nn.Module, optimizer: optim.Optimizer, loss: nn.CrossEntropyLoss, dataloader: DataLoader):\n",
    "    # Set model to training mode.\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_accuracy = 0\n",
    "        train_loss = 0\n",
    "        train_total = 0\n",
    "        train_process = 0\n",
    "        train_time = datetime.now().timestamp()\n",
    "\n",
    "        for texts, labels in dataloader:\n",
    "            texts: torch.Tensor\n",
    "            labels: torch.Tensor\n",
    "\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs: torch.Tensor = model(texts)\n",
    "            losses: torch.Tensor = loss(outputs, labels)\n",
    "\n",
    "            # optimizer.zero_grad()\n",
    "            for param in model.parameters(): param.grad = None\n",
    "\n",
    "            # Backpropagation.\n",
    "            losses.backward()\n",
    "\n",
    "            # Update parameters.\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predict = torch.max(outputs, 1)\n",
    "            train_accuracy += sum([labels[i][predict[i]] == 1 for i in range(len(predict))])\n",
    "            train_loss += losses.item()\n",
    "            train_total += labels.shape[0]\n",
    "            train_process += 1\n",
    "\n",
    "            print(\n",
    "                f\"{datetime.now().strftime('%Y/%m/%d %H:%M:%S')} \"\n",
    "                f\"Epoch: {epoch:03d} \"\n",
    "                f\"Time: {datetime.now().timestamp() - train_time:.2f} \"\n",
    "                f\"Process: {train_process / len(dataloader) * 100:.2f}% \"\n",
    "                f\"Accuracy: {train_accuracy / train_total * 100:.2f}% \"\n",
    "                f\"Loss: {train_loss:.3f}\",\n",
    "                end=\"\\r\"\n",
    "            )\n",
    "\n",
    "        train_accuracy_h.append(train_accuracy / train_total * 100)\n",
    "        train_loss_h.append(train_loss)\n",
    "\n",
    "        print()\n",
    "\n",
    "        # Early stop.\n",
    "        if train_accuracy / train_total > 0.99:\n",
    "            print(\"Early stopped.\")\n",
    "            break\n",
    "\n",
    "    # Set model to evaluation mode.\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024/04/02 12:35:25 Epoch: 000 Time: 9.41 Process: 100.00% Accuracy: 10.09% Loss: 1750.105\n",
      "2024/04/02 12:35:35 Epoch: 001 Time: 10.13 Process: 100.00% Accuracy: 11.00% Loss: 1743.977\n",
      "2024/04/02 12:35:45 Epoch: 002 Time: 10.01 Process: 100.00% Accuracy: 11.41% Loss: 1741.669\n",
      "2024/04/02 12:35:55 Epoch: 003 Time: 10.49 Process: 100.00% Accuracy: 12.62% Loss: 1730.741\n",
      "2024/04/02 12:36:05 Epoch: 004 Time: 10.05 Process: 100.00% Accuracy: 15.82% Loss: 1676.209\n",
      "2024/04/02 12:36:15 Epoch: 005 Time: 9.92 Process: 100.00% Accuracy: 20.30% Loss: 1579.555\n",
      "2024/04/02 12:36:25 Epoch: 006 Time: 9.33 Process: 100.00% Accuracy: 27.44% Loss: 1465.120\n",
      "2024/04/02 12:36:34 Epoch: 007 Time: 9.13 Process: 100.00% Accuracy: 33.78% Loss: 1354.083\n",
      "2024/04/02 12:36:44 Epoch: 008 Time: 9.83 Process: 100.00% Accuracy: 38.74% Loss: 1249.916\n",
      "2024/04/02 12:36:53 Epoch: 009 Time: 9.27 Process: 100.00% Accuracy: 44.62% Loss: 1122.707\n",
      "2024/04/02 12:37:02 Epoch: 010 Time: 9.50 Process: 100.00% Accuracy: 52.14% Loss: 971.652\n",
      "2024/04/02 12:37:12 Epoch: 011 Time: 9.22 Process: 100.00% Accuracy: 61.47% Loss: 803.493\n",
      "2024/04/02 12:37:21 Epoch: 012 Time: 9.60 Process: 100.00% Accuracy: 69.40% Loss: 631.012\n",
      "2024/04/02 12:37:31 Epoch: 013 Time: 9.95 Process: 100.00% Accuracy: 77.29% Loss: 462.004\n",
      "2024/04/02 12:37:40 Epoch: 014 Time: 9.15 Process: 100.00% Accuracy: 85.00% Loss: 322.429\n",
      "2024/04/02 12:37:49 Epoch: 015 Time: 9.26 Process: 100.00% Accuracy: 89.99% Loss: 223.868\n",
      "2024/04/02 12:37:59 Epoch: 016 Time: 9.11 Process: 100.00% Accuracy: 92.62% Loss: 158.070\n",
      "2024/04/02 12:38:08 Epoch: 017 Time: 9.29 Process: 100.00% Accuracy: 94.56% Loss: 118.581\n",
      "2024/04/02 12:38:18 Epoch: 018 Time: 9.70 Process: 100.00% Accuracy: 96.14% Loss: 83.690\n",
      "2024/04/02 12:38:27 Epoch: 019 Time: 9.23 Process: 100.00% Accuracy: 97.69% Loss: 50.117\n",
      "2024/04/02 12:38:36 Epoch: 020 Time: 9.61 Process: 100.00% Accuracy: 97.85% Loss: 50.432\n",
      "2024/04/02 12:38:46 Epoch: 021 Time: 9.50 Process: 100.00% Accuracy: 98.58% Loss: 32.449\n",
      "2024/04/02 12:38:56 Epoch: 022 Time: 9.77 Process: 100.00% Accuracy: 98.13% Loss: 44.708\n",
      "2024/04/02 12:39:05 Epoch: 023 Time: 9.72 Process: 100.00% Accuracy: 98.63% Loss: 32.816\n",
      "2024/04/02 12:39:15 Epoch: 024 Time: 9.33 Process: 100.00% Accuracy: 98.26% Loss: 36.306\n",
      "2024/04/02 12:39:24 Epoch: 025 Time: 9.24 Process: 100.00% Accuracy: 99.32% Loss: 16.293\n",
      "Early stopped.\n"
     ]
    }
   ],
   "source": [
    "train(epochs=epochs, model=model, optimizer=optimizer, loss=loss, dataloader=train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1195, 2208, 2594, 3421,   46, 1542, 3671, 4293, 4120, 4179, 1349,  589,\n",
      "         4293, 4120,  820,  355, 1284, 5347, 4911, 4911, 4911, 4911, 4911, 4911,\n",
      "         4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911],\n",
      "        [3411, 4130, 1349, 4098, 1284, 4705, 2956, 3671,  214, 5231, 3592, 4919,\n",
      "         1284, 1915, 4099,  449, 4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911,\n",
      "         4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911],\n",
      "        [1169,  741, 3251,  473, 4293, 1448, 3411, 1840,  191, 1284, 2133, 4293,\n",
      "         5175, 5258, 3510, 4293, 2065,  883, 4460, 2176, 2270, 3671, 4293, 3411,\n",
      "          479,  665, 1284, 1393, 4293, 5028,  311,  846],\n",
      "        [1169, 1284, 2792, 5216, 3860, 1284, 2079, 1448, 3456, 3664, 3024, 2145,\n",
      "         3331, 3003, 4895, 3671, 4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911,\n",
      "         4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911],\n",
      "        [4089, 5090, 3998, 4548, 3543, 4293, 1762, 1284, 3338, 3998, 4404, 2113,\n",
      "         3742, 2235, 4293, 3898, 3411,  581, 4074, 4089, 5272, 4293, 3616, 1928,\n",
      "         2602, 5028, 4062, 4293, 3780,  103, 3454,  115],\n",
      "        [3780, 4698, 4293, 3123, 5110, 3671, 4911, 4911, 4911, 4911, 4911, 4911,\n",
      "         4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911,\n",
      "         4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911],\n",
      "        [ 326, 2114, 4293,  587, 5309, 3671, 4911, 4911, 4911, 4911, 4911, 4911,\n",
      "         4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911,\n",
      "         4911, 4911, 4911, 4911, 4911, 4911, 4911, 4911],\n",
      "        [1004, 1728,  834, 4293, 4133, 5023, 3671, 4293, 5253, 1742, 4293, 2242,\n",
      "         3787, 3414, 4815, 3157, 4293, 2580, 5023, 4293, 3710, 4293,   95, 3560,\n",
      "         1284, 1808,  234, 4293,  226, 4293, 2721, 4293]])\n",
      "tensor([[-8.4196e+00, -1.0669e+01, -1.0919e+01, -1.1760e+01, -8.3644e+00,\n",
      "         -1.7419e+01, -9.6538e+00, -1.3011e+01, -5.8193e-04, -2.1703e+01,\n",
      "         -1.2621e+01, -1.1577e+01],\n",
      "        [-2.4433e+01, -1.6507e+01, -2.0908e+01, -4.6492e-06, -2.0456e+01,\n",
      "         -1.9998e+01, -2.0532e+01, -1.2686e+01, -2.1692e+01, -1.3558e+01,\n",
      "         -1.5401e+01, -1.8949e+01],\n",
      "        [-1.8987e+01, -2.8565e-02, -7.9963e+00, -1.3187e+01, -1.4792e+01,\n",
      "         -1.0421e+01, -3.6180e+00, -1.2093e+01, -1.3710e+01, -1.3357e+01,\n",
      "         -1.1471e+01, -6.9736e+00],\n",
      "        [-1.9345e+01, -3.5387e+01, -1.8442e+01, -3.0980e+01, -2.3842e-06,\n",
      "         -1.6256e+01, -2.6683e+01, -2.3274e+01, -1.5180e+01, -3.4834e+01,\n",
      "         -3.2013e+01, -1.3082e+01],\n",
      "        [-1.1360e-03, -9.7229e+00, -1.2530e+01, -1.3703e+01, -1.0042e+01,\n",
      "         -1.0301e+01, -1.6418e+01, -6.9225e+00, -1.1749e+01, -1.5383e+01,\n",
      "         -1.8485e+01, -1.6357e+01],\n",
      "        [-2.1606e+01, -9.1121e+00, -2.4385e+01, -1.4817e+01, -2.3554e+01,\n",
      "         -8.9776e+00, -1.8683e+01, -1.9359e+01, -2.3562e+01, -4.5849e-04,\n",
      "         -8.4159e+00, -1.5394e+01],\n",
      "        [-3.0342e+01, -2.4896e+01, -1.4899e+01, -2.6706e+01, -8.4953e+00,\n",
      "         -1.2714e+01, -1.6031e+01, -2.6414e+01, -1.3016e+01, -2.1808e+01,\n",
      "         -1.5850e+01, -2.1026e-04],\n",
      "        [-1.7807e+01, -1.3928e+01, -1.2491e+01, -1.8956e+01, -6.3627e+00,\n",
      "         -1.2033e+01, -1.0736e+01, -1.2484e+01, -8.8366e+00, -1.6615e+01,\n",
      "         -1.1084e+01, -1.9234e-03]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "for i, j in train_dl:\n",
    "    print(i)\n",
    "    print(model(i.to(device)))\n",
    "    print(j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = [\n",
    "    \"勇敢無畏，充滿了活力和冒險精神，他們喜歡追求挑戰，敢於冒險，常常是行動派的領導者。\",\n",
    "    \"穩重可靠，以堅韌的意志力和耐心著稱，他們注重安全和舒適，並對物質生活有著強烈的執著。\",\n",
    "    \"機智聰明，好奇心旺盛，喜歡交際和表達自己，具有多才多藝的特質，常常充滿了靈活的思維和活力。\",\n",
    "    \"情感豐富，善解人意，對家庭和親密關係非常重視，他們總是充滿了溫柔和關懷，是很好的傾聽者和支持者。\",\n",
    "    \"自信大方，追求著成為焦點的慾望，他們充滿了熱情和活力，喜歡引領和影響身邊的人，時常展現出優越感和領導能力。\",\n",
    "    \"細心謹慎，追求完美，他們善於分析和解決問題，注重細節和有組織性，常常是值得信賴的夥伴和顧問。\",\n",
    "    \"追求和諧，優雅而公正，他們注重平衡和公平，善於溝通協調，是很好的調解者和中介者。\",\n",
    "    \"神秘內斂，充滿了熱情和直覺，他們擁有強烈的意志力和洞察力，常常是充滿挑戰性和魅力的個體。\",\n",
    "    \"自由奔放，熱愛冒險和探索，他們追求著廣闊的視野和新鮮的體驗，時常充滿了樂觀和幽默。\",\n",
    "    \"勤奮負責，追求事業成功和社會地位，他們具有堅毅的意志力和耐心，常常是穩健和實際的決策者。\",\n",
    "    \"獨立思考，充滿了理想主義和創意，他們追求著獨特的生活方式和社會價值觀，常常是前衛和不拘一格的個體。\",\n",
    "    \"敏感善良，充滿了同情心和想像力，他們常常是理想主義者和夢想家，追求著內心的情感和精神實踐。\",\n",
    "    \"勇敢無畏，充滿活力，喜歡追求挑戰，常常是行動派的領導者。\",\n",
    "    \"穩重可靠，堅韌耐心，注重安全舒適，對物質生活有強烈執著。\",\n",
    "    \"機智聰明，好奇心旺盛，善於交際表達，充滿靈活思維和活力。\",\n",
    "    \"情感豐富，善解人意，重視家庭和親密關係，溫柔關懷，傾聽支持者。\",\n",
    "    \"自信大方，追求成為焦點，充滿熱情活力，喜歡引領影響身邊的人。\",\n",
    "    \"細心謹慎，追求完美，善於分析解決問題，注重細節有組織性。\",\n",
    "    \"追求和諧，優雅公正，注重平衡公平，善於溝通協調。\",\n",
    "    \"神秘內斂，熱情直覺，意志力洞察力強，充滿挑戰性和魅力。\",\n",
    "    \"自由奔放，熱愛冒險探索，追求廣闊視野和新鮮體驗，樂觀幽默。\",\n",
    "    \"勤奮負責，追求事業成功社會地位，具堅毅意志力和耐心，穩健決策者。\",\n",
    "    \"獨立思考，理想主義創意，追求獨特生活方式和價值觀，前衛不拘一格。\",\n",
    "    \"敏感善良，同情心想像力豐富，理想主義夢想家，追求內心情感精神實踐。\",\n",
    "    \"勇敢果敢，充滿活力，愛冒險。\",\n",
    "    \"穩重堅定，堅持自我價值觀。\",\n",
    "    \"靈活機智，善於溝通交際。\",\n",
    "    \"情感豐富，家庭意識強。\",\n",
    "    \"自信領導，熱情奔放。\",\n",
    "    \"細心謹慎，追求完美。\",\n",
    "    \"追求和諧，公正公平。\",\n",
    "    \"神秘敏感，探索深度。\",\n",
    "    \"自由探險，樂觀向上。\",\n",
    "    \"勤奮穩健，追求成功。\",\n",
    "    \"獨立創新，理想主義者。\",\n",
    "    \"敏感浪漫，夢想家。\",\n",
    "    \"生命充滿了勇氣與活力，他們勇於面對挑戰，永不退縮，常常是行動派的領袖，樂於率先嘗試新事物。\",\n",
    "    \"展現出穩重且堅定的品性，他們始終堅持自身的價值觀與信念，喜歡在自己熟悉的領域中深耕不輟。\",\n",
    "    \"擁有機智敏捷的頭腦，他們善於溝通交際，充滿活力與靈活的思維，對於各種新奇的事物都抱有濃厚的好奇心。\",\n",
    "    \"的人情感豐富且懂得關懷，他們對家庭有著極為強烈的連結感，願意為了家人無條件地付出與奉獻。\",\n",
    "    \"展現出極度的自信與熱情，他們常常是眾人注目的焦點，充滿著領導力與活力，喜歡成為團體中的中心人物。\",\n",
    "    \"的人細心謹慎，追求完美與規律，他們擁有出色的分析能力與解決問題的技巧，喜歡保持事情的有序與井然有序。\",\n",
    "    \"追求和諧與公正，他們擅長於溝通協調，注重平衡與公平，善於解決衝突，是團隊中不可或缺的調解者。\",\n",
    "    \"的人神秘內斂，充滿了深度與直覺，他們擁有強烈的意志力與洞察力，總是對事情有著深入的探索與研究。\",\n",
    "    \"熱愛自由與探險，他們樂觀向上，喜歡挑戰與冒險，追求著廣闊的視野與新鮮的體驗，不斷探索未知的領域。\",\n",
    "    \"的人勤奮穩健，追求事業成功與社會地位，他們具有堅毅的意志力與耐心，是穩健可靠的決策者與領導者。\",\n",
    "    \"獨立創新，理想主義者，他們勇於打破傳統的束縛，追求獨特的生活方式與社會價值觀，常常是前衛與不拘一格的個體。\",\n",
    "    \"的人敏感浪漫，是真正的夢想家，他們充滿了同情心與想像力，常常是理想主義者，追求內心情感與精神的實踐。\",\n",
    "]\n",
    "\n",
    "answare = [\n",
    "    '牡羊', '金牛', '雙子', '巨蠍', '獅子', '處女', '天秤', '天蠍', '射手', '魔羯', '水瓶', '雙魚',\n",
    "    '牡羊', '金牛', '雙子', '巨蠍', '獅子', '處女', '天秤', '天蠍', '射手', '魔羯', '水瓶', '雙魚',\n",
    "    '牡羊', '金牛', '雙子', '巨蠍', '獅子', '處女', '天秤', '天蠍', '射手', '魔羯', '水瓶', '雙魚',\n",
    "    '牡羊', '金牛', '雙子', '巨蠍', '獅子', '處女', '天秤', '天蠍', '射手', '魔羯', '水瓶', '雙魚',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(testset: list) -> torch.Tensor:\n",
    "    result = []\n",
    "\n",
    "    for line in testset:\n",
    "        temp = jieba.lcut(line)\n",
    "        temp = temp + [\"<PAD>\"] * (32 - len(temp))\n",
    "        temp = [data.w2i[x] if x in data.w2i else data.w2i[\"<PAD>\"] for x in temp][:32]\n",
    "        result.append(temp)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = process(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding): Embedding(5470, 64)\n",
       "  (lstm): LSTM(64, 256, num_layers=4, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (linear): Linear(in_features=16384, out_features=12, bias=True)\n",
       "  (classifier): AttentionClassifier(\n",
       "    (attention): Attention(\n",
       "      (linear1): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (linear2): Linear(in_features=64, out_features=1, bias=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (linear): Linear(in_features=256, out_features=12, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.8120e-05, -1.9818e+01, -1.8563e+01, -2.4603e+01, -1.3343e+01,\n",
      "         -2.0963e+01, -2.3896e+01, -1.6780e+01, -1.1008e+01, -2.8327e+01,\n",
      "         -2.7027e+01, -2.3756e+01],\n",
      "        [-1.1979e+01, -8.3695e-01, -6.3657e+00, -2.4787e+00, -1.0015e+01,\n",
      "         -7.5398e+00, -3.8698e+00, -7.9994e-01, -1.1511e+01, -6.8414e+00,\n",
      "         -6.5932e+00, -4.8101e+00],\n",
      "        [-1.0268e+01, -4.8108e+00, -2.4234e-01, -1.3628e+01, -3.3417e+00,\n",
      "         -8.1393e+00, -2.5979e+00, -9.1532e+00, -3.7665e+00, -1.8564e+01,\n",
      "         -1.3129e+01, -2.6078e+00],\n",
      "        [-1.8752e+01, -5.1291e+00, -4.6999e-01, -1.1184e+00, -1.1203e+01,\n",
      "         -1.1297e+01, -3.1730e+00, -9.8429e+00, -8.6751e+00, -1.7246e+01,\n",
      "         -1.2159e+01, -8.8168e+00],\n",
      "        [-2.9810e+00, -1.7192e+01, -1.0166e+01, -1.4512e+01, -9.2100e-01,\n",
      "         -1.3714e+01, -1.5234e+01, -5.9668e-01, -8.2355e+00, -2.1062e+01,\n",
      "         -1.7055e+01, -8.5395e+00],\n",
      "        [-1.8949e+01, -8.7164e+00, -8.2307e+00, -2.0350e+01, -1.3415e+01,\n",
      "         -4.5325e-04, -1.6518e+01, -1.8832e+01, -2.2657e+01, -1.3395e+01,\n",
      "         -2.2537e+01, -1.0833e+01],\n",
      "        [-4.3484e+01, -2.3150e+01, -1.2797e+01, -3.4619e+01, -2.9504e+01,\n",
      "         -2.4866e+01, -4.4107e-06, -3.5239e+01, -2.4012e+01, -3.6669e+01,\n",
      "         -2.3216e+01, -1.3314e+01],\n",
      "        [-4.3919e+00, -1.5007e+01, -6.8806e+00, -1.1293e+01, -3.3549e+00,\n",
      "         -1.5627e+01, -9.3779e+00, -3.3349e+00, -8.8981e-02, -2.1828e+01,\n",
      "         -1.2020e+01, -6.8156e+00],\n",
      "        [-9.6875e+00, -1.8367e+01, -1.4581e+01, -1.8382e+01, -8.7456e+00,\n",
      "         -1.8968e+01, -1.3298e+01, -1.4823e+01, -4.1369e-04, -2.2259e+01,\n",
      "         -1.1586e+01, -8.6200e+00],\n",
      "        [-7.9216e+00, -4.4119e+00, -1.0362e+01, -1.0170e+01, -7.7977e+00,\n",
      "         -8.0098e+00, -8.4154e+00, -1.1237e-01, -9.8172e+00, -6.5946e+00,\n",
      "         -4.7028e+00, -2.4978e+00],\n",
      "        [-8.3065e+00, -1.4099e+01, -1.4214e+01, -9.9087e+00, -1.0925e+01,\n",
      "         -1.7917e+01, -8.7452e+00, -6.1902e+00, -5.8770e-01, -1.3505e+01,\n",
      "         -8.1916e-01, -6.8404e+00],\n",
      "        [-9.7071e+00, -9.7445e+00, -1.2386e+01, -8.1327e+00, -4.6500e+00,\n",
      "         -8.1909e+00, -9.8912e+00, -4.1193e+00, -5.1369e+00, -4.9776e+00,\n",
      "         -9.2095e-01, -5.7531e-01],\n",
      "        [-5.0186e-05, -1.9726e+01, -1.7430e+01, -2.4103e+01, -1.2009e+01,\n",
      "         -2.0248e+01, -2.2524e+01, -1.5583e+01, -1.0032e+01, -2.7928e+01,\n",
      "         -2.5983e+01, -2.1986e+01],\n",
      "        [-1.2975e+01, -1.8684e+00, -7.6844e+00, -5.7711e+00, -1.0814e+01,\n",
      "         -7.4600e+00, -5.4530e+00, -1.8837e-01, -1.4446e+01, -7.0760e+00,\n",
      "         -8.0913e+00, -4.8653e+00],\n",
      "        [-1.9688e+01, -1.0241e+01, -1.1056e+01, -1.2112e+01, -8.1626e+00,\n",
      "         -6.1635e+00, -1.2462e+01, -9.9356e+00, -1.3981e+01, -7.7466e+00,\n",
      "         -8.4009e+00, -3.1619e-03],\n",
      "        [-2.3111e+01, -1.3182e+01, -1.3556e+01, -1.2636e-05, -1.3884e+01,\n",
      "         -1.3422e+01, -2.0590e+01, -1.2119e+01, -1.9421e+01, -1.3819e+01,\n",
      "         -1.8000e+01, -1.4458e+01],\n",
      "        [-4.0315e+00, -2.1954e+01, -1.2000e+01, -2.0041e+01, -1.8137e-02,\n",
      "         -1.5729e+01, -1.9518e+01, -8.9956e+00, -9.2910e+00, -2.7682e+01,\n",
      "         -2.4734e+01, -1.2458e+01],\n",
      "        [-1.9069e+01, -1.4863e+01, -1.3567e+01, -2.5716e+01, -1.6244e+01,\n",
      "         -2.2650e-06, -2.2766e+01, -2.2167e+01, -2.7626e+01, -1.4710e+01,\n",
      "         -2.6372e+01, -1.6351e+01],\n",
      "        [-4.1645e+01, -2.2777e+01, -8.5054e+00, -3.1705e+01, -2.4848e+01,\n",
      "         -2.1022e+01, -2.6008e-04, -3.1885e+01, -2.3105e+01, -3.4358e+01,\n",
      "         -2.1773e+01, -9.7616e+00],\n",
      "        [-4.9912e+00, -1.9028e+01, -8.5907e+00, -1.1082e+01, -1.9297e+00,\n",
      "         -1.6494e+01, -1.5888e+01, -1.6610e-01, -7.1487e+00, -2.3631e+01,\n",
      "         -1.7196e+01, -9.8787e+00],\n",
      "        [-1.3835e+01, -1.7024e+01, -1.6767e+01, -2.0081e+01, -1.5353e+01,\n",
      "         -2.0683e+01, -1.2155e+01, -1.9160e+01, -2.1420e-03, -1.9622e+01,\n",
      "         -6.1894e+00, -9.4074e+00],\n",
      "        [-5.9112e+00, -5.4532e+00, -1.0102e+01, -9.3116e+00, -4.6843e+00,\n",
      "         -6.2810e+00, -1.1108e+01, -7.8328e-02, -8.8059e+00, -6.5005e+00,\n",
      "         -6.0002e+00, -2.9382e+00],\n",
      "        [-1.4952e+01, -1.1595e+01, -1.8475e+01, -8.3691e+00, -1.6557e+01,\n",
      "         -1.8076e+01, -1.1351e+01, -7.5340e+00, -1.0434e+01, -8.8396e+00,\n",
      "         -1.0472e-03, -9.3800e+00],\n",
      "        [-9.0346e+00, -1.3834e+01, -1.0949e+01, -6.6170e-01, -1.4554e+00,\n",
      "         -9.2601e+00, -1.4970e+01, -1.4373e+00, -7.0634e+00, -9.9761e+00,\n",
      "         -8.3813e+00, -4.4413e+00],\n",
      "        [-1.8103e+00, -1.5046e+01, -8.2207e+00, -1.2916e+01, -7.3192e-01,\n",
      "         -1.2175e+01, -1.1217e+01, -1.0916e+00, -4.0624e+00, -1.9082e+01,\n",
      "         -1.3564e+01, -6.1049e+00],\n",
      "        [-1.7553e+01, -2.4534e-02, -3.7567e+00, -1.0489e+01, -1.7412e+01,\n",
      "         -1.0207e+01, -7.9038e+00, -7.7325e+00, -1.9945e+01, -1.4855e+01,\n",
      "         -1.6976e+01, -1.3028e+01],\n",
      "        [-3.5078e+01, -2.1317e+01, -9.1791e-06, -2.6614e+01, -2.0009e+01,\n",
      "         -1.1995e+01, -1.8798e+01, -2.7754e+01, -2.6371e+01, -3.0971e+01,\n",
      "         -2.9617e+01, -1.2709e+01],\n",
      "        [-1.5633e+01, -1.3174e+01, -1.4756e+01, -6.5532e-04, -1.3816e+01,\n",
      "         -1.4100e+01, -2.0795e+01, -7.3429e+00, -1.6540e+01, -1.2634e+01,\n",
      "         -1.5701e+01, -1.5767e+01],\n",
      "        [-1.0457e-02, -2.1265e+01, -1.5720e+01, -1.9755e+01, -5.1490e+00,\n",
      "         -1.6111e+01, -2.1190e+01, -5.3832e+00, -1.2523e+01, -2.3950e+01,\n",
      "         -2.3197e+01, -1.6127e+01],\n",
      "        [-1.0417e+01, -3.2890e+00, -1.2887e+01, -1.5117e+01, -1.4489e+01,\n",
      "         -4.3017e-02, -1.3669e+01, -1.5973e+01, -1.6105e+01, -5.3442e+00,\n",
      "         -1.3455e+01, -1.3274e+01],\n",
      "        [-3.3551e+01, -1.8019e+01, -1.4485e+01, -2.8617e+01, -2.7070e+01,\n",
      "         -2.1849e+01, -1.0967e-05, -2.9435e+01, -1.6385e+01, -2.7430e+01,\n",
      "         -1.2928e+01, -1.1756e+01],\n",
      "        [-1.0926e+01, -2.0848e+01, -1.6169e+01, -1.3415e+01, -1.0399e+01,\n",
      "         -1.8233e+01, -1.9299e+01, -5.3285e-05, -1.6229e+01, -1.9906e+01,\n",
      "         -1.5803e+01, -1.2715e+01],\n",
      "        [-1.1474e+01, -1.8041e+01, -1.3905e+01, -1.4082e+01, -7.2120e+00,\n",
      "         -1.6281e+01, -1.3944e+01, -1.3820e+01, -1.8052e-03, -1.8794e+01,\n",
      "         -9.3309e+00, -6.9451e+00],\n",
      "        [-1.9429e+00, -1.8807e+00, -1.2595e+01, -7.8372e+00, -9.6828e+00,\n",
      "         -2.3146e+00, -1.2476e+01, -3.5150e+00, -1.1516e+01, -5.5584e-01,\n",
      "         -6.4834e+00, -9.4957e+00],\n",
      "        [-1.1773e+00, -1.0364e+01, -1.5450e+01, -9.8356e+00, -7.6734e+00,\n",
      "         -9.0709e+00, -1.0304e+01, -1.1903e+00, -5.4001e+00, -4.5548e+00,\n",
      "         -9.9315e-01, -6.4174e+00],\n",
      "        [-1.7610e+01, -1.1025e+01, -9.8384e+00, -1.5536e+01, -7.1602e+00,\n",
      "         -2.3727e+00, -1.0444e+01, -1.0268e+01, -1.5128e+01, -8.3120e+00,\n",
      "         -1.0493e+01, -9.9169e-02],\n",
      "        [-4.9888e+00, -1.5942e+01, -1.0914e+01, -1.5286e+01, -4.3017e+00,\n",
      "         -1.4480e+01, -1.4460e+01, -1.2757e+01, -2.0664e-02, -2.2044e+01,\n",
      "         -1.5367e+01, -9.5588e+00],\n",
      "        [-7.8100e+00, -1.5204e+01, -1.2767e+01, -1.3816e+01, -9.1209e+00,\n",
      "         -1.4844e+01, -1.1861e+01, -5.4380e-04, -1.2882e+01, -1.8676e+01,\n",
      "         -1.4721e+01, -1.1162e+01],\n",
      "        [-2.1866e+01, -1.5181e+01, -2.0981e-05, -2.0444e+01, -1.4940e+01,\n",
      "         -1.3055e+01, -1.1376e+01, -1.7301e+01, -1.7147e+01, -2.7708e+01,\n",
      "         -2.3339e+01, -1.1911e+01],\n",
      "        [-1.3023e+01, -1.4119e+01, -1.4604e+01, -1.5014e-02, -7.5834e+00,\n",
      "         -1.1194e+01, -1.7161e+01, -4.2661e+00, -1.1487e+01, -9.1048e+00,\n",
      "         -9.0080e+00, -9.2487e+00],\n",
      "        [-5.9194e+00, -2.0203e+01, -1.2242e+01, -1.7328e+01, -2.0017e-02,\n",
      "         -1.4779e+01, -1.6021e+01, -1.0353e+01, -4.0741e+00, -2.4839e+01,\n",
      "         -1.9558e+01, -9.3608e+00],\n",
      "        [-3.2570e+01, -2.0569e+01, -1.1521e+01, -2.9616e+01, -1.6371e+01,\n",
      "         -1.2342e+01, -7.9328e-01, -3.0122e+01, -1.2098e+01, -2.2833e+01,\n",
      "         -1.2203e+01, -6.0218e-01],\n",
      "        [-4.1328e+01, -2.3286e+01, -8.9695e+00, -3.1759e+01, -2.6009e+01,\n",
      "         -2.2822e+01, -1.3362e-04, -3.3498e+01, -2.1766e+01, -3.6310e+01,\n",
      "         -2.2326e+01, -1.1944e+01],\n",
      "        [-7.7627e+00, -1.9128e+01, -1.3086e+01, -1.1761e+01, -8.8246e+00,\n",
      "         -1.8243e+01, -1.5974e+01, -6.6151e-04, -9.6587e+00, -2.0226e+01,\n",
      "         -1.3125e+01, -1.1245e+01],\n",
      "        [-9.4098e+00, -1.6357e+01, -1.4264e+01, -1.9212e+01, -1.0953e+01,\n",
      "         -1.7063e+01, -1.2502e+01, -1.7170e+01, -1.7212e-04, -2.0626e+01,\n",
      "         -1.1101e+01, -9.8436e+00],\n",
      "        [-6.5343e+00, -1.0670e+00, -6.6843e+00, -9.8072e+00, -9.8351e+00,\n",
      "         -1.6613e+00, -2.2385e+00, -2.6655e+00, -9.6670e+00, -2.2059e+00,\n",
      "         -2.0336e+00, -3.0793e+00],\n",
      "        [-2.4150e+01, -1.9350e+01, -2.1970e+01, -1.9168e+01, -1.9586e+01,\n",
      "         -1.8375e+01, -1.2594e+01, -2.1059e+01, -1.0214e+01, -1.1102e+01,\n",
      "         -9.7370e-04, -6.9932e+00],\n",
      "        [-9.0127e+00, -8.3977e+00, -1.6864e+01, -7.2886e+00, -1.0541e+01,\n",
      "         -5.9980e+00, -1.1825e+01, -2.9173e+00, -1.3511e+01, -1.8653e-01,\n",
      "         -2.2034e+00, -6.1591e+00]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "result = model(torch.tensor(test).to(device))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normolization\n",
    "results = []\n",
    "\n",
    "for i in range(len(result)):\n",
    "    temp = result[i]\n",
    "    temp = [x - min(temp) for x in temp]\n",
    "    temp = [x / max(temp) for x in temp]\n",
    "    temp = [round(x, 3) for x in temp]\n",
    "\n",
    "    results.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "牡羊✅\n",
      "天蠍 <- 金牛\n",
      "雙子✅\n",
      "雙子 <- 巨蠍\n",
      "天蠍 <- 獅子\n",
      "處女✅\n",
      "天秤✅\n",
      "射手 <- 天蠍\n",
      "射手✅\n",
      "天蠍 <- 魔羯\n",
      "射手 <- 水瓶\n",
      "雙魚✅\n",
      "牡羊✅\n",
      "天蠍 <- 金牛\n",
      "雙魚 <- 雙子\n",
      "巨蠍✅\n",
      "獅子✅\n",
      "處女✅\n",
      "天秤✅\n",
      "天蠍✅\n",
      "射手✅\n",
      "天蠍 <- 魔羯\n",
      "水瓶✅\n",
      "巨蠍 <- 雙魚\n",
      "獅子 <- 牡羊\n",
      "金牛✅\n",
      "雙子✅\n",
      "巨蠍✅\n",
      "牡羊 <- 獅子\n",
      "處女✅\n",
      "天秤✅\n",
      "天蠍✅\n",
      "射手✅\n",
      "魔羯✅\n",
      "水瓶✅\n",
      "雙魚✅\n",
      "射手 <- 牡羊\n",
      "天蠍 <- 金牛\n",
      "雙子✅\n",
      "巨蠍✅\n",
      "獅子✅\n",
      "雙魚 <- 處女\n",
      "天秤✅\n",
      "天蠍✅\n",
      "射手✅\n",
      "金牛 <- 魔羯\n",
      "水瓶✅\n",
      "魔羯 <- 雙魚\n",
      "Correct: 31/48 (64.58%)\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    t = classes[int(torch.argmax(torch.tensor(r)))]\n",
    "    if t == answare[i]:\n",
    "        count += 1\n",
    "        print(t + \"✅\")\n",
    "    else:\n",
    "        print(t + \" <- \" + answare[i])\n",
    "\n",
    "print(f\"Correct: {count}/{len(answare)} ({(count / len(answare)) * 100 :.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# angles_A = np.linspace(start=0, stop=2*np.pi, num=len(result)+1, endpoint=True)\n",
    "# values_A = np.concatenate((result, [result[0]]))\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(5, 5), subplot_kw={'projection': 'polar'})\n",
    "# ax.plot(angles_A, values_A, 'o-', color=\"blue\", label=\"A\")\n",
    "\n",
    "# ax.fill(angles_A, values_A, alpha=0.3, color=\"blue\")\n",
    "# ax.set_thetagrids(angles_A[:-1] * 180 / np.pi, range(12), fontsize=15)\n",
    "# ax.set_theta_zero_location('N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files (x86)\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:4315: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "d:\\Program Files (x86)\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\onnx\\_internal\\jit_utils.py:258: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\jit\\passes\\onnx\\shape_type_inference.cpp:1888.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "d:\\Program Files (x86)\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\onnx\\utils.py:687: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\jit\\passes\\onnx\\shape_type_inference.cpp:1888.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "d:\\Program Files (x86)\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\onnx\\utils.py:1178: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\jit\\passes\\onnx\\shape_type_inference.cpp:1888.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "# Please check the virtual input of the ONNX model.\n",
    "torch.onnx.export(model, torch.tensor([test[0]]).to(device=device), 'constellator.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph torch_jit (\n",
      "  %input.1[INT64, 1x32]\n",
      ") initializers (\n",
      "  %embedding.weight[FLOAT, 5470x64]\n",
      "  %classifier.attention.linear1.weight[FLOAT, 64x256]\n",
      "  %classifier.attention.linear1.bias[FLOAT, 64]\n",
      "  %classifier.attention.linear2.weight[FLOAT, 1x64]\n",
      "  %classifier.attention.linear2.bias[FLOAT, 1]\n",
      "  %classifier.linear.weight[FLOAT, 12x256]\n",
      "  %classifier.linear.bias[FLOAT, 12]\n",
      "  %onnx::LSTM_742[FLOAT, 2x2048]\n",
      "  %onnx::LSTM_743[FLOAT, 2x1024x64]\n",
      "  %onnx::LSTM_744[FLOAT, 2x1024x256]\n",
      "  %onnx::LSTM_789[FLOAT, 2x2048]\n",
      "  %onnx::LSTM_790[FLOAT, 2x1024x512]\n",
      "  %onnx::LSTM_791[FLOAT, 2x1024x256]\n",
      "  %onnx::LSTM_836[FLOAT, 2x2048]\n",
      "  %onnx::LSTM_837[FLOAT, 2x1024x512]\n",
      "  %onnx::LSTM_838[FLOAT, 2x1024x256]\n",
      "  %onnx::LSTM_883[FLOAT, 2x2048]\n",
      "  %onnx::LSTM_884[FLOAT, 2x1024x512]\n",
      "  %onnx::LSTM_885[FLOAT, 2x1024x256]\n",
      ") {\n",
      "  %/embedding/Gather_output_0 = Gather(%embedding.weight, %input.1)\n",
      "  %/lstm/Transpose_output_0 = Transpose[perm = [1, 0, 2]](%/embedding/Gather_output_0)\n",
      "  %/lstm/Constant_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Constant_1_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Shape_output_0 = Shape(%/lstm/Transpose_output_0)\n",
      "  %/lstm/Constant_2_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_output_0 = Gather(%/lstm/Shape_output_0, %/lstm/Constant_2_output_0)\n",
      "  %onnx::Unsqueeze_179 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_180 = Unsqueeze(%/lstm/Gather_output_0, %onnx::Unsqueeze_179)\n",
      "  %/lstm/Constant_3_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_747 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_output_0 = Concat[axis = 0](%onnx::Concat_747, %onnx::Concat_180, %/lstm/Constant_3_output_0)\n",
      "  %/lstm/Expand_output_0 = Expand(%/lstm/Constant_output_0, %/lstm/Concat_output_0)\n",
      "  %/lstm/Shape_1_output_0 = Shape(%/lstm/Transpose_output_0)\n",
      "  %/lstm/Constant_4_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_1_output_0 = Gather(%/lstm/Shape_1_output_0, %/lstm/Constant_4_output_0)\n",
      "  %onnx::Unsqueeze_190 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_191 = Unsqueeze(%/lstm/Gather_1_output_0, %onnx::Unsqueeze_190)\n",
      "  %/lstm/Constant_5_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_748 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_1_output_0 = Concat[axis = 0](%onnx::Concat_748, %onnx::Concat_191, %/lstm/Constant_5_output_0)\n",
      "  %/lstm/Expand_1_output_0 = Expand(%/lstm/Constant_1_output_0, %/lstm/Concat_1_output_0)\n",
      "  %/lstm/LSTM_output_0, %/lstm/LSTM_output_1, %/lstm/LSTM_output_2 = LSTM[direction = 'bidirectional', hidden_size = 256](%/lstm/Transpose_output_0, %onnx::LSTM_743, %onnx::LSTM_744, %onnx::LSTM_742, %, %/lstm/Expand_output_0, %/lstm/Expand_1_output_0)\n",
      "  %/lstm/Transpose_1_output_0 = Transpose[perm = [0, 2, 1, 3]](%/lstm/LSTM_output_0)\n",
      "  %/lstm/Constant_6_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Reshape_output_0 = Reshape[allowzero = 0](%/lstm/Transpose_1_output_0, %/lstm/Constant_6_output_0)\n",
      "  %/lstm/Constant_7_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Constant_8_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Shape_2_output_0 = Shape(%/lstm/Reshape_output_0)\n",
      "  %/lstm/Constant_9_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_2_output_0 = Gather(%/lstm/Shape_2_output_0, %/lstm/Constant_9_output_0)\n",
      "  %onnx::Unsqueeze_336 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_337 = Unsqueeze(%/lstm/Gather_2_output_0, %onnx::Unsqueeze_336)\n",
      "  %/lstm/Constant_10_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_794 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_2_output_0 = Concat[axis = 0](%onnx::Concat_794, %onnx::Concat_337, %/lstm/Constant_10_output_0)\n",
      "  %/lstm/Expand_2_output_0 = Expand(%/lstm/Constant_7_output_0, %/lstm/Concat_2_output_0)\n",
      "  %/lstm/Shape_3_output_0 = Shape(%/lstm/Reshape_output_0)\n",
      "  %/lstm/Constant_11_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_3_output_0 = Gather(%/lstm/Shape_3_output_0, %/lstm/Constant_11_output_0)\n",
      "  %onnx::Unsqueeze_347 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_348 = Unsqueeze(%/lstm/Gather_3_output_0, %onnx::Unsqueeze_347)\n",
      "  %/lstm/Constant_12_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_795 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_3_output_0 = Concat[axis = 0](%onnx::Concat_795, %onnx::Concat_348, %/lstm/Constant_12_output_0)\n",
      "  %/lstm/Expand_3_output_0 = Expand(%/lstm/Constant_8_output_0, %/lstm/Concat_3_output_0)\n",
      "  %/lstm/LSTM_1_output_0, %/lstm/LSTM_1_output_1, %/lstm/LSTM_1_output_2 = LSTM[direction = 'bidirectional', hidden_size = 256](%/lstm/Reshape_output_0, %onnx::LSTM_790, %onnx::LSTM_791, %onnx::LSTM_789, %, %/lstm/Expand_2_output_0, %/lstm/Expand_3_output_0)\n",
      "  %/lstm/Transpose_2_output_0 = Transpose[perm = [0, 2, 1, 3]](%/lstm/LSTM_1_output_0)\n",
      "  %/lstm/Constant_13_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Reshape_1_output_0 = Reshape[allowzero = 0](%/lstm/Transpose_2_output_0, %/lstm/Constant_13_output_0)\n",
      "  %/lstm/Constant_14_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Constant_15_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Shape_4_output_0 = Shape(%/lstm/Reshape_1_output_0)\n",
      "  %/lstm/Constant_16_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_4_output_0 = Gather(%/lstm/Shape_4_output_0, %/lstm/Constant_16_output_0)\n",
      "  %onnx::Unsqueeze_493 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_494 = Unsqueeze(%/lstm/Gather_4_output_0, %onnx::Unsqueeze_493)\n",
      "  %/lstm/Constant_17_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_841 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_4_output_0 = Concat[axis = 0](%onnx::Concat_841, %onnx::Concat_494, %/lstm/Constant_17_output_0)\n",
      "  %/lstm/Expand_4_output_0 = Expand(%/lstm/Constant_14_output_0, %/lstm/Concat_4_output_0)\n",
      "  %/lstm/Shape_5_output_0 = Shape(%/lstm/Reshape_1_output_0)\n",
      "  %/lstm/Constant_18_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_5_output_0 = Gather(%/lstm/Shape_5_output_0, %/lstm/Constant_18_output_0)\n",
      "  %onnx::Unsqueeze_504 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_505 = Unsqueeze(%/lstm/Gather_5_output_0, %onnx::Unsqueeze_504)\n",
      "  %/lstm/Constant_19_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_842 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_5_output_0 = Concat[axis = 0](%onnx::Concat_842, %onnx::Concat_505, %/lstm/Constant_19_output_0)\n",
      "  %/lstm/Expand_5_output_0 = Expand(%/lstm/Constant_15_output_0, %/lstm/Concat_5_output_0)\n",
      "  %/lstm/LSTM_2_output_0, %/lstm/LSTM_2_output_1, %/lstm/LSTM_2_output_2 = LSTM[direction = 'bidirectional', hidden_size = 256](%/lstm/Reshape_1_output_0, %onnx::LSTM_837, %onnx::LSTM_838, %onnx::LSTM_836, %, %/lstm/Expand_4_output_0, %/lstm/Expand_5_output_0)\n",
      "  %/lstm/Transpose_3_output_0 = Transpose[perm = [0, 2, 1, 3]](%/lstm/LSTM_2_output_0)\n",
      "  %/lstm/Constant_20_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Reshape_2_output_0 = Reshape[allowzero = 0](%/lstm/Transpose_3_output_0, %/lstm/Constant_20_output_0)\n",
      "  %/lstm/Constant_21_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Constant_22_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Shape_6_output_0 = Shape(%/lstm/Reshape_2_output_0)\n",
      "  %/lstm/Constant_23_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_6_output_0 = Gather(%/lstm/Shape_6_output_0, %/lstm/Constant_23_output_0)\n",
      "  %onnx::Unsqueeze_650 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_651 = Unsqueeze(%/lstm/Gather_6_output_0, %onnx::Unsqueeze_650)\n",
      "  %/lstm/Constant_24_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_888 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_6_output_0 = Concat[axis = 0](%onnx::Concat_888, %onnx::Concat_651, %/lstm/Constant_24_output_0)\n",
      "  %/lstm/Expand_6_output_0 = Expand(%/lstm/Constant_21_output_0, %/lstm/Concat_6_output_0)\n",
      "  %/lstm/Shape_7_output_0 = Shape(%/lstm/Reshape_2_output_0)\n",
      "  %/lstm/Constant_25_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_7_output_0 = Gather(%/lstm/Shape_7_output_0, %/lstm/Constant_25_output_0)\n",
      "  %onnx::Unsqueeze_661 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_662 = Unsqueeze(%/lstm/Gather_7_output_0, %onnx::Unsqueeze_661)\n",
      "  %/lstm/Constant_26_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_889 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_7_output_0 = Concat[axis = 0](%onnx::Concat_889, %onnx::Concat_662, %/lstm/Constant_26_output_0)\n",
      "  %/lstm/Expand_7_output_0 = Expand(%/lstm/Constant_22_output_0, %/lstm/Concat_7_output_0)\n",
      "  %/lstm/LSTM_3_output_0, %/lstm/LSTM_3_output_1, %/lstm/LSTM_3_output_2 = LSTM[direction = 'bidirectional', hidden_size = 256](%/lstm/Reshape_2_output_0, %onnx::LSTM_884, %onnx::LSTM_885, %onnx::LSTM_883, %, %/lstm/Expand_6_output_0, %/lstm/Expand_7_output_0)\n",
      "  %/lstm/Transpose_4_output_0 = Transpose[perm = [0, 2, 1, 3]](%/lstm/LSTM_3_output_0)\n",
      "  %/lstm/Constant_27_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Reshape_3_output_0 = Reshape[allowzero = 0](%/lstm/Transpose_4_output_0, %/lstm/Constant_27_output_0)\n",
      "  %/lstm/Transpose_5_output_0 = Transpose[perm = [1, 0, 2]](%/lstm/Reshape_3_output_0)\n",
      "  %/Constant_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Constant_1_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Constant_2_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Constant_3_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Slice_output_0 = Slice(%/lstm/Transpose_5_output_0, %/Constant_1_output_0, %/Constant_2_output_0, %/Constant_output_0, %/Constant_3_output_0)\n",
      "  %/Add_output_0 = Add(%/Slice_output_0, %/Slice_output_0)\n",
      "  %/classifier/attention/Constant_output_0 = Constant[value = <Tensor>]()\n",
      "  %/classifier/attention/Reshape_output_0 = Reshape[allowzero = 0](%/Add_output_0, %/classifier/attention/Constant_output_0)\n",
      "  %/classifier/attention/linear1/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/classifier/attention/Reshape_output_0, %classifier.attention.linear1.weight, %classifier.attention.linear1.bias)\n",
      "  %/classifier/attention/relu/Relu_output_0 = Relu(%/classifier/attention/linear1/Gemm_output_0)\n",
      "  %/classifier/attention/linear2/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/classifier/attention/relu/Relu_output_0, %classifier.attention.linear2.weight, %classifier.attention.linear2.bias)\n",
      "  %/classifier/attention/Constant_1_output_0 = Constant[value = <Tensor>]()\n",
      "  %/classifier/attention/Reshape_1_output_0 = Reshape[allowzero = 0](%/classifier/attention/linear2/Gemm_output_0, %/classifier/attention/Constant_1_output_0)\n",
      "  %/classifier/attention/Softmax_output_0 = Softmax[axis = 1](%/classifier/attention/Reshape_1_output_0)\n",
      "  %/classifier/attention/Constant_2_output_0 = Constant[value = <Tensor>]()\n",
      "  %/classifier/attention/Unsqueeze_output_0 = Unsqueeze(%/classifier/attention/Softmax_output_0, %/classifier/attention/Constant_2_output_0)\n",
      "  %/classifier/Mul_output_0 = Mul(%/Add_output_0, %/classifier/attention/Unsqueeze_output_0)\n",
      "  %onnx::ReduceSum_698 = Constant[value = <Tensor>]()\n",
      "  %/classifier/ReduceSum_output_0 = ReduceSum[keepdims = 0](%/classifier/Mul_output_0, %onnx::ReduceSum_698)\n",
      "  %/classifier/linear/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/classifier/ReduceSum_output_0, %classifier.linear.weight, %classifier.linear.bias)\n",
      "  %701 = LogSoftmax[axis = 1](%/classifier/linear/Gemm_output_0)\n",
      "  return %701\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnx.helper\n",
    "\n",
    "onnx_model = onnx.load('./constellator.onnx')\n",
    "print(onnx.helper.printable_graph(onnx_model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model size: 22.00 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f\"ONNX model size: {os.path.getsize('./constellator.onnx') / 1048576:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
