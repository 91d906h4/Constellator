{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import jieba\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from data import Data\n",
    "from torch import nn, optim\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed.\n",
    "random_seed = 0\n",
    "\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    0: \"牡羊\",\n",
    "    1: \"金牛\",\n",
    "    2: \"雙子\",\n",
    "    3: \"巨蠍\",\n",
    "    4: \"獅子\",\n",
    "    5: \"處女\",\n",
    "    6: \"天秤\",\n",
    "    7: \"天蠍\",\n",
    "    8: \"射手\",\n",
    "    9: \"魔羯\",\n",
    "    10: \"水瓶\",\n",
    "    11: \"雙魚\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning completed.\n",
      "ToDataset completed.\n",
      "Argumantation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.593 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenlization completed.\n",
      "Padding completed.\n",
      "Token2id completed.\n",
      "Process completed.\n"
     ]
    }
   ],
   "source": [
    "raw = {i: open(f\"./dataset/{classes[i]}.txt\", encoding=\"utf-8\").read() for i in range(12)}\n",
    "data = Data(data=raw, padding_length=32)\n",
    "\n",
    "train_raw = data.get(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5627"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4273,\n",
       "  1555,\n",
       "  5312,\n",
       "  2569,\n",
       "  2128,\n",
       "  5213,\n",
       "  4714,\n",
       "  3737,\n",
       "  444,\n",
       "  3591,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432,\n",
       "  4432],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, data: list, label: list):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index]), torch.tensor(self.label[index], dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, l = [], []\n",
    "\n",
    "for i, j in train_raw:\n",
    "    d.append(i); l.append(j)\n",
    "\n",
    "train_ds = CreateDataset(d, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5213, 4714, 3737,  444, 3591, 2128, 4273, 1555, 5312, 2569, 4432, 4432,\n",
       "         4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432,\n",
       "         4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432]),\n",
       " tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.linear1 = nn.Linear(256, 24)\n",
    "        self.linear2 = nn.Linear(24, 1)\n",
    "        self.relu = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        b = x.size(0)\n",
    "        x = x.reshape(-1, 256)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        x = x.reshape(b, -1)\n",
    "        x = torch.softmax(x, dim=1)\n",
    "\n",
    "        return x.unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionClassifier, self).__init__()\n",
    "        self.attention = Attention()\n",
    "        self.linear = nn.Linear(256, 12)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        attention = self.attention(x)\n",
    "\n",
    "        x = (x * attention).sum(dim=1)\n",
    "        x = torch.log_softmax(self.linear(x), dim=1)\n",
    "\n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(data.get(\"token_len\"), 64)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=64,\n",
    "            hidden_size=256,\n",
    "            num_layers=4,\n",
    "            dropout=0.5,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.linear = nn.Linear(16384, 12)\n",
    "        self.classifier = AttentionClassifier()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        x = x[:, :, :256] + x[:, :, :256]\n",
    "\n",
    "        x, _ = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traning history data.\n",
    "train_accuracy_h = []\n",
    "train_loss_h = []\n",
    "validate_accuracy_h = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs: int, model: nn.Module, optimizer: optim.Optimizer, loss: nn.CrossEntropyLoss, dataloader: DataLoader):\n",
    "    # Set model to training mode.\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_accuracy = 0\n",
    "        train_loss = 0\n",
    "        train_total = 0\n",
    "        train_process = 0\n",
    "        train_time = datetime.now().timestamp()\n",
    "\n",
    "        for texts, labels in dataloader:\n",
    "            texts: torch.Tensor\n",
    "            labels: torch.Tensor\n",
    "\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs: torch.Tensor = model(texts)\n",
    "            losses: torch.Tensor = loss(outputs, labels)\n",
    "\n",
    "            # optimizer.zero_grad()\n",
    "            for param in model.parameters(): param.grad = None\n",
    "\n",
    "            # Backpropagation.\n",
    "            losses.backward()\n",
    "\n",
    "            # Update parameters.\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predict = torch.max(outputs, 1)\n",
    "            train_accuracy += sum([labels[i][predict[i]] == 1 for i in range(len(predict))])\n",
    "            train_loss += losses.item()\n",
    "            train_total += labels.shape[0]\n",
    "            train_process += 1\n",
    "\n",
    "            print(\n",
    "                f\"{datetime.now().strftime('%Y/%m/%d %H:%M:%S')} \"\n",
    "                f\"Epoch: {epoch:03d} \"\n",
    "                f\"Time: {datetime.now().timestamp() - train_time:.2f} \"\n",
    "                f\"Process: {train_process / len(dataloader) * 100:.2f}% \"\n",
    "                f\"Accuracy: {train_accuracy / train_total * 100:.2f}% \"\n",
    "                f\"Loss: {train_loss:.3f}\",\n",
    "                end=\"\\r\"\n",
    "            )\n",
    "\n",
    "        train_accuracy_h.append(train_accuracy / train_total * 100)\n",
    "        train_loss_h.append(train_loss)\n",
    "\n",
    "        print()\n",
    "\n",
    "        # Early stop.\n",
    "        if train_accuracy / train_total > 0.95:\n",
    "            print(\"Early stopped.\")\n",
    "            break\n",
    "\n",
    "    # Set model to evaluation mode.\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024/04/01 17:59:06 Epoch: 000 Time: 15.32 Process: 100.00% Accuracy: 10.09% Loss: 1749.044\n",
      "2024/04/01 17:59:20 Epoch: 001 Time: 13.68 Process: 100.00% Accuracy: 11.23% Loss: 1743.776\n",
      "2024/04/01 17:59:34 Epoch: 002 Time: 13.93 Process: 100.00% Accuracy: 11.52% Loss: 1741.714\n",
      "2024/04/01 17:59:48 Epoch: 003 Time: 13.79 Process: 100.00% Accuracy: 12.46% Loss: 1735.675\n",
      "2024/04/01 18:00:04 Epoch: 004 Time: 15.76 Process: 100.00% Accuracy: 13.95% Loss: 1722.923\n",
      "2024/04/01 18:00:18 Epoch: 005 Time: 14.07 Process: 100.00% Accuracy: 17.04% Loss: 1673.418\n",
      "2024/04/01 18:00:34 Epoch: 006 Time: 16.41 Process: 100.00% Accuracy: 21.91% Loss: 1566.390\n",
      "2024/04/01 18:00:49 Epoch: 007 Time: 15.23 Process: 100.00% Accuracy: 31.12% Loss: 1418.244\n",
      "2024/04/01 18:01:04 Epoch: 008 Time: 14.98 Process: 100.00% Accuracy: 38.08% Loss: 1282.270\n",
      "2024/04/01 18:01:19 Epoch: 009 Time: 14.29 Process: 100.00% Accuracy: 44.43% Loss: 1150.651\n",
      "2024/04/01 18:01:33 Epoch: 010 Time: 14.34 Process: 100.00% Accuracy: 52.27% Loss: 993.300\n",
      "2024/04/01 18:01:48 Epoch: 011 Time: 14.86 Process: 100.00% Accuracy: 60.89% Loss: 816.023\n",
      "2024/04/01 18:02:02 Epoch: 012 Time: 14.58 Process: 100.00% Accuracy: 71.17% Loss: 617.543\n",
      "2024/04/01 18:02:16 Epoch: 013 Time: 14.02 Process: 100.00% Accuracy: 79.39% Loss: 439.890\n",
      "2024/04/01 18:02:31 Epoch: 014 Time: 14.47 Process: 100.00% Accuracy: 85.69% Loss: 308.605\n",
      "2024/04/01 18:02:45 Epoch: 015 Time: 14.20 Process: 100.00% Accuracy: 90.87% Loss: 200.005\n",
      "2024/04/01 18:02:59 Epoch: 016 Time: 14.28 Process: 100.00% Accuracy: 92.61% Loss: 159.303\n",
      "2024/04/01 18:03:13 Epoch: 017 Time: 14.11 Process: 100.00% Accuracy: 95.75% Loss: 96.231\n",
      "Early stopped.\n"
     ]
    }
   ],
   "source": [
    "train(epochs=epochs, model=model, optimizer=optimizer, loss=loss, dataloader=train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5269, 2867, 4689, 2962, 1035, 3909, 4689, 5439, 2129, 3287,  165, 4077,\n",
      "         3889, 5329, 2136, 4245, 3591, 4432, 4432, 4432, 4432, 4432, 4432, 4432,\n",
      "         4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432],\n",
      "        [5018, 2115, 2001, 1565, 1190, 2984, 3591, 4689, 1565, 2424, 3473, 4689,\n",
      "         3933, 4901, 3521, 1565, 3382, 3441, 4432, 4432, 4432, 4432, 4432, 4432,\n",
      "         4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432],\n",
      "        [1102, 4689, 1363, 3745, 3737,  602, 5374, 1043, 4749, 2895, 4077, 5248,\n",
      "         3591, 4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432,\n",
      "         4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432],\n",
      "        [4238, 2115, 4273, 1555, 5216, 1085, 3737, 3564, 3591, 2128, 3297,  208,\n",
      "         3564, 3737, 3369, 1436,  809, 4858, 3737, 5386, 4432, 4432, 4432, 4432,\n",
      "         4432, 4432, 4432, 4432, 4432, 4432, 4432, 4432],\n",
      "        [3018, 4068, 4689, 3149, 4273, 2934, 1642, 3737, 3564, 4689, 1363, 3745,\n",
      "          479, 1830, 2228, 3737,   68, 4689, 1224, 1363, 3745, 3004, 1259, 4077,\n",
      "          602, 3010, 3078, 3454,  580, 3591, 4432, 4432],\n",
      "        [2926, 4689, 3584, 3908, 4613, 4689, 3908, 4173, 4689, 2709, 2609, 4689,\n",
      "         4798, 4908, 3117, 4689, 3129, 3737,  992, 2002, 4660, 3985, 1426, 3403,\n",
      "          516, 3336, 3591, 4689, 1137, 5127, 4689, 3441],\n",
      "        [2239, 4689,  334, 4689, 2962, 2842, 3966, 4689, 5297, 3591, 4689, 5422,\n",
      "         5235, 4689, 2980,  872, 4689, 5422, 3926, 1119, 3737,  718, 4689, 1922,\n",
      "         1328, 4689, 3441, 2709, 3660, 4689, 5422,  509],\n",
      "        [  77, 2397,  114, 4141, 4077, 2829, 5107, 3591, 4689, 3149, 3004,  602,\n",
      "         3010, 4295, 4829, 4459, 3909, 4689, 3018, 3737,  490, 3382, 4689, 3042,\n",
      "         4077, 3135, 3737,   68, 4432, 4432, 4432, 4432]])\n",
      "tensor([[-1.2010e+01, -2.7287e+01, -1.4348e+01, -1.6447e+01, -7.7486e-06,\n",
      "         -1.7910e+01, -2.5754e+01, -2.1535e+01, -2.5399e+01, -1.8095e+01,\n",
      "         -2.2465e+01, -1.3857e+01],\n",
      "        [-1.5026e+01, -1.2145e+01, -9.2144e+00, -1.2835e+01, -1.3187e+01,\n",
      "         -1.6787e+01, -1.1840e+01, -2.1129e+01, -6.6460e+00, -1.4282e+01,\n",
      "         -1.5377e-02, -4.2800e+00],\n",
      "        [-8.2540e+00, -1.6040e+01, -9.8840e+00, -1.6320e+01, -1.5182e+01,\n",
      "         -1.3194e+01, -1.2903e+01, -3.5399e-04, -1.0182e+01, -1.9424e+01,\n",
      "         -2.0123e+01, -1.7796e+01],\n",
      "        [-1.8706e+01, -1.7121e+01, -1.7142e+01, -1.0705e+01, -7.7784e+00,\n",
      "         -7.1894e+00, -2.0710e+01, -2.7414e+01, -1.7216e+01, -1.2102e+01,\n",
      "         -7.7139e+00, -1.6492e-03],\n",
      "        [-1.9234e+01, -2.6890e-04, -1.7100e+01, -1.1853e+01, -2.0708e+01,\n",
      "         -1.2298e+01, -9.7612e+00, -1.1835e+01, -1.4205e+01, -9.0664e+00,\n",
      "         -9.4838e+00, -1.8233e+01],\n",
      "        [-1.2673e+01, -1.7303e+01, -1.6009e-03, -2.5994e+01, -7.9538e+00,\n",
      "         -6.7137e+00, -1.7921e+01, -1.0387e+01, -2.0782e+01, -1.6989e+01,\n",
      "         -2.8065e+01, -1.8055e+01],\n",
      "        [-5.3057e+00, -1.9397e+00, -1.1968e+01, -1.4266e+01, -6.8820e+00,\n",
      "         -3.7715e+00, -1.2076e+01, -7.5905e+00, -9.9553e+00, -1.9038e-01,\n",
      "         -1.0439e+01, -1.6657e+01],\n",
      "        [-9.7934e+00, -9.9843e+00, -7.6653e+00, -7.7615e+00, -1.0873e+01,\n",
      "         -1.7980e+01, -1.1696e+01, -1.4718e+01, -6.8661e+00, -1.2215e+01,\n",
      "         -2.8200e-03, -7.2031e+00]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "for i, j in train_dl:\n",
    "    print(i)\n",
    "    print(model(i.to(device)))\n",
    "    print(j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = [\n",
    "    \"勇敢無畏，充滿了活力和冒險精神，他們喜歡追求挑戰，敢於冒險，常常是行動派的領導者。\",\n",
    "    \"穩重可靠，以堅韌的意志力和耐心著稱，他們注重安全和舒適，並對物質生活有著強烈的執著。\",\n",
    "    \"機智聰明，好奇心旺盛，喜歡交際和表達自己，具有多才多藝的特質，常常充滿了靈活的思維和活力。\",\n",
    "    \"情感豐富，善解人意，對家庭和親密關係非常重視，他們總是充滿了溫柔和關懷，是很好的傾聽者和支持者。\",\n",
    "    \"自信大方，追求著成為焦點的慾望，他們充滿了熱情和活力，喜歡引領和影響身邊的人，時常展現出優越感和領導能力。\",\n",
    "    \"細心謹慎，追求完美，他們善於分析和解決問題，注重細節和有組織性，常常是值得信賴的夥伴和顧問。\",\n",
    "    \"追求和諧，優雅而公正，他們注重平衡和公平，善於溝通協調，是很好的調解者和中介者。\",\n",
    "    \"神秘內斂，充滿了熱情和直覺，他們擁有強烈的意志力和洞察力，常常是充滿挑戰性和魅力的個體。\",\n",
    "    \"自由奔放，熱愛冒險和探索，他們追求著廣闊的視野和新鮮的體驗，時常充滿了樂觀和幽默。\",\n",
    "    \"勤奮負責，追求事業成功和社會地位，他們具有堅毅的意志力和耐心，常常是穩健和實際的決策者。\",\n",
    "    \"獨立思考，充滿了理想主義和創意，他們追求著獨特的生活方式和社會價值觀，常常是前衛和不拘一格的個體。\",\n",
    "    \"敏感善良，充滿了同情心和想像力，他們常常是理想主義者和夢想家，追求著內心的情感和精神實踐。\",\n",
    "    \"勇敢無畏，充滿活力，喜歡追求挑戰，常常是行動派的領導者。\",\n",
    "    \"穩重可靠，堅韌耐心，注重安全舒適，對物質生活有強烈執著。\",\n",
    "    \"機智聰明，好奇心旺盛，善於交際表達，充滿靈活思維和活力。\",\n",
    "    \"情感豐富，善解人意，重視家庭和親密關係，溫柔關懷，傾聽支持者。\",\n",
    "    \"自信大方，追求成為焦點，充滿熱情活力，喜歡引領影響身邊的人。\",\n",
    "    \"細心謹慎，追求完美，善於分析解決問題，注重細節有組織性。\",\n",
    "    \"追求和諧，優雅公正，注重平衡公平，善於溝通協調。\",\n",
    "    \"神秘內斂，熱情直覺，意志力洞察力強，充滿挑戰性和魅力。\",\n",
    "    \"自由奔放，熱愛冒險探索，追求廣闊視野和新鮮體驗，樂觀幽默。\",\n",
    "    \"勤奮負責，追求事業成功社會地位，具堅毅意志力和耐心，穩健決策者。\",\n",
    "    \"獨立思考，理想主義創意，追求獨特生活方式和價值觀，前衛不拘一格。\",\n",
    "    \"敏感善良，同情心想像力豐富，理想主義夢想家，追求內心情感精神實踐。\",\n",
    "    \"勇敢果敢，充滿活力，愛冒險。\",\n",
    "    \"穩重堅定，堅持自我價值觀。\",\n",
    "    \"靈活機智，善於溝通交際。\",\n",
    "    \"情感豐富，家庭意識強。\",\n",
    "    \"自信領導，熱情奔放。\",\n",
    "    \"細心謹慎，追求完美。\",\n",
    "    \"追求和諧，公正公平。\",\n",
    "    \"神秘敏感，探索深度。\",\n",
    "    \"自由探險，樂觀向上。\",\n",
    "    \"勤奮穩健，追求成功。\",\n",
    "    \"獨立創新，理想主義者。\",\n",
    "    \"敏感浪漫，夢想家。\",\n",
    "    \"生命充滿了勇氣與活力，他們勇於面對挑戰，永不退縮，常常是行動派的領袖，樂於率先嘗試新事物。\",\n",
    "    \"展現出穩重且堅定的品性，他們始終堅持自身的價值觀與信念，喜歡在自己熟悉的領域中深耕不輟。\",\n",
    "    \"擁有機智敏捷的頭腦，他們善於溝通交際，充滿活力與靈活的思維，對於各種新奇的事物都抱有濃厚的好奇心。\",\n",
    "    \"的人情感豐富且懂得關懷，他們對家庭有著極為強烈的連結感，願意為了家人無條件地付出與奉獻。\",\n",
    "    \"展現出極度的自信與熱情，他們常常是眾人注目的焦點，充滿著領導力與活力，喜歡成為團體中的中心人物。\",\n",
    "    \"的人細心謹慎，追求完美與規律，他們擁有出色的分析能力與解決問題的技巧，喜歡保持事情的有序與井然有序。\",\n",
    "    \"追求和諧與公正，他們擅長於溝通協調，注重平衡與公平，善於解決衝突，是團隊中不可或缺的調解者。\",\n",
    "    \"的人神秘內斂，充滿了深度與直覺，他們擁有強烈的意志力與洞察力，總是對事情有著深入的探索與研究。\",\n",
    "    \"熱愛自由與探險，他們樂觀向上，喜歡挑戰與冒險，追求著廣闊的視野與新鮮的體驗，不斷探索未知的領域。\",\n",
    "    \"的人勤奮穩健，追求事業成功與社會地位，他們具有堅毅的意志力與耐心，是穩健可靠的決策者與領導者。\",\n",
    "    \"獨立創新，理想主義者，他們勇於打破傳統的束縛，追求獨特的生活方式與社會價值觀，常常是前衛與不拘一格的個體。\",\n",
    "    \"的人敏感浪漫，是真正的夢想家，他們充滿了同情心與想像力，常常是理想主義者，追求內心情感與精神的實踐。\",\n",
    "]\n",
    "\n",
    "answare = [\n",
    "    '牡羊', '金牛', '雙子', '巨蠍', '獅子', '處女', '天秤', '天蠍', '射手', '魔羯', '水瓶', '雙魚',\n",
    "    '牡羊', '金牛', '雙子', '巨蠍', '獅子', '處女', '天秤', '天蠍', '射手', '魔羯', '水瓶', '雙魚',\n",
    "    '牡羊', '金牛', '雙子', '巨蠍', '獅子', '處女', '天秤', '天蠍', '射手', '魔羯', '水瓶', '雙魚',\n",
    "    '牡羊', '金牛', '雙子', '巨蠍', '獅子', '處女', '天秤', '天蠍', '射手', '魔羯', '水瓶', '雙魚',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(testset: list) -> torch.Tensor:\n",
    "    result = []\n",
    "\n",
    "    for line in testset:\n",
    "        temp = jieba.lcut(line)\n",
    "        temp = temp + [\"<PAD>\"] * (32 - len(temp))\n",
    "        temp = [data.w2i[x] if x in data.w2i else data.w2i[\"<PAD>\"] for x in temp][:32]\n",
    "        result.append(temp)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = process(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding): Embedding(5470, 64)\n",
       "  (lstm): LSTM(64, 256, num_layers=4, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (linear): Linear(in_features=16384, out_features=12, bias=True)\n",
       "  (classifier): AttentionClassifier(\n",
       "    (attention): Attention(\n",
       "      (linear1): Linear(in_features=256, out_features=24, bias=True)\n",
       "      (linear2): Linear(in_features=24, out_features=1, bias=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (linear): Linear(in_features=256, out_features=12, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-9.5898e-03, -1.6573e+01, -1.0449e+01, -1.9202e+01, -7.8233e+00,\n",
      "         -1.1584e+01, -1.7885e+01, -1.7001e+01, -4.6994e+00, -1.6643e+01,\n",
      "         -1.2480e+01, -1.4074e+01],\n",
      "        [-5.6875e+00, -2.8133e+00, -1.1723e+01, -6.2441e+00, -8.2075e+00,\n",
      "         -3.2032e+00, -5.8264e+00, -4.2164e-01, -5.6591e+00, -1.4662e+00,\n",
      "         -7.5372e+00, -1.1250e+01],\n",
      "        [-6.9632e+00, -1.0731e+01, -9.5585e-02, -1.3813e+01, -6.8964e+00,\n",
      "         -6.3424e+00, -3.0259e+00, -9.5691e+00, -3.6242e+00, -1.7827e+01,\n",
      "         -1.1207e+01, -4.4106e+00],\n",
      "        [-1.7587e+01, -1.3591e+00, -8.8564e+00, -3.4010e+00, -1.4686e+01,\n",
      "         -1.0288e+01, -6.4250e-01, -1.2804e+01, -8.5606e+00, -1.2221e+01,\n",
      "         -1.7584e+00, -4.5014e+00],\n",
      "        [-1.5679e-02, -1.3361e+01, -6.5882e+00, -1.2985e+01, -5.9097e+00,\n",
      "         -1.2428e+01, -1.5508e+01, -1.1656e+01, -4.4905e+00, -1.4022e+01,\n",
      "         -8.4164e+00, -1.1201e+01],\n",
      "        [-8.1932e+00, -8.1881e-02, -1.1450e+01, -1.5450e+01, -1.1718e+01,\n",
      "         -2.7873e+00, -6.0297e+00, -1.4304e+01, -4.4624e+00, -7.1104e+00,\n",
      "         -6.2331e+00, -1.1287e+01],\n",
      "        [-2.9005e+01, -1.2985e+01, -1.5571e+01, -1.4534e+01, -2.2707e+01,\n",
      "         -1.4898e+01, -5.5908e-05, -1.9480e+01, -1.5473e+01, -2.5246e+01,\n",
      "         -1.6634e+01, -9.8593e+00],\n",
      "        [-8.6603e-01, -1.2679e+01, -1.2176e+00, -1.4544e+01, -8.7711e+00,\n",
      "         -1.0732e+01, -1.0461e+01, -6.2352e+00, -1.2688e+00, -1.8455e+01,\n",
      "         -1.0505e+01, -9.6638e+00],\n",
      "        [-7.6272e+00, -1.3953e+01, -1.0184e+01, -2.0411e+01, -1.7579e+01,\n",
      "         -1.6903e+01, -1.1749e+01, -1.6261e+01, -8.9450e-04, -2.0529e+01,\n",
      "         -7.9316e+00, -1.3728e+01],\n",
      "        [-6.0765e+00, -3.6830e+00, -9.1412e+00, -8.0926e+00, -7.9465e+00,\n",
      "         -7.3781e+00, -7.2446e+00, -1.1537e+01, -2.1983e+00, -5.4399e+00,\n",
      "         -1.5874e-01, -6.2661e+00],\n",
      "        [-1.3130e+01, -1.0000e+01, -1.4247e+01, -1.0765e+01, -1.6749e+01,\n",
      "         -1.7836e+01, -1.6545e+01, -1.9694e+01, -7.8880e+00, -1.2317e+01,\n",
      "         -4.8506e-04, -1.0233e+01],\n",
      "        [-1.9624e+01, -1.5179e+01, -1.3053e+01, -7.0101e+00, -1.3716e+01,\n",
      "         -1.3402e+01, -1.7615e+01, -2.2656e+01, -1.3200e+01, -1.7770e+01,\n",
      "         -3.8204e+00, -2.3094e-02],\n",
      "        [-1.3553e-02, -1.6356e+01, -1.0193e+01, -2.2144e+01, -1.0890e+01,\n",
      "         -1.3434e+01, -1.8651e+01, -1.6511e+01, -4.3124e+00, -1.8135e+01,\n",
      "         -1.3236e+01, -1.7061e+01],\n",
      "        [-1.6801e+01, -1.7724e+00, -1.8152e+01, -2.2846e+00, -1.5264e+01,\n",
      "         -1.0091e+01, -1.2784e+01, -1.2806e+01, -1.2201e+01, -1.2930e+00,\n",
      "         -7.9029e-01, -1.0332e+01],\n",
      "        [-1.4478e+01, -1.2619e+01, -2.8005e+00, -1.3240e+01, -8.0116e+00,\n",
      "         -4.6862e+00, -3.6410e+00, -1.2819e+01, -8.1083e+00, -1.8482e+01,\n",
      "         -1.2134e+01, -1.0189e-01],\n",
      "        [-2.3353e+01, -4.6606e+00, -1.7443e+01, -1.9891e-01, -1.7649e+01,\n",
      "         -1.5286e+01, -1.0602e+01, -1.8540e+01, -1.7715e+01, -1.0081e+01,\n",
      "         -1.7699e+00, -7.6041e+00],\n",
      "        [-2.6237e-02, -1.2904e+01, -4.8783e+00, -1.8230e+01, -4.3330e+00,\n",
      "         -1.0512e+01, -1.6683e+01, -1.3865e+01, -5.2824e+00, -1.2901e+01,\n",
      "         -1.0112e+01, -1.2299e+01],\n",
      "        [-1.6032e+01, -4.6937e+00, -1.5903e+01, -1.3953e+01, -1.3866e+01,\n",
      "         -1.0152e-02, -9.6158e+00, -2.0635e+01, -1.0459e+01, -1.0138e+01,\n",
      "         -9.1267e+00, -7.2622e+00],\n",
      "        [-3.1307e+01, -1.2654e+01, -1.8422e+01, -1.5729e+01, -2.5525e+01,\n",
      "         -1.6199e+01, -3.8147e-06, -1.8930e+01, -1.9081e+01, -2.5975e+01,\n",
      "         -2.0409e+01, -1.4721e+01],\n",
      "        [-5.4472e+00, -1.0953e+01, -1.9051e-01, -1.3505e+01, -7.8743e+00,\n",
      "         -3.8087e+00, -6.4390e+00, -2.9333e+00, -2.4472e+00, -1.7289e+01,\n",
      "         -1.3153e+01, -5.2555e+00],\n",
      "        [-7.0540e+00, -9.0068e+00, -1.1416e+01, -1.8298e+01, -1.6133e+01,\n",
      "         -1.5278e+01, -1.2813e+01, -1.6126e+01, -2.5293e-01, -1.1903e+01,\n",
      "         -1.5030e+00, -1.2965e+01],\n",
      "        [-1.2618e+01, -4.7035e+00, -1.6764e+01, -1.0302e+01, -1.1875e+01,\n",
      "         -1.0606e+01, -1.4385e+01, -1.6725e+01, -9.4765e+00, -8.8967e-01,\n",
      "         -5.4475e-01, -1.0998e+01],\n",
      "        [-1.4081e+01, -7.0253e+00, -1.4594e+01, -7.7612e+00, -1.7269e+01,\n",
      "         -1.6059e+01, -1.4752e+01, -1.8123e+01, -8.6595e+00, -1.1180e+01,\n",
      "         -1.5517e-03, -9.9799e+00],\n",
      "        [-2.2077e+01, -1.8283e+01, -1.4563e+01, -2.0250e+00, -1.2543e+01,\n",
      "         -1.4507e+01, -1.6557e+01, -1.9032e+01, -1.7733e+01, -1.9398e+01,\n",
      "         -8.2265e+00, -1.4188e-01],\n",
      "        [-5.6273e+00, -1.6219e+01, -3.2416e+00, -1.9077e+01, -7.0741e-02,\n",
      "         -3.6951e+00, -1.8172e+01, -1.6953e+01, -1.4102e+01, -1.4534e+01,\n",
      "         -1.7297e+01, -7.1933e+00],\n",
      "        [-6.8460e+00, -5.3506e+00, -7.4386e+00, -7.4036e+00, -1.0911e+01,\n",
      "         -7.8596e+00, -3.0641e+00, -7.4337e-02, -4.0568e+00, -1.0438e+01,\n",
      "         -8.7909e+00, -9.9334e+00],\n",
      "        [-6.8910e+00, -1.3556e+01, -2.5676e-01, -9.9211e+00, -1.5700e+00,\n",
      "         -1.0977e+01, -7.0001e+00, -7.2278e+00, -7.2523e+00, -1.5673e+01,\n",
      "         -1.0715e+01, -4.2036e+00],\n",
      "        [-1.1813e+01, -1.2816e+01, -1.3926e+01, -3.9163e-03, -9.0433e+00,\n",
      "         -1.1112e+01, -1.0305e+01, -8.5571e+00, -1.1540e+01, -1.1174e+01,\n",
      "         -7.9275e+00, -5.7589e+00],\n",
      "        [-5.8436e-02, -1.5765e+01, -4.6901e+00, -1.3068e+01, -3.0818e+00,\n",
      "         -1.1152e+01, -1.3616e+01, -7.2001e+00, -7.0266e+00, -1.4737e+01,\n",
      "         -1.2536e+01, -1.0022e+01],\n",
      "        [-1.4152e+00, -4.3927e+00, -6.9848e+00, -1.5213e+01, -7.3204e+00,\n",
      "         -5.0870e-01, -1.0325e+01, -8.9706e+00, -1.9631e+00, -7.0501e+00,\n",
      "         -7.8641e+00, -9.7265e+00],\n",
      "        [-1.4411e+01, -7.2287e+00, -4.6059e+00, -7.2370e+00, -1.1927e+01,\n",
      "         -1.1022e+01, -1.7253e-02, -7.2851e+00, -8.4831e+00, -1.6481e+01,\n",
      "         -8.4965e+00, -5.3933e+00],\n",
      "        [-1.4382e+01, -7.3867e+00, -3.6287e-01, -1.4376e+01, -1.1990e+01,\n",
      "         -1.6989e+00, -2.5878e+00, -3.1009e+00, -1.0561e+01, -1.6944e+01,\n",
      "         -1.7547e+01, -7.4267e+00],\n",
      "        [-5.4653e+00, -1.2564e+01, -1.0417e+01, -1.9442e+01, -1.4715e+01,\n",
      "         -7.4666e+00, -1.2858e+01, -1.3938e+01, -4.8975e-03, -1.7398e+01,\n",
      "         -1.0457e+01, -1.1021e+01],\n",
      "        [-1.0883e+01, -1.2937e+00, -1.3165e+01, -9.7140e+00, -1.1792e+01,\n",
      "         -9.3416e+00, -1.2767e+01, -1.7249e+01, -1.0477e+01, -2.0144e+00,\n",
      "         -5.2403e-01, -1.1116e+01],\n",
      "        [-1.9528e+00, -8.5813e+00, -5.5388e+00, -2.2241e+00, -3.8190e+00,\n",
      "         -1.0428e+01, -8.1093e+00, -6.6778e+00, -2.4155e+00, -7.5135e+00,\n",
      "         -5.0904e-01, -3.4623e+00],\n",
      "        [-1.5496e+01, -1.0670e+01, -6.5398e+00, -4.8223e+00, -8.5311e+00,\n",
      "         -3.4326e+00, -6.2476e+00, -7.2441e+00, -1.1585e+01, -1.4049e+01,\n",
      "         -1.0930e+01, -4.5724e-02],\n",
      "        [-7.5868e-01, -8.9038e+00, -6.9353e+00, -1.2649e+01, -8.2531e+00,\n",
      "         -8.9752e+00, -1.2842e+01, -1.3573e+01, -7.3188e-01, -1.0989e+01,\n",
      "         -3.0165e+00, -8.4320e+00],\n",
      "        [-7.3338e+00, -8.9245e+00, -3.9627e+00, -1.0431e+01, -1.1227e+01,\n",
      "         -1.2340e+01, -2.4192e+00, -4.3980e+00, -1.3121e-01, -1.6475e+01,\n",
      "         -6.8182e+00, -7.2172e+00],\n",
      "        [-4.3126e+00, -9.7179e+00, -8.1616e-02, -1.1235e+01, -2.9675e+00,\n",
      "         -7.4647e+00, -7.1173e+00, -5.9441e+00, -4.8147e+00, -1.2540e+01,\n",
      "         -1.0203e+01, -6.6350e+00],\n",
      "        [-1.4258e+01, -7.1490e+00, -1.3942e+01, -1.1809e+00, -1.2202e+01,\n",
      "         -1.4719e+01, -1.1645e+01, -1.2501e+01, -1.2252e+01, -5.6499e+00,\n",
      "         -3.7352e-01, -7.9633e+00],\n",
      "        [-2.0997e+00, -1.5040e+01, -6.7410e+00, -1.9779e+01, -1.2648e+01,\n",
      "         -1.2784e+01, -1.3745e+01, -1.4969e+01, -1.3206e-01, -2.0789e+01,\n",
      "         -1.0757e+01, -1.2789e+01],\n",
      "        [-4.5414e+00, -3.3181e+00, -2.8999e+00, -8.2311e+00, -6.4805e+00,\n",
      "         -1.2247e+00, -1.7736e+00, -5.0648e+00, -9.6596e-01, -1.0824e+01,\n",
      "         -5.9220e+00, -3.1442e+00],\n",
      "        [-2.3630e+01, -1.2549e+01, -1.2383e+01, -8.9815e+00, -1.7418e+01,\n",
      "         -1.3339e+01, -1.6879e-02, -1.7089e+01, -1.1634e+01, -2.1743e+01,\n",
      "         -1.0672e+01, -4.1002e+00],\n",
      "        [-1.2007e+01, -6.9982e+00, -5.0328e+00, -8.5354e+00, -1.2291e+01,\n",
      "         -8.8984e+00, -1.2040e-01, -2.2543e+00, -7.5025e+00, -1.5197e+01,\n",
      "         -1.1885e+01, -8.7621e+00],\n",
      "        [-9.3255e+00, -7.6904e+00, -1.0604e+01, -1.4837e+01, -1.4570e+01,\n",
      "         -9.8359e+00, -1.0495e+01, -1.7089e+01, -3.8851e-01, -1.2204e+01,\n",
      "         -1.1385e+00, -6.9255e+00],\n",
      "        [-1.1224e+01, -6.5569e+00, -1.0337e+01, -1.0467e+01, -1.4428e+01,\n",
      "         -1.3407e+01, -6.1840e+00, -1.4945e+01, -2.7065e+00, -1.1735e+01,\n",
      "         -7.3759e-02, -7.1728e+00],\n",
      "        [-1.9711e+01, -1.1249e+01, -1.8516e+01, -1.2269e+01, -2.0497e+01,\n",
      "         -2.1874e+01, -1.8878e+01, -2.5355e+01, -1.2918e+01, -1.3593e+01,\n",
      "         -2.7656e-05, -1.1985e+01],\n",
      "        [-1.2886e+01, -3.2954e+00, -7.9377e+00, -1.6540e+00, -1.2086e+01,\n",
      "         -4.9148e+00, -3.1033e+00, -9.5889e+00, -3.9081e+00, -1.0273e+01,\n",
      "         -1.1892e+00, -9.3038e-01]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "result = model(torch.tensor(test).to(device))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normolization\n",
    "results = []\n",
    "\n",
    "for i in range(len(result)):\n",
    "    temp = result[i]\n",
    "    temp = [x - min(temp) for x in temp]\n",
    "    temp = [x / max(temp) for x in temp]\n",
    "    temp = [round(x, 3) for x in temp]\n",
    "\n",
    "    results.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "牡羊✅\n",
      "天蠍 <- 金牛\n",
      "雙子✅\n",
      "天秤 <- 巨蠍\n",
      "牡羊 <- 獅子\n",
      "金牛 <- 處女\n",
      "天秤✅\n",
      "牡羊 <- 天蠍\n",
      "射手✅\n",
      "水瓶 <- 魔羯\n",
      "水瓶✅\n",
      "雙魚✅\n",
      "牡羊✅\n",
      "水瓶 <- 金牛\n",
      "雙魚 <- 雙子\n",
      "巨蠍✅\n",
      "牡羊 <- 獅子\n",
      "處女✅\n",
      "天秤✅\n",
      "雙子 <- 天蠍\n",
      "射手✅\n",
      "水瓶 <- 魔羯\n",
      "水瓶✅\n",
      "雙魚✅\n",
      "獅子 <- 牡羊\n",
      "天蠍 <- 金牛\n",
      "雙子✅\n",
      "巨蠍✅\n",
      "牡羊 <- 獅子\n",
      "處女✅\n",
      "天秤✅\n",
      "雙子 <- 天蠍\n",
      "射手✅\n",
      "水瓶 <- 魔羯\n",
      "水瓶✅\n",
      "雙魚✅\n",
      "射手 <- 牡羊\n",
      "射手 <- 金牛\n",
      "雙子✅\n",
      "水瓶 <- 巨蠍\n",
      "射手 <- 獅子\n",
      "射手 <- 處女\n",
      "天秤✅\n",
      "天秤 <- 天蠍\n",
      "射手✅\n",
      "水瓶 <- 魔羯\n",
      "水瓶✅\n",
      "雙魚✅\n",
      "Correct: 25 (52.08%)\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    t = classes[int(torch.argmax(torch.tensor(r)))]\n",
    "    if t == answare[i]:\n",
    "        count += 1\n",
    "        print(t + \"✅\")\n",
    "    else:\n",
    "        print(t + \" <- \" + answare[i])\n",
    "\n",
    "print(f\"Correct: {count} ({(count / len(answare)) * 100 :.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# angles_A = np.linspace(start=0, stop=2*np.pi, num=len(result)+1, endpoint=True)\n",
    "# values_A = np.concatenate((result, [result[0]]))\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(5, 5), subplot_kw={'projection': 'polar'})\n",
    "# ax.plot(angles_A, values_A, 'o-', color=\"blue\", label=\"A\")\n",
    "\n",
    "# ax.fill(angles_A, values_A, alpha=0.3, color=\"blue\")\n",
    "# ax.set_thetagrids(angles_A[:-1] * 180 / np.pi, range(12), fontsize=15)\n",
    "# ax.set_theta_zero_location('N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:4315: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\onnx\\_internal\\jit_utils.py:258: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\jit\\passes\\onnx\\shape_type_inference.cpp:1888.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "c:\\Users\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\onnx\\utils.py:687: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\jit\\passes\\onnx\\shape_type_inference.cpp:1888.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "c:\\Users\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\onnx\\utils.py:1178: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\jit\\passes\\onnx\\shape_type_inference.cpp:1888.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "# Please check the virtual input of the ONNX model.\n",
    "torch.onnx.export(model, torch.tensor([test[0]]).to(device=device), 'constellator.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph torch_jit (\n",
      "  %input.1[INT64, 1x32]\n",
      ") initializers (\n",
      "  %embedding.weight[FLOAT, 5470x64]\n",
      "  %classifier.attention.linear1.weight[FLOAT, 24x256]\n",
      "  %classifier.attention.linear1.bias[FLOAT, 24]\n",
      "  %classifier.attention.linear2.weight[FLOAT, 1x24]\n",
      "  %classifier.attention.linear2.bias[FLOAT, 1]\n",
      "  %classifier.linear.weight[FLOAT, 12x256]\n",
      "  %classifier.linear.bias[FLOAT, 12]\n",
      "  %onnx::LSTM_742[FLOAT, 2x2048]\n",
      "  %onnx::LSTM_743[FLOAT, 2x1024x64]\n",
      "  %onnx::LSTM_744[FLOAT, 2x1024x256]\n",
      "  %onnx::LSTM_789[FLOAT, 2x2048]\n",
      "  %onnx::LSTM_790[FLOAT, 2x1024x512]\n",
      "  %onnx::LSTM_791[FLOAT, 2x1024x256]\n",
      "  %onnx::LSTM_836[FLOAT, 2x2048]\n",
      "  %onnx::LSTM_837[FLOAT, 2x1024x512]\n",
      "  %onnx::LSTM_838[FLOAT, 2x1024x256]\n",
      "  %onnx::LSTM_883[FLOAT, 2x2048]\n",
      "  %onnx::LSTM_884[FLOAT, 2x1024x512]\n",
      "  %onnx::LSTM_885[FLOAT, 2x1024x256]\n",
      ") {\n",
      "  %/embedding/Gather_output_0 = Gather(%embedding.weight, %input.1)\n",
      "  %/lstm/Transpose_output_0 = Transpose[perm = [1, 0, 2]](%/embedding/Gather_output_0)\n",
      "  %/lstm/Constant_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Constant_1_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Shape_output_0 = Shape(%/lstm/Transpose_output_0)\n",
      "  %/lstm/Constant_2_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_output_0 = Gather(%/lstm/Shape_output_0, %/lstm/Constant_2_output_0)\n",
      "  %onnx::Unsqueeze_179 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_180 = Unsqueeze(%/lstm/Gather_output_0, %onnx::Unsqueeze_179)\n",
      "  %/lstm/Constant_3_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_747 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_output_0 = Concat[axis = 0](%onnx::Concat_747, %onnx::Concat_180, %/lstm/Constant_3_output_0)\n",
      "  %/lstm/Expand_output_0 = Expand(%/lstm/Constant_output_0, %/lstm/Concat_output_0)\n",
      "  %/lstm/Shape_1_output_0 = Shape(%/lstm/Transpose_output_0)\n",
      "  %/lstm/Constant_4_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_1_output_0 = Gather(%/lstm/Shape_1_output_0, %/lstm/Constant_4_output_0)\n",
      "  %onnx::Unsqueeze_190 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_191 = Unsqueeze(%/lstm/Gather_1_output_0, %onnx::Unsqueeze_190)\n",
      "  %/lstm/Constant_5_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_748 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_1_output_0 = Concat[axis = 0](%onnx::Concat_748, %onnx::Concat_191, %/lstm/Constant_5_output_0)\n",
      "  %/lstm/Expand_1_output_0 = Expand(%/lstm/Constant_1_output_0, %/lstm/Concat_1_output_0)\n",
      "  %/lstm/LSTM_output_0, %/lstm/LSTM_output_1, %/lstm/LSTM_output_2 = LSTM[direction = 'bidirectional', hidden_size = 256](%/lstm/Transpose_output_0, %onnx::LSTM_743, %onnx::LSTM_744, %onnx::LSTM_742, %, %/lstm/Expand_output_0, %/lstm/Expand_1_output_0)\n",
      "  %/lstm/Transpose_1_output_0 = Transpose[perm = [0, 2, 1, 3]](%/lstm/LSTM_output_0)\n",
      "  %/lstm/Constant_6_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Reshape_output_0 = Reshape[allowzero = 0](%/lstm/Transpose_1_output_0, %/lstm/Constant_6_output_0)\n",
      "  %/lstm/Constant_7_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Constant_8_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Shape_2_output_0 = Shape(%/lstm/Reshape_output_0)\n",
      "  %/lstm/Constant_9_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_2_output_0 = Gather(%/lstm/Shape_2_output_0, %/lstm/Constant_9_output_0)\n",
      "  %onnx::Unsqueeze_336 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_337 = Unsqueeze(%/lstm/Gather_2_output_0, %onnx::Unsqueeze_336)\n",
      "  %/lstm/Constant_10_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_794 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_2_output_0 = Concat[axis = 0](%onnx::Concat_794, %onnx::Concat_337, %/lstm/Constant_10_output_0)\n",
      "  %/lstm/Expand_2_output_0 = Expand(%/lstm/Constant_7_output_0, %/lstm/Concat_2_output_0)\n",
      "  %/lstm/Shape_3_output_0 = Shape(%/lstm/Reshape_output_0)\n",
      "  %/lstm/Constant_11_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_3_output_0 = Gather(%/lstm/Shape_3_output_0, %/lstm/Constant_11_output_0)\n",
      "  %onnx::Unsqueeze_347 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_348 = Unsqueeze(%/lstm/Gather_3_output_0, %onnx::Unsqueeze_347)\n",
      "  %/lstm/Constant_12_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_795 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_3_output_0 = Concat[axis = 0](%onnx::Concat_795, %onnx::Concat_348, %/lstm/Constant_12_output_0)\n",
      "  %/lstm/Expand_3_output_0 = Expand(%/lstm/Constant_8_output_0, %/lstm/Concat_3_output_0)\n",
      "  %/lstm/LSTM_1_output_0, %/lstm/LSTM_1_output_1, %/lstm/LSTM_1_output_2 = LSTM[direction = 'bidirectional', hidden_size = 256](%/lstm/Reshape_output_0, %onnx::LSTM_790, %onnx::LSTM_791, %onnx::LSTM_789, %, %/lstm/Expand_2_output_0, %/lstm/Expand_3_output_0)\n",
      "  %/lstm/Transpose_2_output_0 = Transpose[perm = [0, 2, 1, 3]](%/lstm/LSTM_1_output_0)\n",
      "  %/lstm/Constant_13_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Reshape_1_output_0 = Reshape[allowzero = 0](%/lstm/Transpose_2_output_0, %/lstm/Constant_13_output_0)\n",
      "  %/lstm/Constant_14_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Constant_15_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Shape_4_output_0 = Shape(%/lstm/Reshape_1_output_0)\n",
      "  %/lstm/Constant_16_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_4_output_0 = Gather(%/lstm/Shape_4_output_0, %/lstm/Constant_16_output_0)\n",
      "  %onnx::Unsqueeze_493 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_494 = Unsqueeze(%/lstm/Gather_4_output_0, %onnx::Unsqueeze_493)\n",
      "  %/lstm/Constant_17_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_841 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_4_output_0 = Concat[axis = 0](%onnx::Concat_841, %onnx::Concat_494, %/lstm/Constant_17_output_0)\n",
      "  %/lstm/Expand_4_output_0 = Expand(%/lstm/Constant_14_output_0, %/lstm/Concat_4_output_0)\n",
      "  %/lstm/Shape_5_output_0 = Shape(%/lstm/Reshape_1_output_0)\n",
      "  %/lstm/Constant_18_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_5_output_0 = Gather(%/lstm/Shape_5_output_0, %/lstm/Constant_18_output_0)\n",
      "  %onnx::Unsqueeze_504 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_505 = Unsqueeze(%/lstm/Gather_5_output_0, %onnx::Unsqueeze_504)\n",
      "  %/lstm/Constant_19_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_842 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_5_output_0 = Concat[axis = 0](%onnx::Concat_842, %onnx::Concat_505, %/lstm/Constant_19_output_0)\n",
      "  %/lstm/Expand_5_output_0 = Expand(%/lstm/Constant_15_output_0, %/lstm/Concat_5_output_0)\n",
      "  %/lstm/LSTM_2_output_0, %/lstm/LSTM_2_output_1, %/lstm/LSTM_2_output_2 = LSTM[direction = 'bidirectional', hidden_size = 256](%/lstm/Reshape_1_output_0, %onnx::LSTM_837, %onnx::LSTM_838, %onnx::LSTM_836, %, %/lstm/Expand_4_output_0, %/lstm/Expand_5_output_0)\n",
      "  %/lstm/Transpose_3_output_0 = Transpose[perm = [0, 2, 1, 3]](%/lstm/LSTM_2_output_0)\n",
      "  %/lstm/Constant_20_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Reshape_2_output_0 = Reshape[allowzero = 0](%/lstm/Transpose_3_output_0, %/lstm/Constant_20_output_0)\n",
      "  %/lstm/Constant_21_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Constant_22_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Shape_6_output_0 = Shape(%/lstm/Reshape_2_output_0)\n",
      "  %/lstm/Constant_23_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_6_output_0 = Gather(%/lstm/Shape_6_output_0, %/lstm/Constant_23_output_0)\n",
      "  %onnx::Unsqueeze_650 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_651 = Unsqueeze(%/lstm/Gather_6_output_0, %onnx::Unsqueeze_650)\n",
      "  %/lstm/Constant_24_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_888 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_6_output_0 = Concat[axis = 0](%onnx::Concat_888, %onnx::Concat_651, %/lstm/Constant_24_output_0)\n",
      "  %/lstm/Expand_6_output_0 = Expand(%/lstm/Constant_21_output_0, %/lstm/Concat_6_output_0)\n",
      "  %/lstm/Shape_7_output_0 = Shape(%/lstm/Reshape_2_output_0)\n",
      "  %/lstm/Constant_25_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_7_output_0 = Gather(%/lstm/Shape_7_output_0, %/lstm/Constant_25_output_0)\n",
      "  %onnx::Unsqueeze_661 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_662 = Unsqueeze(%/lstm/Gather_7_output_0, %onnx::Unsqueeze_661)\n",
      "  %/lstm/Constant_26_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_889 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_7_output_0 = Concat[axis = 0](%onnx::Concat_889, %onnx::Concat_662, %/lstm/Constant_26_output_0)\n",
      "  %/lstm/Expand_7_output_0 = Expand(%/lstm/Constant_22_output_0, %/lstm/Concat_7_output_0)\n",
      "  %/lstm/LSTM_3_output_0, %/lstm/LSTM_3_output_1, %/lstm/LSTM_3_output_2 = LSTM[direction = 'bidirectional', hidden_size = 256](%/lstm/Reshape_2_output_0, %onnx::LSTM_884, %onnx::LSTM_885, %onnx::LSTM_883, %, %/lstm/Expand_6_output_0, %/lstm/Expand_7_output_0)\n",
      "  %/lstm/Transpose_4_output_0 = Transpose[perm = [0, 2, 1, 3]](%/lstm/LSTM_3_output_0)\n",
      "  %/lstm/Constant_27_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Reshape_3_output_0 = Reshape[allowzero = 0](%/lstm/Transpose_4_output_0, %/lstm/Constant_27_output_0)\n",
      "  %/lstm/Transpose_5_output_0 = Transpose[perm = [1, 0, 2]](%/lstm/Reshape_3_output_0)\n",
      "  %/Constant_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Constant_1_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Constant_2_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Constant_3_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Slice_output_0 = Slice(%/lstm/Transpose_5_output_0, %/Constant_1_output_0, %/Constant_2_output_0, %/Constant_output_0, %/Constant_3_output_0)\n",
      "  %/Add_output_0 = Add(%/Slice_output_0, %/Slice_output_0)\n",
      "  %/classifier/attention/Constant_output_0 = Constant[value = <Tensor>]()\n",
      "  %/classifier/attention/Reshape_output_0 = Reshape[allowzero = 0](%/Add_output_0, %/classifier/attention/Constant_output_0)\n",
      "  %/classifier/attention/linear1/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/classifier/attention/Reshape_output_0, %classifier.attention.linear1.weight, %classifier.attention.linear1.bias)\n",
      "  %/classifier/attention/relu/Relu_output_0 = Relu(%/classifier/attention/linear1/Gemm_output_0)\n",
      "  %/classifier/attention/linear2/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/classifier/attention/relu/Relu_output_0, %classifier.attention.linear2.weight, %classifier.attention.linear2.bias)\n",
      "  %/classifier/attention/Constant_1_output_0 = Constant[value = <Tensor>]()\n",
      "  %/classifier/attention/Reshape_1_output_0 = Reshape[allowzero = 0](%/classifier/attention/linear2/Gemm_output_0, %/classifier/attention/Constant_1_output_0)\n",
      "  %/classifier/attention/Softmax_output_0 = Softmax[axis = 1](%/classifier/attention/Reshape_1_output_0)\n",
      "  %/classifier/attention/Constant_2_output_0 = Constant[value = <Tensor>]()\n",
      "  %/classifier/attention/Unsqueeze_output_0 = Unsqueeze(%/classifier/attention/Softmax_output_0, %/classifier/attention/Constant_2_output_0)\n",
      "  %/classifier/Mul_output_0 = Mul(%/Add_output_0, %/classifier/attention/Unsqueeze_output_0)\n",
      "  %onnx::ReduceSum_698 = Constant[value = <Tensor>]()\n",
      "  %/classifier/ReduceSum_output_0 = ReduceSum[keepdims = 0](%/classifier/Mul_output_0, %onnx::ReduceSum_698)\n",
      "  %/classifier/linear/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/classifier/ReduceSum_output_0, %classifier.linear.weight, %classifier.linear.bias)\n",
      "  %701 = LogSoftmax[axis = 1](%/classifier/linear/Gemm_output_0)\n",
      "  return %701\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnx.helper\n",
    "\n",
    "onnx_model = onnx.load('./constellator.onnx')\n",
    "print(onnx.helper.printable_graph(onnx_model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
