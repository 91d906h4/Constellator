{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import jieba\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from data import Data\n",
    "from torch import nn, optim\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed.\n",
    "random_seed = 0\n",
    "\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    0: \"牡羊\",\n",
    "    1: \"金牛\",\n",
    "    2: \"雙子\",\n",
    "    3: \"巨蠍\",\n",
    "    4: \"獅子\",\n",
    "    5: \"處女\",\n",
    "    6: \"天秤\",\n",
    "    7: \"天蠍\",\n",
    "    8: \"射手\",\n",
    "    9: \"魔羯\",\n",
    "    10: \"水瓶\",\n",
    "    11: \"雙魚\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning completed.\n",
      "ToDataset completed.\n",
      "Argumantation completed.\n",
      "Tokenlization completed.\n",
      "Padding completed.\n",
      "Token2id completed.\n",
      "Process completed.\n"
     ]
    }
   ],
   "source": [
    "raw = {i: open(f\"./dataset/{classes[i]}.txt\", \"r\", encoding='UTF-8').read() for i in range(12)}\n",
    "data = Data(data=raw, padding_length=32)\n",
    "\n",
    "train_raw = data.get(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5627"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4048,\n",
       "  1507,\n",
       "  2980,\n",
       "  3759,\n",
       "  1681,\n",
       "  3147,\n",
       "  3531,\n",
       "  3837,\n",
       "  611,\n",
       "  2004,\n",
       "  2517,\n",
       "  2517,\n",
       "  2517,\n",
       "  2517,\n",
       "  2517,\n",
       "  2517,\n",
       "  2517,\n",
       "  2517,\n",
       "  2517,\n",
       "  2517,\n",
       "  2517,\n",
       "  2517,\n",
       "  2517,\n",
       "  2517,\n",
       "  2517,\n",
       "  2517,\n",
       "  2517,\n",
       "  2517,\n",
       "  2517,\n",
       "  2517,\n",
       "  2517,\n",
       "  2517],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, data: list, label: list):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index]), torch.tensor(self.label[index], dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, l = [], []\n",
    "\n",
    "for i, j in train_raw:\n",
    "    d.append(i); l.append(j)\n",
    "\n",
    "train_ds = CreateDataset(d, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3147, 3531, 3837,  611, 2004, 1681, 4048, 1507, 2980, 3759, 2517, 2517,\n",
       "         2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517,\n",
       "         2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517]),\n",
       " tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F_Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(F_Attention, self).__init__()\n",
    "        self.query = nn.Linear(64, 64)\n",
    "        self.key = nn.Linear(64, 64)\n",
    "        self.value = nn.Linear(64, 64)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        query: torch.Tensor = self.query(x)\n",
    "        key: torch.Tensor = self.key(x)\n",
    "        value: torch.Tensor = self.value(x)\n",
    "\n",
    "        score = torch.bmm(query, key.transpose(1, 2)) / 8\n",
    "        attention = torch.softmax(score, dim=2)\n",
    "        x = torch.bmm(attention, value)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.linear1 = nn.Linear(256, 64)\n",
    "        self.linear2 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        b = x.size(0)\n",
    "        x = x.reshape(-1, 256)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        x = x.reshape(b, -1)\n",
    "        x = torch.softmax(x, dim=1)\n",
    "\n",
    "        return x.unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionClassifier, self).__init__()\n",
    "        self.attention = Attention()\n",
    "        self.linear = nn.Linear(256, 12)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        attention = self.attention(x)\n",
    "\n",
    "        x = (x * attention).sum(dim=1)\n",
    "        x = torch.log_softmax(self.linear(x), dim=1)\n",
    "\n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(data.get(\"token_len\"), 64)\n",
    "        self.f_attention = F_Attention()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=64,\n",
    "            hidden_size=256,\n",
    "            num_layers=4,\n",
    "            dropout=0.5,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.linear = nn.Linear(16384, 12)\n",
    "        self.classifier = AttentionClassifier()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x = self.f_attention(x)\n",
    "\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        # x = x.reshape(x.size(0), -1)\n",
    "        # x = self.linear(x)\n",
    "\n",
    "        x = x[:, :, :256] + x[:, :, :256]\n",
    "        x, _ = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traning history data.\n",
    "train_accuracy_h = []\n",
    "train_loss_h = []\n",
    "validate_accuracy_h = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs: int, model: nn.Module, optimizer: optim.Optimizer, loss: nn.CrossEntropyLoss, dataloader: DataLoader):\n",
    "    # Set model to training mode.\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_accuracy = 0\n",
    "        train_loss = 0\n",
    "        train_total = 0\n",
    "        train_process = 0\n",
    "        train_time = datetime.now().timestamp()\n",
    "\n",
    "        for texts, labels in dataloader:\n",
    "            texts: torch.Tensor\n",
    "            labels: torch.Tensor\n",
    "\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs: torch.Tensor = model(texts)\n",
    "            losses: torch.Tensor = loss(outputs, labels)\n",
    "\n",
    "            # optimizer.zero_grad()\n",
    "            for param in model.parameters(): param.grad = None\n",
    "\n",
    "            # Backpropagation.\n",
    "            losses.backward()\n",
    "\n",
    "            # Update parameters.\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predict = torch.max(outputs, 1)\n",
    "            train_accuracy += sum([labels[i][predict[i]] == 1 for i in range(len(predict))])\n",
    "            train_loss += losses.item()\n",
    "            train_total += labels.shape[0]\n",
    "            train_process += 1\n",
    "\n",
    "            print(\n",
    "                f\"{datetime.now().strftime('%Y/%m/%d %H:%M:%S')} \"\n",
    "                f\"Epoch: {epoch:03d} \"\n",
    "                f\"Time: {datetime.now().timestamp() - train_time:.2f} \"\n",
    "                f\"Process: {train_process / len(dataloader) * 100:.2f}% \"\n",
    "                f\"Accuracy: {train_accuracy / train_total * 100:.2f}% \"\n",
    "                f\"Loss: {train_loss:.3f}\",\n",
    "                end=\"\\r\"\n",
    "            )\n",
    "\n",
    "        train_accuracy_h.append(train_accuracy / train_total * 100)\n",
    "        train_loss_h.append(train_loss)\n",
    "\n",
    "        print()\n",
    "\n",
    "        # Early stop.\n",
    "        if train_accuracy / train_total > 0.99:\n",
    "            print(\"Early stopped.\")\n",
    "            break\n",
    "\n",
    "    # Set model to evaluation mode.\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024/04/02 23:43:52 Epoch: 000 Time: 9.77 Process: 100.00% Accuracy: 10.06% Loss: 1748.167\n",
      "2024/04/02 23:44:02 Epoch: 001 Time: 9.76 Process: 100.00% Accuracy: 10.93% Loss: 1743.748\n",
      "2024/04/02 23:44:11 Epoch: 002 Time: 9.20 Process: 100.00% Accuracy: 11.48% Loss: 1740.928\n",
      "2024/04/02 23:44:20 Epoch: 003 Time: 9.45 Process: 100.00% Accuracy: 11.85% Loss: 1735.239\n",
      "2024/04/02 23:44:30 Epoch: 004 Time: 9.96 Process: 100.00% Accuracy: 13.29% Loss: 1719.685\n",
      "2024/04/02 23:44:40 Epoch: 005 Time: 9.92 Process: 100.00% Accuracy: 17.20% Loss: 1669.804\n",
      "2024/04/02 23:44:50 Epoch: 006 Time: 9.55 Process: 100.00% Accuracy: 24.22% Loss: 1527.779\n",
      "2024/04/02 23:44:59 Epoch: 007 Time: 9.16 Process: 100.00% Accuracy: 32.15% Loss: 1375.015\n",
      "2024/04/02 23:45:08 Epoch: 008 Time: 9.31 Process: 100.00% Accuracy: 38.71% Loss: 1262.126\n",
      "2024/04/02 23:45:17 Epoch: 009 Time: 9.14 Process: 100.00% Accuracy: 44.50% Loss: 1136.231\n",
      "2024/04/02 23:45:27 Epoch: 010 Time: 9.35 Process: 100.00% Accuracy: 53.87% Loss: 974.455\n",
      "2024/04/02 23:45:36 Epoch: 011 Time: 9.34 Process: 100.00% Accuracy: 62.56% Loss: 787.797\n",
      "2024/04/02 23:45:46 Epoch: 012 Time: 9.69 Process: 100.00% Accuracy: 70.52% Loss: 609.064\n",
      "2024/04/02 23:45:56 Epoch: 013 Time: 10.33 Process: 100.00% Accuracy: 78.03% Loss: 464.855\n",
      "2024/04/02 23:46:06 Epoch: 014 Time: 9.62 Process: 100.00% Accuracy: 84.63% Loss: 319.203\n",
      "2024/04/02 23:46:15 Epoch: 015 Time: 9.74 Process: 100.00% Accuracy: 90.10% Loss: 219.450\n",
      "Early stopped.\n"
     ]
    }
   ],
   "source": [
    "train(epochs=epochs, model=model, optimizer=optimizer, loss=loss, dataloader=train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5047, 4536, 5122, 4141, 2995, 1681,  243, 1681, 2237, 3837, 4185, 2004,\n",
      "         2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517,\n",
      "         2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517],\n",
      "        [5404,  904, 3815, 1804, 2364, 3837, 2204, 5343, 4001,   57, 2004, 1681,\n",
      "         1266, 2277, 4539, 2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517,\n",
      "         2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517],\n",
      "        [1239, 3834, 2192, 3292, 3837, 4048, 3176, 2098, 4626, 4777, 3837,  611,\n",
      "         5388, 5299, 1068, 1364, 2577, 3837,  799, 4048, 2975, 1681, 2936, 1364,\n",
      "         3230, 4625, 1680, 3837, 4001, 2204, 4000, 1364],\n",
      "        [3837, 2102, 2081,  782, 3839, 4111, 1364, 2142, 5436, 4750, 4536,  214,\n",
      "         2995, 1364,  233, 2894, 1129, 1364, 5404, 1416,  763,  652,  436, 2004,\n",
      "         1364, 2943, 4878,  857, 3067, 1364, 1672, 4441],\n",
      "        [2326, 3843, 3837, 3526, 2218, 1364, 4574, 1364, 3894, 4057, 3869, 1364,\n",
      "         2317,  214, 3947, 1364, 1704, 5058,  353, 1364, 4141, 2409, 1364, 5052,\n",
      "         1463, 1364, 5122, 1985, 4033, 2394, 1364, 3230],\n",
      "        [3216, 4869, 1364,  992, 4101,  883, 3837, 5370, 1609, 1178, 2004, 2517,\n",
      "         2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517,\n",
      "         2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517],\n",
      "        [ 452, 3467, 1364, 1956, 4201,  219, 2004, 2517, 2517, 2517, 2517, 2517,\n",
      "         2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517,\n",
      "         2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517],\n",
      "        [2664, 2065, 4141, 1468,  842, 4552, 1364,  396, 3843, 2065, 4141, 1468,\n",
      "          842, 3715, 1204, 5385, 2004, 2517, 2517, 2517, 2517, 2517, 2517, 2517,\n",
      "         2517, 2517, 2517, 2517, 2517, 2517, 2517, 2517]])\n",
      "tensor([[-4.5208e+00, -1.2902e+01, -8.9707e+00, -9.2601e+00, -8.5996e-02,\n",
      "         -9.9217e+00, -1.4869e+01, -2.6719e+00, -7.4009e+00, -1.2404e+01,\n",
      "         -8.6795e+00, -6.6122e+00],\n",
      "        [-1.9853e+01, -1.0503e+01, -9.4257e+00, -3.0020e+00, -8.1580e+00,\n",
      "         -6.5170e-02, -1.2239e+01, -1.1108e+01, -1.4547e+01, -4.3461e+00,\n",
      "         -1.1038e+01, -1.1032e+01],\n",
      "        [-4.1755e+00, -6.6022e+00, -7.7410e+00, -4.8818e+00, -7.7971e+00,\n",
      "         -8.2741e+00, -5.9430e+00, -3.8451e+00, -1.2113e+00, -8.7506e+00,\n",
      "         -4.3033e-01, -6.0681e+00],\n",
      "        [-1.6236e+01, -4.1897e-01, -9.3056e+00, -1.4585e+00, -1.3863e+01,\n",
      "         -3.9283e+00, -6.1517e+00, -1.0136e+01, -1.2918e+01, -2.4419e+00,\n",
      "         -8.0558e+00, -7.7100e+00],\n",
      "        [-1.6057e+01, -2.9746e+01, -1.8263e+01, -2.8714e+01, -5.9605e-07,\n",
      "         -1.6369e+01, -3.0818e+01, -2.1695e+01, -1.8462e+01, -2.4699e+01,\n",
      "         -1.9358e+01, -1.4673e+01],\n",
      "        [-4.8912e+00, -1.8322e+01, -8.7666e+00, -2.3887e+01, -8.6266e+00,\n",
      "         -1.2058e+01, -1.1399e+01, -1.4292e+01, -4.8562e+00, -2.8533e+01,\n",
      "         -1.2868e+01, -1.5772e-02],\n",
      "        [-4.8311e+00, -5.8902e+00, -5.7529e+00, -3.2079e+00, -7.3961e+00,\n",
      "         -8.7388e+00, -7.7406e+00, -5.9338e-02, -7.7993e+00, -1.0158e+01,\n",
      "         -9.4046e+00, -6.4944e+00],\n",
      "        [-2.1092e+01, -2.3155e+01, -1.1103e+01, -2.9123e+01, -1.0885e+01,\n",
      "         -5.3353e+00, -1.5016e+01, -2.6735e+01, -1.3471e+01, -2.7458e+01,\n",
      "         -1.9986e+01, -4.8658e-03]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "for i, j in train_dl:\n",
    "    print(i)\n",
    "    print(model(i.to(device)))\n",
    "    print(j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = [\n",
    "    \"勇敢無畏，充滿了活力和冒險精神，他們喜歡追求挑戰，敢於冒險，常常是行動派的領導者。\",\n",
    "    \"穩重可靠，以堅韌的意志力和耐心著稱，他們注重安全和舒適，並對物質生活有著強烈的執著。\",\n",
    "    \"機智聰明，好奇心旺盛，喜歡交際和表達自己，具有多才多藝的特質，常常充滿了靈活的思維和活力。\",\n",
    "    \"情感豐富，善解人意，對家庭和親密關係非常重視，他們總是充滿了溫柔和關懷，是很好的傾聽者和支持者。\",\n",
    "    \"自信大方，追求著成為焦點的慾望，他們充滿了熱情和活力，喜歡引領和影響身邊的人，時常展現出優越感和領導能力。\",\n",
    "    \"細心謹慎，追求完美，他們善於分析和解決問題，注重細節和有組織性，常常是值得信賴的夥伴和顧問。\",\n",
    "    \"追求和諧，優雅而公正，他們注重平衡和公平，善於溝通協調，是很好的調解者和中介者。\",\n",
    "    \"神秘內斂，充滿了熱情和直覺，他們擁有強烈的意志力和洞察力，常常是充滿挑戰性和魅力的個體。\",\n",
    "    \"自由奔放，熱愛冒險和探索，他們追求著廣闊的視野和新鮮的體驗，時常充滿了樂觀和幽默。\",\n",
    "    \"勤奮負責，追求事業成功和社會地位，他們具有堅毅的意志力和耐心，常常是穩健和實際的決策者。\",\n",
    "    \"獨立思考，充滿了理想主義和創意，他們追求著獨特的生活方式和社會價值觀，常常是前衛和不拘一格的個體。\",\n",
    "    \"敏感善良，充滿了同情心和想像力，他們常常是理想主義者和夢想家，追求著內心的情感和精神實踐。\",\n",
    "    \"勇敢無畏，充滿活力，喜歡追求挑戰，常常是行動派的領導者。\",\n",
    "    \"穩重可靠，堅韌耐心，注重安全舒適，對物質生活有強烈執著。\",\n",
    "    \"機智聰明，好奇心旺盛，善於交際表達，充滿靈活思維和活力。\",\n",
    "    \"情感豐富，善解人意，重視家庭和親密關係，溫柔關懷，傾聽支持者。\",\n",
    "    \"自信大方，追求成為焦點，充滿熱情活力，喜歡引領影響身邊的人。\",\n",
    "    \"細心謹慎，追求完美，善於分析解決問題，注重細節有組織性。\",\n",
    "    \"追求和諧，優雅公正，注重平衡公平，善於溝通協調。\",\n",
    "    \"神秘內斂，熱情直覺，意志力洞察力強，充滿挑戰性和魅力。\",\n",
    "    \"自由奔放，熱愛冒險探索，追求廣闊視野和新鮮體驗，樂觀幽默。\",\n",
    "    \"勤奮負責，追求事業成功社會地位，具堅毅意志力和耐心，穩健決策者。\",\n",
    "    \"獨立思考，理想主義創意，追求獨特生活方式和價值觀，前衛不拘一格。\",\n",
    "    \"敏感善良，同情心想像力豐富，理想主義夢想家，追求內心情感精神實踐。\",\n",
    "    \"勇敢果敢，充滿活力，愛冒險。\",\n",
    "    \"穩重堅定，堅持自我價值觀。\",\n",
    "    \"靈活機智，善於溝通交際。\",\n",
    "    \"情感豐富，家庭意識強。\",\n",
    "    \"自信領導，熱情奔放。\",\n",
    "    \"細心謹慎，追求完美。\",\n",
    "    \"追求和諧，公正公平。\",\n",
    "    \"神秘敏感，探索深度。\",\n",
    "    \"自由探險，樂觀向上。\",\n",
    "    \"勤奮穩健，追求成功。\",\n",
    "    \"獨立創新，理想主義者。\",\n",
    "    \"敏感浪漫，夢想家。\",\n",
    "    \"生命充滿了勇氣與活力，他們勇於面對挑戰，永不退縮，常常是行動派的領袖，樂於率先嘗試新事物。\",\n",
    "    \"展現出穩重且堅定的品性，他們始終堅持自身的價值觀與信念，喜歡在自己熟悉的領域中深耕不輟。\",\n",
    "    \"擁有機智敏捷的頭腦，他們善於溝通交際，充滿活力與靈活的思維，對於各種新奇的事物都抱有濃厚的好奇心。\",\n",
    "    \"的人情感豐富且懂得關懷，他們對家庭有著極為強烈的連結感，願意為了家人無條件地付出與奉獻。\",\n",
    "    \"展現出極度的自信與熱情，他們常常是眾人注目的焦點，充滿著領導力與活力，喜歡成為團體中的中心人物。\",\n",
    "    \"的人細心謹慎，追求完美與規律，他們擁有出色的分析能力與解決問題的技巧，喜歡保持事情的有序與井然有序。\",\n",
    "    \"追求和諧與公正，他們擅長於溝通協調，注重平衡與公平，善於解決衝突，是團隊中不可或缺的調解者。\",\n",
    "    \"的人神秘內斂，充滿了深度與直覺，他們擁有強烈的意志力與洞察力，總是對事情有著深入的探索與研究。\",\n",
    "    \"熱愛自由與探險，他們樂觀向上，喜歡挑戰與冒險，追求著廣闊的視野與新鮮的體驗，不斷探索未知的領域。\",\n",
    "    \"的人勤奮穩健，追求事業成功與社會地位，他們具有堅毅的意志力與耐心，是穩健可靠的決策者與領導者。\",\n",
    "    \"獨立創新，理想主義者，他們勇於打破傳統的束縛，追求獨特的生活方式與社會價值觀，常常是前衛與不拘一格的個體。\",\n",
    "    \"的人敏感浪漫，是真正的夢想家，他們充滿了同情心與想像力，常常是理想主義者，追求內心情感與精神的實踐。\",\n",
    "]\n",
    "\n",
    "answare = [\n",
    "    '牡羊', '金牛', '雙子', '巨蠍', '獅子', '處女', '天秤', '天蠍', '射手', '魔羯', '水瓶', '雙魚',\n",
    "    '牡羊', '金牛', '雙子', '巨蠍', '獅子', '處女', '天秤', '天蠍', '射手', '魔羯', '水瓶', '雙魚',\n",
    "    '牡羊', '金牛', '雙子', '巨蠍', '獅子', '處女', '天秤', '天蠍', '射手', '魔羯', '水瓶', '雙魚',\n",
    "    '牡羊', '金牛', '雙子', '巨蠍', '獅子', '處女', '天秤', '天蠍', '射手', '魔羯', '水瓶', '雙魚',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(testset: list) -> torch.Tensor:\n",
    "    result = []\n",
    "\n",
    "    for line in testset:\n",
    "        temp = jieba.lcut(line)\n",
    "        temp = temp + [\"<PAD>\"] * (32 - len(temp))\n",
    "        temp = [data.w2i[x] if x in data.w2i else data.w2i[\"<PAD>\"] for x in temp][:32]\n",
    "        result.append(temp)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = process(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding): Embedding(5470, 64)\n",
       "  (f_attention): F_Attention(\n",
       "    (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (lstm): LSTM(64, 256, num_layers=4, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (linear): Linear(in_features=16384, out_features=12, bias=True)\n",
       "  (classifier): AttentionClassifier(\n",
       "    (attention): Attention(\n",
       "      (linear1): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (linear2): Linear(in_features=64, out_features=1, bias=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (linear): Linear(in_features=256, out_features=12, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1765e-01, -1.9629e+01, -9.1400e+00, -1.5665e+01, -7.3810e+00,\n",
      "         -2.0351e+01, -1.3112e+01, -8.6578e+00, -2.2074e+00, -2.0208e+01,\n",
      "         -9.3177e+00, -1.1710e+01],\n",
      "        [-5.3995e+00, -4.1170e+00, -3.1809e+00, -2.5395e-01, -7.5983e+00,\n",
      "         -6.6918e+00, -3.8490e+00, -2.1283e+00, -4.9332e+00, -5.8055e+00,\n",
      "         -5.7332e+00, -5.0565e+00],\n",
      "        [-3.2537e+00, -1.0043e+01, -5.3831e+00, -3.9422e+00, -4.7011e+00,\n",
      "         -7.7596e+00, -6.5669e+00, -2.2455e+00, -2.4333e-01, -1.0629e+01,\n",
      "         -3.9287e+00, -4.0852e+00],\n",
      "        [-1.4176e+01, -9.4901e+00, -9.7644e+00, -1.5839e-02, -1.3994e+01,\n",
      "         -1.1428e+01, -5.1549e+00, -8.9585e+00, -7.2575e+00, -4.7510e+00,\n",
      "         -8.0478e+00, -1.3602e+01],\n",
      "        [-8.8867e-01, -1.5780e+01, -7.6809e+00, -8.8323e+00, -1.1291e+00,\n",
      "         -1.3964e+01, -1.3698e+01, -1.3600e+00, -4.8254e+00, -1.4455e+01,\n",
      "         -9.7673e+00, -9.0996e+00],\n",
      "        [-1.8369e+01, -6.2271e+00, -1.1301e+01, -4.8079e+00, -1.2098e+01,\n",
      "         -3.3360e+00, -6.1566e+00, -1.5720e+01, -8.6493e+00, -3.9328e-01,\n",
      "         -1.2833e+00, -1.1844e+01],\n",
      "        [-2.4830e+01, -1.5958e+01, -1.3613e+01, -1.8687e+01, -3.2386e+01,\n",
      "         -1.7707e+01, -5.6028e-06, -2.9289e+01, -1.2397e+01, -2.1511e+01,\n",
      "         -1.7820e+01, -1.6933e+01],\n",
      "        [-2.6126e-01, -1.6140e+01, -9.7041e+00, -1.0021e+01, -4.8717e+00,\n",
      "         -1.7028e+01, -1.3255e+01, -1.5795e+00, -4.1347e+00, -1.6317e+01,\n",
      "         -9.7069e+00, -1.1379e+01],\n",
      "        [-2.2929e+00, -1.2276e+01, -7.3872e+00, -5.6448e+00, -6.7347e+00,\n",
      "         -1.3976e+01, -6.7686e+00, -3.8334e+00, -1.4813e-01, -1.0812e+01,\n",
      "         -4.7769e+00, -8.9169e+00],\n",
      "        [-3.1001e+00, -6.4131e+00, -3.6506e+00, -2.1011e+00, -4.2676e+00,\n",
      "         -7.9995e+00, -4.9780e+00, -3.2527e+00, -8.5840e-01, -5.1307e+00,\n",
      "         -1.1658e+00, -5.6051e+00],\n",
      "        [-2.3889e+00, -9.8969e+00, -1.0442e+01, -8.3696e+00, -6.7287e+00,\n",
      "         -1.4226e+01, -9.3810e+00, -5.3994e+00, -1.0706e+00, -9.5449e+00,\n",
      "         -5.8127e-01, -9.3924e+00],\n",
      "        [-7.0853e+00, -1.6893e+01, -1.0226e+01, -1.1070e+01, -8.0727e+00,\n",
      "         -1.4266e+01, -9.7965e+00, -1.0936e+01, -1.3237e-02, -1.4330e+01,\n",
      "         -4.4424e+00, -9.1500e+00],\n",
      "        [-5.8782e-01, -1.7813e+01, -9.2118e+00, -1.3193e+01, -7.1256e+00,\n",
      "         -1.8542e+01, -1.1537e+01, -7.1010e+00, -8.1574e-01, -1.8446e+01,\n",
      "         -7.9190e+00, -9.8893e+00],\n",
      "        [-1.4155e+01, -4.4651e+00, -9.0520e+00, -1.2164e+00, -1.0749e+01,\n",
      "         -7.0130e+00, -8.0352e+00, -9.4799e+00, -1.1099e+01, -3.7305e-01,\n",
      "         -6.1714e+00, -1.1020e+01],\n",
      "        [-2.8152e+00, -9.2780e+00, -7.7069e+00, -3.6183e+00, -5.0968e+00,\n",
      "         -1.0633e+01, -7.4700e+00, -1.4405e+00, -5.4510e-01, -9.0598e+00,\n",
      "         -2.4316e+00, -6.5571e+00],\n",
      "        [-1.6954e+01, -1.1765e+01, -9.4540e+00, -2.4793e-03, -1.3911e+01,\n",
      "         -1.1093e+01, -7.9702e+00, -9.1118e+00, -1.1342e+01, -6.2637e+00,\n",
      "         -1.3365e+01, -1.4846e+01],\n",
      "        [-3.7320e+00, -2.1386e+01, -9.4278e+00, -1.6082e+01, -2.6699e-02,\n",
      "         -1.6079e+01, -1.8538e+01, -9.8594e+00, -6.1162e+00, -1.7628e+01,\n",
      "         -1.0178e+01, -1.0741e+01],\n",
      "        [-2.0416e+01, -9.5445e+00, -1.2384e+01, -8.6078e+00, -9.6844e+00,\n",
      "         -7.5600e-02, -1.0789e+01, -1.7160e+01, -1.1440e+01, -3.5853e+00,\n",
      "         -3.1073e+00, -1.1539e+01],\n",
      "        [-2.4541e+01, -1.6433e+01, -1.4125e+01, -2.0081e+01, -3.3066e+01,\n",
      "         -1.8228e+01, -5.6028e-06, -2.9613e+01, -1.2244e+01, -2.3489e+01,\n",
      "         -1.8123e+01, -1.6253e+01],\n",
      "        [-3.5782e+00, -1.1606e+01, -9.5008e+00, -4.5094e+00, -7.6802e+00,\n",
      "         -1.2749e+01, -6.8774e+00, -1.6706e+00, -2.7130e-01, -1.1439e+01,\n",
      "         -4.7248e+00, -9.2968e+00],\n",
      "        [-6.0030e+00, -1.6917e+01, -1.1193e+01, -1.4660e+01, -1.0686e+01,\n",
      "         -1.5990e+01, -1.0195e+01, -1.2836e+01, -1.8553e-02, -1.7518e+01,\n",
      "         -4.1559e+00, -8.7259e+00],\n",
      "        [-4.3824e+00, -3.6934e+00, -2.2495e+00, -7.2689e-01, -5.2406e+00,\n",
      "         -7.3919e+00, -3.3874e+00, -4.5969e+00, -2.8692e+00, -2.0854e+00,\n",
      "         -1.9747e+00, -5.4863e+00],\n",
      "        [-1.0280e+00, -1.0292e+01, -9.6268e+00, -1.0994e+01, -6.3273e+00,\n",
      "         -1.3165e+01, -9.5084e+00, -5.4449e+00, -6.5932e-01, -1.3726e+01,\n",
      "         -2.1406e+00, -6.7272e+00],\n",
      "        [-7.0554e+00, -9.8003e+00, -9.0885e+00, -2.1373e+00, -4.1901e+00,\n",
      "         -3.5334e+00, -7.3678e+00, -1.4080e+00, -1.0199e+00, -8.7185e+00,\n",
      "         -1.4919e+00, -5.1751e+00],\n",
      "        [-3.9846e-01, -1.8114e+01, -9.0587e+00, -1.2799e+01, -5.8560e+00,\n",
      "         -1.7202e+01, -1.1505e+01, -5.7169e+00, -1.1328e+00, -1.8770e+01,\n",
      "         -8.8771e+00, -9.3347e+00],\n",
      "        [-1.3630e+01, -3.6918e+00, -4.7967e+00, -2.4561e+00, -8.3242e+00,\n",
      "         -4.7051e+00, -7.9704e+00, -1.0589e+01, -1.2187e+01, -1.3831e-01,\n",
      "         -7.7638e+00, -8.9583e+00],\n",
      "        [-1.0165e+01, -9.2130e+00, -1.8095e-02, -5.8454e+00, -4.9963e+00,\n",
      "         -6.4465e+00, -1.1113e+01, -1.1089e+01, -1.1077e+01, -5.1270e+00,\n",
      "         -8.7312e+00, -7.7981e+00],\n",
      "        [-6.2869e+00, -1.2504e+01, -1.0799e+01, -3.0091e+00, -8.3602e+00,\n",
      "         -1.4553e+01, -1.0238e+01, -5.4516e-02, -6.4885e+00, -1.1078e+01,\n",
      "         -1.0610e+01, -1.2333e+01],\n",
      "        [-6.0871e-01, -1.6170e+01, -7.8089e+00, -1.2727e+01, -8.2989e-01,\n",
      "         -1.4002e+01, -1.4467e+01, -4.1446e+00, -5.7172e+00, -1.6294e+01,\n",
      "         -1.0182e+01, -8.2508e+00],\n",
      "        [-1.6677e+01, -8.7205e+00, -1.1359e+01, -1.0931e+01, -4.2383e+00,\n",
      "         -8.1407e-02, -1.5348e+01, -1.5002e+01, -1.2867e+01, -4.1942e+00,\n",
      "         -3.0321e+00, -8.2687e+00],\n",
      "        [-2.0545e+01, -7.3857e+00, -9.8458e+00, -1.1875e+01, -2.6195e+01,\n",
      "         -1.1088e+01, -7.1083e-04, -2.2578e+01, -1.2119e+01, -1.2468e+01,\n",
      "         -1.2308e+01, -1.3456e+01],\n",
      "        [-3.6306e+00, -8.8997e+00, -8.9252e+00, -3.1497e+00, -1.0699e+01,\n",
      "         -1.2360e+01, -5.0264e+00, -2.3186e-01, -2.0527e+00, -1.2323e+01,\n",
      "         -6.1185e+00, -8.9972e+00],\n",
      "        [-2.6983e+00, -1.7172e+01, -1.0299e+01, -1.2995e+01, -6.2073e+00,\n",
      "         -1.6378e+01, -1.0531e+01, -8.8403e+00, -7.7172e-02, -1.5877e+01,\n",
      "         -5.3653e+00, -9.8472e+00],\n",
      "        [-6.3884e+00, -5.8802e+00, -3.2567e+00, -2.5726e+00, -9.8442e-01,\n",
      "         -1.0094e+00, -7.4136e+00, -3.3295e+00, -4.7854e+00, -4.7011e+00,\n",
      "         -3.0943e+00, -3.1367e+00],\n",
      "        [-3.3982e+00, -8.9189e+00, -1.0534e+01, -8.8045e+00, -7.0840e+00,\n",
      "         -1.2120e+01, -9.6266e+00, -6.0891e+00, -2.3947e+00, -1.0013e+01,\n",
      "         -1.3750e-01, -8.1025e+00],\n",
      "        [-4.5540e+00, -9.8457e+00, -1.0177e+01, -4.6897e+00, -7.3126e+00,\n",
      "         -1.1379e+01, -9.1026e+00, -2.9262e-02, -4.8953e+00, -1.1811e+01,\n",
      "         -7.4239e+00, -8.7572e+00],\n",
      "        [-1.3228e+00, -1.7631e+01, -6.5771e+00, -1.0108e+01, -6.0922e+00,\n",
      "         -1.7772e+01, -9.4973e+00, -6.9534e+00, -3.1672e-01, -1.5324e+01,\n",
      "         -8.0424e+00, -1.0272e+01],\n",
      "        [-4.3352e+00, -1.1801e+01, -8.0483e+00, -2.2600e+00, -5.9955e+00,\n",
      "         -1.3111e+01, -6.1901e+00, -1.1886e+00, -5.6036e-01, -8.9285e+00,\n",
      "         -6.3335e+00, -9.0222e+00],\n",
      "        [-2.7920e+00, -1.3941e+01, -3.4665e+00, -5.3031e+00, -2.7532e-01,\n",
      "         -9.8149e+00, -1.0947e+01, -2.1303e+00, -3.7818e+00, -1.0991e+01,\n",
      "         -8.2855e+00, -6.6876e+00],\n",
      "        [-1.1867e+01, -1.2947e+01, -1.2763e+01, -1.8508e-01, -1.1030e+01,\n",
      "         -1.1569e+01, -9.0432e+00, -1.7838e+00, -7.2123e+00, -9.9050e+00,\n",
      "         -1.0898e+01, -1.4027e+01],\n",
      "        [-2.7871e+00, -1.9427e+01, -9.5258e+00, -9.8928e+00, -3.8193e+00,\n",
      "         -1.6252e+01, -1.1255e+01, -5.1851e+00, -9.3763e-02, -1.5574e+01,\n",
      "         -8.4757e+00, -1.0958e+01],\n",
      "        [-1.7884e+01, -9.1153e+00, -1.2202e+01, -3.8117e+00, -7.3447e+00,\n",
      "         -1.4945e-01, -9.3110e+00, -1.1439e+01, -8.7321e+00, -2.6265e+00,\n",
      "         -3.1395e+00, -1.0042e+01],\n",
      "        [-2.0250e+01, -1.6449e+01, -1.3310e+01, -1.6006e+01, -2.7518e+01,\n",
      "         -1.7562e+01, -7.5205e-04, -2.4461e+01, -7.1970e+00, -2.0286e+01,\n",
      "         -1.3926e+01, -1.5078e+01],\n",
      "        [-7.4371e+00, -1.7288e+01, -1.2480e+01, -6.7025e+00, -1.1426e+01,\n",
      "         -1.4570e+01, -6.7715e+00, -5.1030e+00, -9.4512e-03, -1.5903e+01,\n",
      "         -7.9628e+00, -1.3092e+01],\n",
      "        [-4.5100e+00, -1.6495e+01, -9.2824e+00, -9.4707e+00, -6.3328e+00,\n",
      "         -1.4754e+01, -8.8094e+00, -7.8935e+00, -1.6067e-02, -1.3815e+01,\n",
      "         -6.0308e+00, -9.6243e+00],\n",
      "        [-1.0861e+01, -6.3648e+00, -1.0227e+01, -9.8972e-02, -1.2780e+01,\n",
      "         -9.0714e+00, -2.9864e+00, -5.7619e+00, -3.9726e+00, -5.5957e+00,\n",
      "         -4.1252e+00, -1.0616e+01],\n",
      "        [-1.2404e+01, -7.1734e+00, -1.2044e+01, -9.5639e+00, -1.3970e+01,\n",
      "         -1.0179e+01, -6.0850e+00, -1.5074e+01, -4.8946e+00, -6.8422e+00,\n",
      "         -1.1868e-02, -9.4355e+00],\n",
      "        [-8.1081e+00, -1.3325e+01, -1.2390e+01, -6.0736e+00, -8.5568e+00,\n",
      "         -9.7593e+00, -7.9572e+00, -6.0322e+00, -1.3134e-01, -1.1067e+01,\n",
      "         -2.1420e+00, -1.0308e+01]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "result = model(torch.tensor(test).to(device))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normolization\n",
    "results = []\n",
    "\n",
    "for i in range(len(result)):\n",
    "    temp = result[i]\n",
    "    temp = [x - min(temp) for x in temp]\n",
    "    temp = [x / max(temp) for x in temp]\n",
    "    temp = [round(x, 3) for x in temp]\n",
    "\n",
    "    results.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "牡羊✅\n",
      "巨蠍 <- 金牛\n",
      "射手 <- 雙子\n",
      "巨蠍✅\n",
      "牡羊 <- 獅子\n",
      "魔羯 <- 處女\n",
      "天秤✅\n",
      "牡羊 <- 天蠍\n",
      "射手✅\n",
      "射手 <- 魔羯\n",
      "水瓶✅\n",
      "射手 <- 雙魚\n",
      "牡羊✅\n",
      "魔羯 <- 金牛\n",
      "射手 <- 雙子\n",
      "巨蠍✅\n",
      "獅子✅\n",
      "處女✅\n",
      "天秤✅\n",
      "射手 <- 天蠍\n",
      "射手✅\n",
      "巨蠍 <- 魔羯\n",
      "射手 <- 水瓶\n",
      "射手 <- 雙魚\n",
      "牡羊✅\n",
      "魔羯 <- 金牛\n",
      "雙子✅\n",
      "天蠍 <- 巨蠍\n",
      "牡羊 <- 獅子\n",
      "處女✅\n",
      "天秤✅\n",
      "天蠍✅\n",
      "射手✅\n",
      "獅子 <- 魔羯\n",
      "水瓶✅\n",
      "天蠍 <- 雙魚\n",
      "射手 <- 牡羊\n",
      "射手 <- 金牛\n",
      "獅子 <- 雙子\n",
      "巨蠍✅\n",
      "射手 <- 獅子\n",
      "處女✅\n",
      "天秤✅\n",
      "射手 <- 天蠍\n",
      "射手✅\n",
      "巨蠍 <- 魔羯\n",
      "水瓶✅\n",
      "射手 <- 雙魚\n",
      "Correct: 23/48 (47.92%)\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    t = classes[int(torch.argmax(torch.tensor(r)))]\n",
    "    if t == answare[i]:\n",
    "        count += 1\n",
    "        print(t + \"✅\")\n",
    "    else:\n",
    "        print(t + \" <- \" + answare[i])\n",
    "\n",
    "print(f\"Correct: {count}/{len(answare)} ({(count / len(answare)) * 100 :.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# angles_A = np.linspace(start=0, stop=2*np.pi, num=len(result)+1, endpoint=True)\n",
    "# values_A = np.concatenate((result, [result[0]]))\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(5, 5), subplot_kw={'projection': 'polar'})\n",
    "# ax.plot(angles_A, values_A, 'o-', color=\"blue\", label=\"A\")\n",
    "\n",
    "# ax.fill(angles_A, values_A, alpha=0.3, color=\"blue\")\n",
    "# ax.set_thetagrids(angles_A[:-1] * 180 / np.pi, range(12), fontsize=15)\n",
    "# ax.set_theta_zero_location('N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please check the virtual input of the ONNX model.\n",
    "torch.onnx.export(model, torch.tensor([test[0]]).to(device=device), 'constellator.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph torch_jit (\n",
      "  %input.1[INT64, 1x32]\n",
      ") initializers (\n",
      "  %embedding.weight[FLOAT, 5470x64]\n",
      "  %classifier.attention.linear1.weight[FLOAT, 64x256]\n",
      "  %classifier.attention.linear1.bias[FLOAT, 64]\n",
      "  %classifier.attention.linear2.weight[FLOAT, 1x64]\n",
      "  %classifier.attention.linear2.bias[FLOAT, 1]\n",
      "  %classifier.linear.weight[FLOAT, 12x256]\n",
      "  %classifier.linear.bias[FLOAT, 12]\n",
      "  %onnx::LSTM_748[FLOAT, 2x2048]\n",
      "  %onnx::LSTM_749[FLOAT, 2x1024x64]\n",
      "  %onnx::LSTM_750[FLOAT, 2x1024x256]\n",
      "  %onnx::LSTM_795[FLOAT, 2x2048]\n",
      "  %onnx::LSTM_796[FLOAT, 2x1024x512]\n",
      "  %onnx::LSTM_797[FLOAT, 2x1024x256]\n",
      "  %onnx::LSTM_842[FLOAT, 2x2048]\n",
      "  %onnx::LSTM_843[FLOAT, 2x1024x512]\n",
      "  %onnx::LSTM_844[FLOAT, 2x1024x256]\n",
      "  %onnx::LSTM_889[FLOAT, 2x2048]\n",
      "  %onnx::LSTM_890[FLOAT, 2x1024x512]\n",
      "  %onnx::LSTM_891[FLOAT, 2x1024x256]\n",
      ") {\n",
      "  %/embedding/Gather_output_0 = Gather(%embedding.weight, %input.1)\n",
      "  %/lstm/Transpose_output_0 = Transpose[perm = [1, 0, 2]](%/embedding/Gather_output_0)\n",
      "  %/lstm/Constant_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Constant_1_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Shape_output_0 = Shape(%/lstm/Transpose_output_0)\n",
      "  %/lstm/Constant_2_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_output_0 = Gather(%/lstm/Shape_output_0, %/lstm/Constant_2_output_0)\n",
      "  %onnx::Unsqueeze_185 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_186 = Unsqueeze(%/lstm/Gather_output_0, %onnx::Unsqueeze_185)\n",
      "  %/lstm/Constant_3_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_753 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_output_0 = Concat[axis = 0](%onnx::Concat_753, %onnx::Concat_186, %/lstm/Constant_3_output_0)\n",
      "  %/lstm/Expand_output_0 = Expand(%/lstm/Constant_output_0, %/lstm/Concat_output_0)\n",
      "  %/lstm/Shape_1_output_0 = Shape(%/lstm/Transpose_output_0)\n",
      "  %/lstm/Constant_4_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_1_output_0 = Gather(%/lstm/Shape_1_output_0, %/lstm/Constant_4_output_0)\n",
      "  %onnx::Unsqueeze_196 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_197 = Unsqueeze(%/lstm/Gather_1_output_0, %onnx::Unsqueeze_196)\n",
      "  %/lstm/Constant_5_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_754 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_1_output_0 = Concat[axis = 0](%onnx::Concat_754, %onnx::Concat_197, %/lstm/Constant_5_output_0)\n",
      "  %/lstm/Expand_1_output_0 = Expand(%/lstm/Constant_1_output_0, %/lstm/Concat_1_output_0)\n",
      "  %/lstm/LSTM_output_0, %/lstm/LSTM_output_1, %/lstm/LSTM_output_2 = LSTM[direction = 'bidirectional', hidden_size = 256](%/lstm/Transpose_output_0, %onnx::LSTM_749, %onnx::LSTM_750, %onnx::LSTM_748, %, %/lstm/Expand_output_0, %/lstm/Expand_1_output_0)\n",
      "  %/lstm/Transpose_1_output_0 = Transpose[perm = [0, 2, 1, 3]](%/lstm/LSTM_output_0)\n",
      "  %/lstm/Constant_6_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Reshape_output_0 = Reshape[allowzero = 0](%/lstm/Transpose_1_output_0, %/lstm/Constant_6_output_0)\n",
      "  %/lstm/Constant_7_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Constant_8_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Shape_2_output_0 = Shape(%/lstm/Reshape_output_0)\n",
      "  %/lstm/Constant_9_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_2_output_0 = Gather(%/lstm/Shape_2_output_0, %/lstm/Constant_9_output_0)\n",
      "  %onnx::Unsqueeze_342 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_343 = Unsqueeze(%/lstm/Gather_2_output_0, %onnx::Unsqueeze_342)\n",
      "  %/lstm/Constant_10_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_800 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_2_output_0 = Concat[axis = 0](%onnx::Concat_800, %onnx::Concat_343, %/lstm/Constant_10_output_0)\n",
      "  %/lstm/Expand_2_output_0 = Expand(%/lstm/Constant_7_output_0, %/lstm/Concat_2_output_0)\n",
      "  %/lstm/Shape_3_output_0 = Shape(%/lstm/Reshape_output_0)\n",
      "  %/lstm/Constant_11_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_3_output_0 = Gather(%/lstm/Shape_3_output_0, %/lstm/Constant_11_output_0)\n",
      "  %onnx::Unsqueeze_353 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_354 = Unsqueeze(%/lstm/Gather_3_output_0, %onnx::Unsqueeze_353)\n",
      "  %/lstm/Constant_12_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_801 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_3_output_0 = Concat[axis = 0](%onnx::Concat_801, %onnx::Concat_354, %/lstm/Constant_12_output_0)\n",
      "  %/lstm/Expand_3_output_0 = Expand(%/lstm/Constant_8_output_0, %/lstm/Concat_3_output_0)\n",
      "  %/lstm/LSTM_1_output_0, %/lstm/LSTM_1_output_1, %/lstm/LSTM_1_output_2 = LSTM[direction = 'bidirectional', hidden_size = 256](%/lstm/Reshape_output_0, %onnx::LSTM_796, %onnx::LSTM_797, %onnx::LSTM_795, %, %/lstm/Expand_2_output_0, %/lstm/Expand_3_output_0)\n",
      "  %/lstm/Transpose_2_output_0 = Transpose[perm = [0, 2, 1, 3]](%/lstm/LSTM_1_output_0)\n",
      "  %/lstm/Constant_13_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Reshape_1_output_0 = Reshape[allowzero = 0](%/lstm/Transpose_2_output_0, %/lstm/Constant_13_output_0)\n",
      "  %/lstm/Constant_14_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Constant_15_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Shape_4_output_0 = Shape(%/lstm/Reshape_1_output_0)\n",
      "  %/lstm/Constant_16_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_4_output_0 = Gather(%/lstm/Shape_4_output_0, %/lstm/Constant_16_output_0)\n",
      "  %onnx::Unsqueeze_499 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_500 = Unsqueeze(%/lstm/Gather_4_output_0, %onnx::Unsqueeze_499)\n",
      "  %/lstm/Constant_17_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_847 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_4_output_0 = Concat[axis = 0](%onnx::Concat_847, %onnx::Concat_500, %/lstm/Constant_17_output_0)\n",
      "  %/lstm/Expand_4_output_0 = Expand(%/lstm/Constant_14_output_0, %/lstm/Concat_4_output_0)\n",
      "  %/lstm/Shape_5_output_0 = Shape(%/lstm/Reshape_1_output_0)\n",
      "  %/lstm/Constant_18_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_5_output_0 = Gather(%/lstm/Shape_5_output_0, %/lstm/Constant_18_output_0)\n",
      "  %onnx::Unsqueeze_510 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_511 = Unsqueeze(%/lstm/Gather_5_output_0, %onnx::Unsqueeze_510)\n",
      "  %/lstm/Constant_19_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_848 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_5_output_0 = Concat[axis = 0](%onnx::Concat_848, %onnx::Concat_511, %/lstm/Constant_19_output_0)\n",
      "  %/lstm/Expand_5_output_0 = Expand(%/lstm/Constant_15_output_0, %/lstm/Concat_5_output_0)\n",
      "  %/lstm/LSTM_2_output_0, %/lstm/LSTM_2_output_1, %/lstm/LSTM_2_output_2 = LSTM[direction = 'bidirectional', hidden_size = 256](%/lstm/Reshape_1_output_0, %onnx::LSTM_843, %onnx::LSTM_844, %onnx::LSTM_842, %, %/lstm/Expand_4_output_0, %/lstm/Expand_5_output_0)\n",
      "  %/lstm/Transpose_3_output_0 = Transpose[perm = [0, 2, 1, 3]](%/lstm/LSTM_2_output_0)\n",
      "  %/lstm/Constant_20_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Reshape_2_output_0 = Reshape[allowzero = 0](%/lstm/Transpose_3_output_0, %/lstm/Constant_20_output_0)\n",
      "  %/lstm/Constant_21_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Constant_22_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Shape_6_output_0 = Shape(%/lstm/Reshape_2_output_0)\n",
      "  %/lstm/Constant_23_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_6_output_0 = Gather(%/lstm/Shape_6_output_0, %/lstm/Constant_23_output_0)\n",
      "  %onnx::Unsqueeze_656 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_657 = Unsqueeze(%/lstm/Gather_6_output_0, %onnx::Unsqueeze_656)\n",
      "  %/lstm/Constant_24_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_894 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_6_output_0 = Concat[axis = 0](%onnx::Concat_894, %onnx::Concat_657, %/lstm/Constant_24_output_0)\n",
      "  %/lstm/Expand_6_output_0 = Expand(%/lstm/Constant_21_output_0, %/lstm/Concat_6_output_0)\n",
      "  %/lstm/Shape_7_output_0 = Shape(%/lstm/Reshape_2_output_0)\n",
      "  %/lstm/Constant_25_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/lstm/Gather_7_output_0 = Gather(%/lstm/Shape_7_output_0, %/lstm/Constant_25_output_0)\n",
      "  %onnx::Unsqueeze_667 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_668 = Unsqueeze(%/lstm/Gather_7_output_0, %onnx::Unsqueeze_667)\n",
      "  %/lstm/Constant_26_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Concat_895 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Concat_7_output_0 = Concat[axis = 0](%onnx::Concat_895, %onnx::Concat_668, %/lstm/Constant_26_output_0)\n",
      "  %/lstm/Expand_7_output_0 = Expand(%/lstm/Constant_22_output_0, %/lstm/Concat_7_output_0)\n",
      "  %/lstm/LSTM_3_output_0, %/lstm/LSTM_3_output_1, %/lstm/LSTM_3_output_2 = LSTM[direction = 'bidirectional', hidden_size = 256](%/lstm/Reshape_2_output_0, %onnx::LSTM_890, %onnx::LSTM_891, %onnx::LSTM_889, %, %/lstm/Expand_6_output_0, %/lstm/Expand_7_output_0)\n",
      "  %/lstm/Transpose_4_output_0 = Transpose[perm = [0, 2, 1, 3]](%/lstm/LSTM_3_output_0)\n",
      "  %/lstm/Constant_27_output_0 = Constant[value = <Tensor>]()\n",
      "  %/lstm/Reshape_3_output_0 = Reshape[allowzero = 0](%/lstm/Transpose_4_output_0, %/lstm/Constant_27_output_0)\n",
      "  %/lstm/Transpose_5_output_0 = Transpose[perm = [1, 0, 2]](%/lstm/Reshape_3_output_0)\n",
      "  %/Constant_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Constant_1_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Constant_2_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Constant_3_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Slice_output_0 = Slice(%/lstm/Transpose_5_output_0, %/Constant_1_output_0, %/Constant_2_output_0, %/Constant_output_0, %/Constant_3_output_0)\n",
      "  %/Add_output_0 = Add(%/Slice_output_0, %/Slice_output_0)\n",
      "  %/classifier/attention/Constant_output_0 = Constant[value = <Tensor>]()\n",
      "  %/classifier/attention/Reshape_output_0 = Reshape[allowzero = 0](%/Add_output_0, %/classifier/attention/Constant_output_0)\n",
      "  %/classifier/attention/linear1/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/classifier/attention/Reshape_output_0, %classifier.attention.linear1.weight, %classifier.attention.linear1.bias)\n",
      "  %/classifier/attention/relu/Relu_output_0 = Relu(%/classifier/attention/linear1/Gemm_output_0)\n",
      "  %/classifier/attention/linear2/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/classifier/attention/relu/Relu_output_0, %classifier.attention.linear2.weight, %classifier.attention.linear2.bias)\n",
      "  %/classifier/attention/Constant_1_output_0 = Constant[value = <Tensor>]()\n",
      "  %/classifier/attention/Reshape_1_output_0 = Reshape[allowzero = 0](%/classifier/attention/linear2/Gemm_output_0, %/classifier/attention/Constant_1_output_0)\n",
      "  %/classifier/attention/Softmax_output_0 = Softmax[axis = 1](%/classifier/attention/Reshape_1_output_0)\n",
      "  %/classifier/attention/Constant_2_output_0 = Constant[value = <Tensor>]()\n",
      "  %/classifier/attention/Unsqueeze_output_0 = Unsqueeze(%/classifier/attention/Softmax_output_0, %/classifier/attention/Constant_2_output_0)\n",
      "  %/classifier/Mul_output_0 = Mul(%/Add_output_0, %/classifier/attention/Unsqueeze_output_0)\n",
      "  %onnx::ReduceSum_704 = Constant[value = <Tensor>]()\n",
      "  %/classifier/ReduceSum_output_0 = ReduceSum[keepdims = 0](%/classifier/Mul_output_0, %onnx::ReduceSum_704)\n",
      "  %/classifier/linear/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/classifier/ReduceSum_output_0, %classifier.linear.weight, %classifier.linear.bias)\n",
      "  %707 = LogSoftmax[axis = 1](%/classifier/linear/Gemm_output_0)\n",
      "  return %707\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnx.helper\n",
    "\n",
    "onnx_model = onnx.load('./constellator.onnx')\n",
    "print(onnx.helper.printable_graph(onnx_model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model size: 22.00 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f\"ONNX model size: {os.path.getsize('./constellator.onnx') / 1048576:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
